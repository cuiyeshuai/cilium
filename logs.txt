level=debug msg="Skipped reading configuration file" error="Config File \"cilium\" Not Found in \"[/root]\"" subsys=config
level=info msg="Memory available for map entries (0.003% of 4124385280B): 10310963B" subsys=config
level=debug msg="Total memory for default map entries: 149422080" subsys=config
level=info msg="option bpf-ct-global-tcp-max set by dynamic sizing to 131072" subsys=config
level=info msg="option bpf-ct-global-any-max set by dynamic sizing to 65536" subsys=config
level=info msg="option bpf-nat-global-max set by dynamic sizing to 131072" subsys=config
level=info msg="option bpf-neigh-global-max set by dynamic sizing to 131072" subsys=config
level=info msg="option bpf-sock-rev-map-max set by dynamic sizing to 65536" subsys=config
level=info msg="  --agent-health-port='9879'" subsys=daemon
level=info msg="  --agent-labels=''" subsys=daemon
level=info msg="  --agent-not-ready-taint-key='node.cilium.io/agent-not-ready'" subsys=daemon
level=info msg="  --allocator-list-timeout='3m0s'" subsys=daemon
level=info msg="  --allow-icmp-frag-needed='true'" subsys=daemon
level=info msg="  --allow-localhost='auto'" subsys=daemon
level=info msg="  --annotate-k8s-node='false'" subsys=daemon
level=info msg="  --api-rate-limit=''" subsys=daemon
level=info msg="  --arping-refresh-period='30s'" subsys=daemon
level=info msg="  --auto-create-cilium-node-resource='true'" subsys=daemon
level=info msg="  --auto-direct-node-routes='true'" subsys=daemon
level=info msg="  --bgp-announce-lb-ip='false'" subsys=daemon
level=info msg="  --bgp-announce-pod-cidr='false'" subsys=daemon
level=info msg="  --bgp-config-path='/var/lib/cilium/bgp/config.yaml'" subsys=daemon
level=info msg="  --bpf-auth-map-max='524288'" subsys=daemon
level=info msg="  --bpf-ct-global-any-max='262144'" subsys=daemon
level=info msg="  --bpf-ct-global-tcp-max='524288'" subsys=daemon
level=info msg="  --bpf-ct-timeout-regular-any='1m0s'" subsys=daemon
level=info msg="  --bpf-ct-timeout-regular-tcp='6h0m0s'" subsys=daemon
level=info msg="  --bpf-ct-timeout-regular-tcp-fin='10s'" subsys=daemon
level=info msg="  --bpf-ct-timeout-regular-tcp-syn='1m0s'" subsys=daemon
level=info msg="  --bpf-ct-timeout-service-any='1m0s'" subsys=daemon
level=info msg="  --bpf-ct-timeout-service-tcp='6h0m0s'" subsys=daemon
level=info msg="  --bpf-ct-timeout-service-tcp-grace='1m0s'" subsys=daemon
level=info msg="  --bpf-filter-priority='1'" subsys=daemon
level=info msg="  --bpf-fragments-map-max='8192'" subsys=daemon
level=info msg="  --bpf-lb-acceleration='disabled'" subsys=daemon
level=info msg="  --bpf-lb-affinity-map-max='0'" subsys=daemon
level=info msg="  --bpf-lb-algorithm='random'" subsys=daemon
level=info msg="  --bpf-lb-dev-ip-addr-inherit=''" subsys=daemon
level=info msg="  --bpf-lb-dsr-dispatch='opt'" subsys=daemon
level=info msg="  --bpf-lb-dsr-l4-xlate='frontend'" subsys=daemon
level=info msg="  --bpf-lb-external-clusterip='false'" subsys=daemon
level=info msg="  --bpf-lb-maglev-hash-seed='JLfvgnHc2kaSUFaI'" subsys=daemon
level=info msg="  --bpf-lb-maglev-map-max='0'" subsys=daemon
level=info msg="  --bpf-lb-maglev-table-size='16381'" subsys=daemon
level=info msg="  --bpf-lb-map-max='65536'" subsys=daemon
level=info msg="  --bpf-lb-mode='snat'" subsys=daemon
level=info msg="  --bpf-lb-rev-nat-map-max='0'" subsys=daemon
level=info msg="  --bpf-lb-rss-ipv4-src-cidr=''" subsys=daemon
level=info msg="  --bpf-lb-rss-ipv6-src-cidr=''" subsys=daemon
level=info msg="  --bpf-lb-service-backend-map-max='0'" subsys=daemon
level=info msg="  --bpf-lb-service-map-max='0'" subsys=daemon
level=info msg="  --bpf-lb-sock='false'" subsys=daemon
level=info msg="  --bpf-lb-sock-hostns-only='true'" subsys=daemon
level=info msg="  --bpf-lb-source-range-map-max='0'" subsys=daemon
level=info msg="  --bpf-map-dynamic-size-ratio='0.0025'" subsys=daemon
level=info msg="  --bpf-map-event-buffers=''" subsys=daemon
level=info msg="  --bpf-nat-global-max='524288'" subsys=daemon
level=info msg="  --bpf-neigh-global-max='524288'" subsys=daemon
level=info msg="  --bpf-policy-map-max='16384'" subsys=daemon
level=info msg="  --bpf-root='/sys/fs/bpf'" subsys=daemon
level=info msg="  --bpf-sock-rev-map-max='262144'" subsys=daemon
level=info msg="  --bypass-ip-availability-upon-restore='false'" subsys=daemon
level=info msg="  --certificates-directory='/var/run/cilium/certs'" subsys=daemon
level=info msg="  --cflags=''" subsys=daemon
level=info msg="  --cgroup-root='/run/cilium/cgroupv2'" subsys=daemon
level=info msg="  --cilium-endpoint-gc-interval='5m0s'" subsys=daemon
level=info msg="  --cluster-health-port='4240'" subsys=daemon
level=info msg="  --cluster-id='0'" subsys=daemon
level=info msg="  --cluster-name='kind-kind'" subsys=daemon
level=info msg="  --clustermesh-config='/var/lib/cilium/clustermesh/'" subsys=daemon
level=info msg="  --cmdref=''" subsys=daemon
level=info msg="  --cni-chaining-mode='none'" subsys=daemon
level=info msg="  --cni-exclusive='true'" subsys=daemon
level=info msg="  --cni-log-file='/var/run/cilium/cilium-cni.log'" subsys=daemon
level=info msg="  --cnp-node-status-gc-interval='0s'" subsys=daemon
level=info msg="  --config=''" subsys=daemon
level=info msg="  --config-dir='/tmp/cilium/config-map'" subsys=daemon
level=info msg="  --config-sources='config-map:kube-system/cilium-config'" subsys=daemon
level=info msg="  --conntrack-gc-interval='0s'" subsys=daemon
level=info msg="  --crab='true'" subsys=daemon
level=info msg="  --crd-wait-timeout='5m0s'" subsys=daemon
level=info msg="  --custom-cni-conf='false'" subsys=daemon
level=info msg="  --datapath-mode='veth'" subsys=daemon
level=info msg="  --debug='true'" subsys=daemon
level=info msg="  --debug-verbose='datapath'" subsys=daemon
level=info msg="  --derive-masquerade-ip-addr-from-device=''" subsys=daemon
level=info msg="  --devices=''" subsys=daemon
level=info msg="  --direct-routing-device=''" subsys=daemon
level=info msg="  --disable-cnp-status-updates='true'" subsys=daemon
level=info msg="  --disable-endpoint-crd='false'" subsys=daemon
level=info msg="  --disable-envoy-version-check='false'" subsys=daemon
level=info msg="  --disable-iptables-feeder-rules=''" subsys=daemon
level=info msg="  --dns-max-ips-per-restored-rule='1000'" subsys=daemon
level=info msg="  --dns-policy-unload-on-shutdown='false'" subsys=daemon
level=info msg="  --dnsproxy-concurrency-limit='0'" subsys=daemon
level=info msg="  --dnsproxy-concurrency-processing-grace-period='0s'" subsys=daemon
level=info msg="  --dnsproxy-lock-count='128'" subsys=daemon
level=info msg="  --dnsproxy-lock-timeout='500ms'" subsys=daemon
level=info msg="  --egress-gateway-policy-map-max='16384'" subsys=daemon
level=info msg="  --egress-masquerade-interfaces=''" subsys=daemon
level=info msg="  --egress-multi-home-ip-rule-compat='false'" subsys=daemon
level=info msg="  --enable-auto-protect-node-port-range='true'" subsys=daemon
level=info msg="  --enable-bandwidth-manager='false'" subsys=daemon
level=info msg="  --enable-bbr='false'" subsys=daemon
level=info msg="  --enable-bgp-control-plane='false'" subsys=daemon
level=info msg="  --enable-bpf-clock-probe='true'" subsys=daemon
level=info msg="  --enable-bpf-masquerade='true'" subsys=daemon
level=info msg="  --enable-bpf-tproxy='false'" subsys=daemon
level=info msg="  --enable-cilium-api-server-access='*'" subsys=daemon
level=info msg="  --enable-cilium-endpoint-slice='false'" subsys=daemon
level=info msg="  --enable-cilium-health-api-server-access='*'" subsys=daemon
level=info msg="  --enable-custom-calls='false'" subsys=daemon
level=info msg="  --enable-endpoint-health-checking='true'" subsys=daemon
level=info msg="  --enable-endpoint-routes='false'" subsys=daemon
level=info msg="  --enable-envoy-config='false'" subsys=daemon
level=info msg="  --enable-external-ips='true'" subsys=daemon
level=info msg="  --enable-health-check-nodeport='true'" subsys=daemon
level=info msg="  --enable-health-checking='true'" subsys=daemon
level=info msg="  --enable-host-firewall='false'" subsys=daemon
level=info msg="  --enable-host-legacy-routing='false'" subsys=daemon
level=info msg="  --enable-host-port='true'" subsys=daemon
level=info msg="  --enable-hubble='true'" subsys=daemon
level=info msg="  --enable-hubble-recorder-api='true'" subsys=daemon
level=info msg="  --enable-icmp-rules='true'" subsys=daemon
level=info msg="  --enable-identity-mark='true'" subsys=daemon
level=info msg="  --enable-ip-masq-agent='false'" subsys=daemon
level=info msg="  --enable-ipsec='false'" subsys=daemon
level=info msg="  --enable-ipv4='true'" subsys=daemon
level=info msg="  --enable-ipv4-egress-gateway='false'" subsys=daemon
level=info msg="  --enable-ipv4-fragment-tracking='true'" subsys=daemon
level=info msg="  --enable-ipv4-masquerade='true'" subsys=daemon
level=info msg="  --enable-ipv6='false'" subsys=daemon
level=info msg="  --enable-ipv6-big-tcp='false'" subsys=daemon
level=info msg="  --enable-ipv6-masquerade='true'" subsys=daemon
level=info msg="  --enable-ipv6-ndp='false'" subsys=daemon
level=info msg="  --enable-k8s='true'" subsys=daemon
level=info msg="  --enable-k8s-api-discovery='false'" subsys=daemon
level=info msg="  --enable-k8s-endpoint-slice='true'" subsys=daemon
level=info msg="  --enable-k8s-event-handover='false'" subsys=daemon
level=info msg="  --enable-k8s-networkpolicy='true'" subsys=daemon
level=info msg="  --enable-k8s-terminating-endpoint='true'" subsys=daemon
level=info msg="  --enable-l2-neigh-discovery='true'" subsys=daemon
level=info msg="  --enable-l7-proxy='true'" subsys=daemon
level=info msg="  --enable-local-node-route='true'" subsys=daemon
level=info msg="  --enable-local-redirect-policy='false'" subsys=daemon
level=info msg="  --enable-mke='false'" subsys=daemon
level=info msg="  --enable-monitor='true'" subsys=daemon
level=info msg="  --enable-nat46x64-gateway='false'" subsys=daemon
level=info msg="  --enable-node-port='false'" subsys=daemon
level=info msg="  --enable-pmtu-discovery='false'" subsys=daemon
level=info msg="  --enable-policy='default'" subsys=daemon
level=info msg="  --enable-recorder='false'" subsys=daemon
level=info msg="  --enable-remote-node-identity='true'" subsys=daemon
level=info msg="  --enable-runtime-device-detection='false'" subsys=daemon
level=info msg="  --enable-sctp='false'" subsys=daemon
level=info msg="  --enable-service-topology='false'" subsys=daemon
level=info msg="  --enable-session-affinity='false'" subsys=daemon
level=info msg="  --enable-srv6='false'" subsys=daemon
level=info msg="  --enable-stale-cilium-endpoint-cleanup='true'" subsys=daemon
level=info msg="  --enable-svc-source-range-check='true'" subsys=daemon
level=info msg="  --enable-tracing='false'" subsys=daemon
level=info msg="  --enable-unreachable-routes='false'" subsys=daemon
level=info msg="  --enable-vtep='false'" subsys=daemon
level=info msg="  --enable-well-known-identities='false'" subsys=daemon
level=info msg="  --enable-wireguard='false'" subsys=daemon
level=info msg="  --enable-wireguard-userspace-fallback='false'" subsys=daemon
level=info msg="  --enable-xdp-prefilter='false'" subsys=daemon
level=info msg="  --enable-xt-socket-fallback='true'" subsys=daemon
level=info msg="  --encrypt-interface=''" subsys=daemon
level=info msg="  --encrypt-node='false'" subsys=daemon
level=info msg="  --endpoint-gc-interval='5m0s'" subsys=daemon
level=info msg="  --endpoint-queue-size='25'" subsys=daemon
level=info msg="  --endpoint-status=''" subsys=daemon
level=info msg="  --envoy-config-timeout='2m0s'" subsys=daemon
level=info msg="  --envoy-log=''" subsys=daemon
level=info msg="  --exclude-local-address=''" subsys=daemon
level=info msg="  --fixed-identity-mapping=''" subsys=daemon
level=info msg="  --fqdn-regex-compile-lru-size='1024'" subsys=daemon
level=info msg="  --gops-port='9890'" subsys=daemon
level=info msg="  --http-403-msg=''" subsys=daemon
level=info msg="  --http-idle-timeout='0'" subsys=daemon
level=info msg="  --http-max-grpc-timeout='0'" subsys=daemon
level=info msg="  --http-normalize-path='true'" subsys=daemon
level=info msg="  --http-request-timeout='3600'" subsys=daemon
level=info msg="  --http-retry-count='3'" subsys=daemon
level=info msg="  --http-retry-timeout='0'" subsys=daemon
level=info msg="  --hubble-disable-tls='false'" subsys=daemon
level=info msg="  --hubble-event-buffer-capacity='4095'" subsys=daemon
level=info msg="  --hubble-event-queue-size='0'" subsys=daemon
level=info msg="  --hubble-export-file-compress='false'" subsys=daemon
level=info msg="  --hubble-export-file-max-backups='5'" subsys=daemon
level=info msg="  --hubble-export-file-max-size-mb='10'" subsys=daemon
level=info msg="  --hubble-export-file-path=''" subsys=daemon
level=info msg="  --hubble-listen-address=':4244'" subsys=daemon
level=info msg="  --hubble-metrics=''" subsys=daemon
level=info msg="  --hubble-metrics-server=''" subsys=daemon
level=info msg="  --hubble-monitor-events=''" subsys=daemon
level=info msg="  --hubble-prefer-ipv6='false'" subsys=daemon
level=info msg="  --hubble-recorder-sink-queue-size='1024'" subsys=daemon
level=info msg="  --hubble-recorder-storage-path='/var/run/cilium/pcaps'" subsys=daemon
level=info msg="  --hubble-skip-unknown-cgroup-ids='true'" subsys=daemon
level=info msg="  --hubble-socket-path='/var/run/cilium/hubble.sock'" subsys=daemon
level=info msg="  --hubble-tls-cert-file='/var/lib/cilium/tls/hubble/server.crt'" subsys=daemon
level=info msg="  --hubble-tls-client-ca-files='/var/lib/cilium/tls/hubble/client-ca.crt'" subsys=daemon
level=info msg="  --hubble-tls-key-file='/var/lib/cilium/tls/hubble/server.key'" subsys=daemon
level=info msg="  --identity-allocation-mode='crd'" subsys=daemon
level=info msg="  --identity-change-grace-period='5s'" subsys=daemon
level=info msg="  --identity-gc-interval='15m0s'" subsys=daemon
level=info msg="  --identity-heartbeat-timeout='30m0s'" subsys=daemon
level=info msg="  --identity-restore-grace-period='10m0s'" subsys=daemon
level=info msg="  --install-egress-gateway-routes='false'" subsys=daemon
level=info msg="  --install-iptables-rules='true'" subsys=daemon
level=info msg="  --install-no-conntrack-iptables-rules='false'" subsys=daemon
level=info msg="  --ip-allocation-timeout='2m0s'" subsys=daemon
level=info msg="  --ip-masq-agent-config-path='/etc/config/ip-masq-agent'" subsys=daemon
level=info msg="  --ipam='kubernetes'" subsys=daemon
level=info msg="  --ipam-cilium-node-update-rate='15s'" subsys=daemon
level=info msg="  --ipsec-key-file=''" subsys=daemon
level=info msg="  --ipsec-key-rotation-duration='5m0s'" subsys=daemon
level=info msg="  --iptables-lock-timeout='5s'" subsys=daemon
level=info msg="  --iptables-random-fully='false'" subsys=daemon
level=info msg="  --ipv4-native-routing-cidr='10.0.0.0/24'" subsys=daemon
level=info msg="  --ipv4-node='auto'" subsys=daemon
level=info msg="  --ipv4-pod-subnets=''" subsys=daemon
level=info msg="  --ipv4-range='auto'" subsys=daemon
level=info msg="  --ipv4-service-loopback-address='169.254.42.1'" subsys=daemon
level=info msg="  --ipv4-service-range='auto'" subsys=daemon
level=info msg="  --ipv6-cluster-alloc-cidr='f00d::/64'" subsys=daemon
level=info msg="  --ipv6-mcast-device=''" subsys=daemon
level=info msg="  --ipv6-native-routing-cidr=''" subsys=daemon
level=info msg="  --ipv6-node='auto'" subsys=daemon
level=info msg="  --ipv6-pod-subnets=''" subsys=daemon
level=info msg="  --ipv6-range='auto'" subsys=daemon
level=info msg="  --ipv6-service-range='auto'" subsys=daemon
level=info msg="  --join-cluster='false'" subsys=daemon
level=info msg="  --k8s-api-server=''" subsys=daemon
level=info msg="  --k8s-client-burst='0'" subsys=daemon
level=info msg="  --k8s-client-qps='0'" subsys=daemon
level=info msg="  --k8s-heartbeat-timeout='30s'" subsys=daemon
level=info msg="  --k8s-kubeconfig-path=''" subsys=daemon
level=info msg="  --k8s-namespace='kube-system'" subsys=daemon
level=info msg="  --k8s-require-ipv4-pod-cidr='false'" subsys=daemon
level=info msg="  --k8s-require-ipv6-pod-cidr='false'" subsys=daemon
level=info msg="  --k8s-service-cache-size='128'" subsys=daemon
level=info msg="  --k8s-service-proxy-name=''" subsys=daemon
level=info msg="  --k8s-sync-timeout='3m0s'" subsys=daemon
level=info msg="  --k8s-watcher-endpoint-selector='metadata.name!=kube-scheduler,metadata.name!=kube-controller-manager,metadata.name!=etcd-operator,metadata.name!=gcp-controller-manager'" subsys=daemon
level=info msg="  --keep-config='false'" subsys=daemon
level=info msg="  --kube-proxy-replacement='strict'" subsys=daemon
level=info msg="  --kube-proxy-replacement-healthz-bind-address=''" subsys=daemon
level=info msg="  --kvstore=''" subsys=daemon
level=info msg="  --kvstore-connectivity-timeout='2m0s'" subsys=daemon
level=info msg="  --kvstore-lease-ttl='15m0s'" subsys=daemon
level=info msg="  --kvstore-max-consecutive-quorum-errors='2'" subsys=daemon
level=info msg="  --kvstore-opt=''" subsys=daemon
level=info msg="  --kvstore-periodic-sync='5m0s'" subsys=daemon
level=info msg="  --label-prefix-file=''" subsys=daemon
level=info msg="  --labels=''" subsys=daemon
level=info msg="  --lib-dir='/var/lib/cilium'" subsys=daemon
level=info msg="  --local-max-addr-scope='252'" subsys=daemon
level=info msg="  --local-router-ipv4=''" subsys=daemon
level=info msg="  --local-router-ipv6=''" subsys=daemon
level=info msg="  --log-driver=''" subsys=daemon
level=info msg="  --log-opt=''" subsys=daemon
level=info msg="  --log-system-load='false'" subsys=daemon
level=info msg="  --max-controller-interval='0'" subsys=daemon
level=info msg="  --mesh-auth-monitor-queue-size='1024'" subsys=daemon
level=info msg="  --mesh-auth-mtls-listener-port='0'" subsys=daemon
level=info msg="  --mesh-auth-rotated-identities-queue-size='1024'" subsys=daemon
level=info msg="  --mesh-auth-spiffe-trust-domain='spiffe.cilium'" subsys=daemon
level=info msg="  --mesh-auth-spire-admin-socket=''" subsys=daemon
level=info msg="  --metrics=''" subsys=daemon
level=info msg="  --mke-cgroup-mount=''" subsys=daemon
level=info msg="  --monitor-aggregation='none'" subsys=daemon
level=info msg="  --monitor-aggregation-flags='all'" subsys=daemon
level=info msg="  --monitor-aggregation-interval='5s'" subsys=daemon
level=info msg="  --monitor-queue-size='0'" subsys=daemon
level=info msg="  --mtu='0'" subsys=daemon
level=info msg="  --node-encryption-opt-out-labels='node-role.kubernetes.io/control-plane'" subsys=daemon
level=info msg="  --node-port-acceleration='disabled'" subsys=daemon
level=info msg="  --node-port-algorithm='random'" subsys=daemon
level=info msg="  --node-port-bind-protection='true'" subsys=daemon
level=info msg="  --node-port-mode='snat'" subsys=daemon
level=info msg="  --node-port-range='30000,32767'" subsys=daemon
level=info msg="  --nodes-gc-interval='5m0s'" subsys=daemon
level=info msg="  --operator-api-serve-addr='127.0.0.1:9234'" subsys=daemon
level=info msg="  --policy-audit-mode='false'" subsys=daemon
level=info msg="  --policy-queue-size='100'" subsys=daemon
level=info msg="  --policy-trigger-interval='1s'" subsys=daemon
level=info msg="  --pprof='false'" subsys=daemon
level=info msg="  --pprof-address='localhost'" subsys=daemon
level=info msg="  --pprof-port='6060'" subsys=daemon
level=info msg="  --preallocate-bpf-maps='false'" subsys=daemon
level=info msg="  --prepend-iptables-chains='true'" subsys=daemon
level=info msg="  --procfs='/host/proc'" subsys=daemon
level=info msg="  --prometheus-serve-addr=':9962'" subsys=daemon
level=info msg="  --proxy-connect-timeout='2'" subsys=daemon
level=info msg="  --proxy-gid='1337'" subsys=daemon
level=info msg="  --proxy-max-connection-duration-seconds='0'" subsys=daemon
level=info msg="  --proxy-max-requests-per-connection='0'" subsys=daemon
level=info msg="  --proxy-prometheus-port='0'" subsys=daemon
level=info msg="  --read-cni-conf=''" subsys=daemon
level=info msg="  --remove-cilium-node-taints='true'" subsys=daemon
level=info msg="  --restore='true'" subsys=daemon
level=info msg="  --route-metric='0'" subsys=daemon
level=info msg="  --routing-mode='native'" subsys=daemon
level=info msg="  --set-cilium-is-up-condition='true'" subsys=daemon
level=info msg="  --set-cilium-node-taints='true'" subsys=daemon
level=info msg="  --sidecar-istio-proxy-image='cilium/istio_proxy'" subsys=daemon
level=info msg="  --single-cluster-route='false'" subsys=daemon
level=info msg="  --skip-cnp-status-startup-clean='false'" subsys=daemon
level=info msg="  --socket-path='/var/run/cilium/cilium.sock'" subsys=daemon
level=info msg="  --srv6-encap-mode='reduced'" subsys=daemon
level=info msg="  --state-dir='/var/run/cilium'" subsys=daemon
level=info msg="  --synchronize-k8s-nodes='true'" subsys=daemon
level=info msg="  --tofqdns-dns-reject-response-code='refused'" subsys=daemon
level=info msg="  --tofqdns-enable-dns-compression='true'" subsys=daemon
level=info msg="  --tofqdns-endpoint-max-ip-per-hostname='50'" subsys=daemon
level=info msg="  --tofqdns-idle-connection-grace-period='0s'" subsys=daemon
level=info msg="  --tofqdns-max-deferred-connection-deletes='10000'" subsys=daemon
level=info msg="  --tofqdns-min-ttl='0'" subsys=daemon
level=info msg="  --tofqdns-pre-cache=''" subsys=daemon
level=info msg="  --tofqdns-proxy-port='0'" subsys=daemon
level=info msg="  --tofqdns-proxy-response-max-delay='100ms'" subsys=daemon
level=info msg="  --trace-payloadlen='128'" subsys=daemon
level=info msg="  --trace-sock='true'" subsys=daemon
level=info msg="  --tunnel=''" subsys=daemon
level=info msg="  --tunnel-port='0'" subsys=daemon
level=info msg="  --tunnel-protocol='vxlan'" subsys=daemon
level=info msg="  --unmanaged-pod-watcher-interval='15'" subsys=daemon
level=info msg="  --version='false'" subsys=daemon
level=info msg="  --vlan-bpf-bypass=''" subsys=daemon
level=info msg="  --vtep-cidr=''" subsys=daemon
level=info msg="  --vtep-endpoint=''" subsys=daemon
level=info msg="  --vtep-mac=''" subsys=daemon
level=info msg="  --vtep-mask=''" subsys=daemon
level=info msg="  --write-cni-conf-when-ready='/host/etc/cni/net.d/05-cilium.conflist'" subsys=daemon
level=debug msg="Enabling datapath debug messages" subsys=daemon
level=info msg="     _ _ _" subsys=daemon
level=info msg=" ___|_| |_|_ _ _____" subsys=daemon
level=info msg="|  _| | | | | |     |" subsys=daemon
level=info msg="|___|_|_|_|___|_|_|_|" subsys=daemon
level=info msg="Cilium 1.14.0-snapshot.2 623ad61749 2023-07-12T23:27:27+01:00 go version go1.20.3 linux/arm64" subsys=daemon
level=info msg="cilium-envoy  version: 3d6c80b860b9b39c616f835d9bdac281a81139ae/1.25.6/Distribution/RELEASE/BoringSSL" subsys=daemon
level=info msg="clang (10.0.0) and kernel (5.15.49) versions: OK!" subsys=linux-datapath
level=info msg="linking environment: OK!" subsys=linux-datapath
level=info msg="Detected mounted BPF filesystem at /sys/fs/bpf" subsys=bpf
level=info msg="Mounted cgroupv2 filesystem at /run/cilium/cgroupv2" subsys=cgroups
level=info msg="Parsing base label prefixes from default label list" subsys=labels-filter
level=info msg="Parsing additional label prefixes from user inputs: []" subsys=labels-filter
level=info msg="Final label prefixes to be used for identity evaluation:" subsys=labels-filter
level=info msg=" - reserved:.*" subsys=labels-filter
level=info msg=" - :io\\.kubernetes\\.pod\\.namespace" subsys=labels-filter
level=info msg=" - :io\\.cilium\\.k8s\\.namespace\\.labels" subsys=labels-filter
level=info msg=" - :app\\.kubernetes\\.io" subsys=labels-filter
level=info msg=" - !:io\\.kubernetes" subsys=labels-filter
level=info msg=" - !:kubernetes\\.io" subsys=labels-filter
level=info msg=" - !:.*beta\\.kubernetes\\.io" subsys=labels-filter
level=info msg=" - !:k8s\\.io" subsys=labels-filter
level=info msg=" - !:pod-template-generation" subsys=labels-filter
level=info msg=" - !:pod-template-hash" subsys=labels-filter
level=info msg=" - !:controller-revision-hash" subsys=labels-filter
level=info msg=" - !:annotation.*" subsys=labels-filter
level=info msg=" - !:etcd_node" subsys=labels-filter
level=info msg="Auto-disabling \"enable-bpf-clock-probe\" feature since KERNEL_HZ cannot be determined" error="open /proc/schedstat: no such file or directory" subsys=daemon
level=info msg="Using autogenerated IPv4 allocation range" subsys=node v4Prefix=10.3.0.0/16
level=debug msg=Invoking function="pprof.glob..func1 (cell.go:51)" subsys=hive
level=info msg=Invoked duration="652.5µs" function="pprof.glob..func1 (cell.go:51)" subsys=hive
level=debug msg=Invoking function="gops.registerGopsHooks (cell.go:39)" subsys=hive
level=info msg=Invoked duration="51.709µs" function="gops.registerGopsHooks (cell.go:39)" subsys=hive
level=debug msg=Invoking function="cmd.glob..func3 (daemon_main.go:1581)" subsys=hive
level=debug msg="getting identity cache for identity allocator manager" subsys=identity-cache
level=info msg="Spire Delegate API Client is disabled as no socket path is configured" subsys=spire-delegate
level=info msg="mTLS Auth Handler is disabled as no port is configured" subsys=auth-manager
level=info msg=Invoked duration=106.496209ms function="cmd.glob..func3 (daemon_main.go:1581)" subsys=hive
level=debug msg=Invoking function="utime.initUtimeSync (cell.go:30)" subsys=hive
level=info msg=Invoked duration="48.625µs" function="utime.initUtimeSync (cell.go:30)" subsys=hive
level=info msg=Starting subsys=hive
level=debug msg="Executing start hook" function="gops.registerGopsHooks.func1 (cell.go:44)" subsys=hive
level=info msg="Started gops server" address="127.0.0.1:9890" subsys=gops
level=info msg="Start hook executed" duration="537.459µs" function="gops.registerGopsHooks.func1 (cell.go:44)" subsys=hive
level=debug msg="Executing start hook" function="client.(*compositeClientset).onStart" subsys=hive
level=info msg="Establishing connection to apiserver" host="https://10.96.0.1:443" subsys=k8s-client
level=info msg="Connected to apiserver" subsys=k8s-client
level=debug msg="Starting new controller" name=k8s-heartbeat subsys=controller uuid=e0350092-266e-4aa5-be07-c2359f111b76
level=debug msg="Skipping Leases support fallback discovery" subsys=k8s
level=info msg="Start hook executed" duration=51.121417ms function="client.(*compositeClientset).onStart" subsys=hive
level=debug msg="Executing start hook" function="authmap.newAuthMap.func1 (cell.go:28)" subsys=hive
level=debug msg="Controller func execution time: 6.301833ms" name=k8s-heartbeat subsys=controller uuid=e0350092-266e-4aa5-be07-c2359f111b76
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_auth_map subsys=ebpf
level=info msg="Start hook executed" duration=11.588584ms function="authmap.newAuthMap.func1 (cell.go:28)" subsys=hive
level=debug msg="Executing start hook" function="configmap.newMap.func1 (cell.go:24)" subsys=hive
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_runtime_config subsys=bpf
level=info msg="Start hook executed" duration="254.459µs" function="configmap.newMap.func1 (cell.go:24)" subsys=hive
level=debug msg="Executing start hook" function="datapath.newDatapath.func1 (cells.go:85)" subsys=hive
level=info msg="Start hook executed" duration=73.34825ms function="datapath.newDatapath.func1 (cells.go:85)" subsys=hive
level=debug msg="Executing start hook" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration=5.354916ms function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=debug msg="Executing start hook" function="*gobgp.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=info msg="Start hook executed" duration=200.861917ms function="*gobgp.diffStore[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Service].Start" subsys=hive
level=debug msg="Executing start hook" function="*resource.resource[*k8s.io/api/core/v1.Node].Start" subsys=hive
level=info msg="Start hook executed" duration="2.292µs" function="*resource.resource[*k8s.io/api/core/v1.Node].Start" subsys=hive
level=debug msg="Executing start hook" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Start" subsys=hive
level=info msg="Start hook executed" duration=833ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNode].Start" subsys=hive
level=debug msg="Executing start hook" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Start" subsys=hive
level=info msg="Start hook executed" duration=625ns function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/slim/k8s/api/core/v1.Namespace].Start" subsys=hive
level=debug msg="Executing start hook" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=info msg="Start hook executed" duration="21.541µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumLoadBalancerIPPool].Start" subsys=hive
level=debug msg="Executing start hook" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Start" subsys=hive
level=info msg="Start hook executed" duration="1.917µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumIdentity].Start" subsys=hive
level=debug msg="Executing start hook" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="5.084µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumNetworkPolicy].Start" subsys=hive
level=debug msg="Executing start hook" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Start" subsys=hive
level=info msg="Start hook executed" duration="5.75µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2.CiliumClusterwideNetworkPolicy].Start" subsys=hive
level=debug msg="Executing start hook" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Start" subsys=hive
level=info msg="Start hook executed" duration="71.042µs" function="*resource.resource[*github.com/cilium/cilium/pkg/k8s/apis/cilium.io/v2alpha1.CiliumCIDRGroup].Start" subsys=hive
level=debug msg="Executing start hook" function="endpointmanager.newDefaultEndpointManager.func1 (cell.go:185)" subsys=hive
level=debug msg="Starting new controller" name=endpoint-gc subsys=controller uuid=fc9299b1-404f-435c-bbc9-2f0b15ac4c21
level=info msg="Start hook executed" duration="71.583µs" function="endpointmanager.newDefaultEndpointManager.func1 (cell.go:185)" subsys=hive
level=debug msg="Executing start hook" function="cmd.newPolicyTrifecta.func1 (policy.go:129)" subsys=hive
level=debug msg="Controller func execution time: 2.291µs" name=endpoint-gc subsys=controller uuid=fc9299b1-404f-435c-bbc9-2f0b15ac4c21
level=debug msg="creating new EventQueue" name=repository-change-queue numBufferedEvents=100 subsys=eventqueue
level=debug msg="creating new EventQueue" name=repository-reaction-queue numBufferedEvents=100 subsys=eventqueue
level=info msg="Start hook executed" duration="295.542µs" function="cmd.newPolicyTrifecta.func1 (policy.go:129)" subsys=hive
level=debug msg="Executing start hook" function="*manager.manager.Start" subsys=hive
level=debug msg="Performing regular background work" subsys=nodemanager syncInterval=1m0s
level=info msg="Start hook executed" duration="141.959µs" function="*manager.manager.Start" subsys=hive
level=debug msg="Executing start hook" function="monitor.(*dropMonitor).OnStart" subsys=hive
level=info msg="Start hook executed" duration="859.291µs" function="monitor.(*dropMonitor).OnStart" subsys=hive
level=debug msg="Executing start hook" function="*cni.cniConfigManager.Start" subsys=hive
level=debug msg="Starting new controller" name=write-cni-file subsys=controller uuid=1d0eb29d-a1a6-48e7-add5-02a2ebb7bd61
level=info msg="Start hook executed" duration="669.458µs" function="*cni.cniConfigManager.Start" subsys=hive
level=debug msg="Executing start hook" function="cmd.newDaemonPromise.func1 (daemon_main.go:1629)" subsys=hive
level=info msg="Generating CNI configuration file with mode none" subsys=cni-config
level=info msg="Auto-enabling \"enable-node-port\", \"enable-external-ips\", \"bpf-lb-sock\", \"enable-host-port\", \"enable-session-affinity\" features" subsys=daemon
level=info msg="Activity in /host/etc/cni/net.d/, re-generating CNI configuration" subsys=cni-config
level=info msg="Activity in /host/etc/cni/net.d/, re-generating CNI configuration" subsys=cni-config
level=info msg="Activity in /host/etc/cni/net.d/, re-generating CNI configuration" subsys=cni-config
level=info msg="Activity in /host/etc/cni/net.d/, re-generating CNI configuration" subsys=cni-config
level=info msg="Activity in /host/etc/cni/net.d/, re-generating CNI configuration" subsys=cni-config
level=info msg="Activity in /host/etc/cni/net.d/, re-generating CNI configuration" subsys=cni-config
level=info msg="Wrote CNI configuration file to /host/etc/cni/net.d/05-cilium.conflist" subsys=cni-config
level=debug msg="Controller func execution time: 5.306334ms" name=write-cni-file subsys=controller uuid=1d0eb29d-a1a6-48e7-add5-02a2ebb7bd61
level=debug msg="Controller run succeeded; waiting for next controller update or stop" name=write-cni-file subsys=controller uuid=1d0eb29d-a1a6-48e7-add5-02a2ebb7bd61
level=info msg="Generating CNI configuration file with mode none" subsys=cni-config
level=debug msg="Existing CNI configuration file /host/etc/cni/net.d/05-cilium.conflist unchanged" subsys=cni-config
level=debug msg="Controller func execution time: 135µs" name=write-cni-file subsys=controller uuid=1d0eb29d-a1a6-48e7-add5-02a2ebb7bd61
level=debug msg="Controller run succeeded; waiting for next controller update or stop" name=write-cni-file subsys=controller uuid=1d0eb29d-a1a6-48e7-add5-02a2ebb7bd61
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_test subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_test subsys=bpf
level=debug msg="enabling events buffer" file-path= name=cilium_lb4_services_v2 size=128 subsys=bpf ttl=0s
level=debug msg="enabling events buffer" file-path= name=cilium_lb4_backends_v2 size=128 subsys=bpf ttl=0s
level=debug msg="enabling events buffer" file-path= name=cilium_lb4_backends_v3 size=128 subsys=bpf ttl=0s
level=debug msg="enabling events buffer" file-path= name=cilium_lb4_reverse_nat size=128 subsys=bpf ttl=0s
level=debug msg="enabling events buffer" file-path= name=cilium_lb4_crab size=128 subsys=bpf ttl=0s
level=debug msg="enabling events buffer" file-path= name=cilium_lb4_crab_long size=128 subsys=bpf ttl=0s
level=debug msg="enabling events buffer" file-path= name=cilium_lb_affinity_match size=128 subsys=bpf ttl=0s
level=debug msg="enabling events buffer" file-path= name=cilium_lb4_source_range size=128 subsys=bpf ttl=0s
level=info msg="Inheriting MTU from external network interface" device=eth0 ipAddr=172.26.0.3 mtu=1500 subsys=mtu
level=debug msg="enabling events buffer" file-path= name=cilium_ipcache size=1024 subsys=bpf ttl=0s
level=debug msg="creating new EventQueue" name=config-modify-queue numBufferedEvents=10 subsys=eventqueue
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ipcache subsys=bpf
level=info msg="Envoy: Starting xDS gRPC server listening on /var/run/cilium/xds.sock" subsys=envoy-manager
level=debug msg="enabling events buffer" file-path= name=cilium_lxc size=128 subsys=bpf ttl=0s
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_lxc subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ipcache subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_node_map subsys=ebpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_metrics subsys=ebpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_lb4_reverse_sk subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_lb4_reverse_sk subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_lb4_services_v2 subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_lb4_backends_v3 subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_lb4_reverse_nat subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_lb4_crab subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_lb4_crab_long subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_events subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_events subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_signals subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_signals subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_call_policy subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_call_policy subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct4_global subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct4_global subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct_any4_global subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct_any4_global subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_nodeport_neigh4 subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_nodeport_neigh4 subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ipv4_frag_datagrams subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ipv4_frag_datagrams subsys=bpf
level=debug msg="Starting new controller" name=metricsmap-bpf-prom-sync subsys=controller uuid=5e26b3a0-3934-459d-a21b-050456120824
level=debug msg="Controller func execution time: 22.458µs" name=metricsmap-bpf-prom-sync subsys=controller uuid=5e26b3a0-3934-459d-a21b-050456120824
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_lb_affinity_match subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_lb4_affinity subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_lb4_source_range subsys=bpf
level=info msg="Restored 0 node IDs from the BPF map" subsys=linux-datapath
level=info msg="Restored services from maps" failedServices=0 restoredServices=0 subsys=service
level=info msg="Restored backends from maps" failedBackends=0 restoredBackends=0 skippedBackends=0 subsys=service
level=info msg="Reading old endpoints..." subsys=daemon
level=info msg="No old endpoints found." subsys=daemon
level=debug msg="Starting new controller" name=dns-garbage-collector-job subsys=controller uuid=d4df90f4-51e5-4ae6-a3c8-7dd8e4b186f7
level=debug msg="Running 'iptables -t mangle -n -L CILIUM_PRE_mangle' command" subsys=iptables
level=debug msg="Controller func execution time: 2.459µs" name=dns-garbage-collector-job subsys=controller uuid=d4df90f4-51e5-4ae6-a3c8-7dd8e4b186f7
level=debug msg="DNS Proxy port is configured to 0. A random port will be assigned by the OS." subsys=fqdn/dnsproxy
level=debug msg="DNS Proxy bound to address" address="[::]:38779" subsys=fqdn/dnsproxy
level=info msg="Waiting until all Cilium CRDs are available" subsys=k8s
level=debug msg="waiting for cache to synchronize" kubernetesResource="crd:ciliumcidrgroups.cilium.io" subsys=k8s
level=debug msg="waiting for cache to synchronize" kubernetesResource="crd:ciliumnodes.cilium.io" subsys=k8s
level=debug msg="waiting for cache to synchronize" kubernetesResource="crd:ciliumnodeconfigs.cilium.io" subsys=k8s
level=debug msg="waiting for cache to synchronize" kubernetesResource="crd:ciliumidentities.cilium.io" subsys=k8s
level=debug msg="waiting for cache to synchronize" kubernetesResource="crd:ciliumloadbalancerippools.cilium.io" subsys=k8s
level=debug msg="waiting for cache to synchronize" kubernetesResource="crd:ciliumendpoints.cilium.io" subsys=k8s
level=debug msg="waiting for cache to synchronize" kubernetesResource="crd:ciliumnetworkpolicies.cilium.io" subsys=k8s
level=debug msg="waiting for cache to synchronize" kubernetesResource="crd:ciliumclusterwidenetworkpolicies.cilium.io" subsys=k8s
level=debug msg="cache synced" kubernetesResource="crd:ciliumnodes.cilium.io" subsys=k8s
level=debug msg="cache synced" kubernetesResource="crd:ciliumnodeconfigs.cilium.io" subsys=k8s
level=debug msg="cache synced" kubernetesResource="crd:ciliumcidrgroups.cilium.io" subsys=k8s
level=debug msg="cache synced" kubernetesResource="crd:ciliumloadbalancerippools.cilium.io" subsys=k8s
level=debug msg="cache synced" kubernetesResource="crd:ciliumidentities.cilium.io" subsys=k8s
level=debug msg="cache synced" kubernetesResource="crd:ciliumendpoints.cilium.io" subsys=k8s
level=debug msg="cache synced" kubernetesResource="crd:ciliumclusterwidenetworkpolicies.cilium.io" subsys=k8s
level=debug msg="cache synced" kubernetesResource="crd:ciliumnetworkpolicies.cilium.io" subsys=k8s
level=info msg="All Cilium CRDs have been found and are available" subsys=k8s
level=debug msg="waiting for cache to synchronize" kubernetesResource="core/v1::Node" subsys=k8s
level=debug msg="cache synced" kubernetesResource="core/v1::Node" subsys=k8s
level=info msg="Retrieved node information from kubernetes node" nodeName=kind-worker3 subsys=k8s
level=info msg="Received own node information from API server" ipAddr.ipv4=172.26.0.3 ipAddr.ipv6="fc00:c111::3" k8sNodeIP=172.26.0.3 labels="map[beta.kubernetes.io/arch:arm64 beta.kubernetes.io/os:linux kubernetes.io/arch:arm64 kubernetes.io/hostname:kind-worker3 kubernetes.io/os:linux]" nodeName=kind-worker3 subsys=k8s v4Prefix=10.244.3.0/24 v6Prefix="fd00:10:244:3::/64"
level=info msg="k8s mode: Allowing localhost to reach local endpoints" subsys=daemon
level=info msg="Direct routing device detected" direct-routing-device=eth0 subsys=linux-datapath
level=info msg="Detected devices" devices="[eth0]" subsys=linux-datapath
level=info msg="Masquerading IP selected for device" device=eth0 ipv4=172.26.0.3 subsys=node
level=info msg="Enabling k8s event listener" subsys=k8s-watcher
level=debug msg="waiting for cache to synchronize" kubernetesResource="core/v1::Service" subsys=k8s
level=debug msg="waiting for cache to synchronize" kubernetesResource="core/v1::Namespace" subsys=k8s
level=debug msg="waiting for cache to synchronize" kubernetesResource="discovery/v1::EndpointSlice" subsys=k8s
level=debug msg="waiting for cache to synchronize" kubernetesResource="core/v1::Pods" subsys=k8s
level=debug msg="waiting for cache to synchronize" kubernetesResource="networking.k8s.io/v1::NetworkPolicy" subsys=k8s
level=debug msg="Processing 1 endpoints for EndpointSlice kubernetes" subsys=k8s
level=debug msg="EndpointSlice kubernetes has 1 backends" subsys=k8s
level=debug msg="Starting new controller" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Processing 0 endpoints for EndpointSlice kube-dns-6b8l9" subsys=k8s
level=debug msg="EndpointSlice kube-dns-6b8l9 has 0 backends" subsys=k8s
level=debug msg="Kubernetes service definition changed" action=service-updated endpoints="172.26.0.2:6443/TCP" k8sNamespace=default k8sSvcName=kubernetes old-service=nil service="frontends:[10.96.0.1]/ports=[https]/selector=map[]" subsys=k8s-watcher
level=debug msg="Upserting service" backends="[172.26.0.2:6443]" l7LBFrontendPorts="[]" l7LBProxyPort=0 loadBalancerSourceRanges="[]" serviceIP="{10.96.0.1 {TCP 443} 0}" serviceName=kubernetes serviceNamespace=default sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcExtTrafficPolicy=Cluster svcHealthCheckNodePort=0 svcIntTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Resolving service" l3n4Addr="{AddrCluster:10.96.0.1 L4Addr:{Protocol:TCP Port:443} Scope:0}" subsys=service
level=debug msg="Acquired service ID" backends="[172.26.0.2:6443]" l7LBFrontendPorts="[]" l7LBProxyPort=0 loadBalancerSourceRanges="[]" serviceID=1 serviceIP="{10.96.0.1 {TCP 443} 0}" serviceName=kubernetes serviceNamespace=default sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcExtTrafficPolicy=Cluster svcHealthCheckNodePort=0 svcIntTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Deleting backends from session affinity match" backends="[]" serviceID=1 subsys=service
level=debug msg="Adding new backend" backendID=1 backendWeight=100 backends="[172.26.0.2:6443]" l3n4Addr="{172.26.0.2 {TCP 6443} 0}" l7LBFrontendPorts="[]" l7LBProxyPort=0 loadBalancerSourceRanges="[]" serviceID=1 serviceIP="{10.96.0.1 {TCP 443} 0}" serviceName=kubernetes serviceNamespace=default sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcExtTrafficPolicy=Cluster svcHealthCheckNodePort=0 svcIntTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="waiting for cache to synchronize" kubernetesResource="cilium/v2::CiliumNode" subsys=k8s
level=info msg="Removing stale endpoint interfaces" subsys=daemon
level=info msg="Skipping kvstore configuration" subsys=daemon
level=info msg="Initializing node addressing" subsys=daemon
level=info msg="Initializing kubernetes IPAM" subsys=ipam v4Prefix=10.244.3.0/24 v6Prefix="<nil>"
level=debug msg="waiting for cache to synchronize" kubernetesResource="cilium/v2::CiliumNetworkPolicy" subsys=k8s
level=debug msg="waiting for cache to synchronize" kubernetesResource="cilium/v2::CiliumClusterwideNetworkPolicy" subsys=k8s
level=info msg="Restoring endpoints..." subsys=daemon
level=debug msg="waiting for cache to synchronize" kubernetesResource="cilium/v2alpha1::CiliumCIDRGroup" subsys=k8s
level=info msg="Waiting until local node addressing before starting watchers depending on it" subsys=k8s-watcher
level=debug msg="waiting for cache to synchronize" kubernetesResource="cilium/v2::CiliumEndpoint" subsys=k8s
level=debug msg="Resolving identity" identityLabels="cidr:172.26.0.2/32,reserved:kube-apiserver,reserved:world" subsys=identity-cache
level=debug msg="UpdateIdentities: Adding a new identity" identity=16777217 labels="[cidr:0.0.0.0/0 cidr:128.0.0.0/1 cidr:128.0.0.0/2 cidr:160.0.0.0/3 cidr:160.0.0.0/4 cidr:168.0.0.0/5 cidr:172.0.0.0/10 cidr:172.0.0.0/11 cidr:172.0.0.0/6 cidr:172.0.0.0/7 cidr:172.0.0.0/8 cidr:172.0.0.0/9 cidr:172.16.0.0/12 cidr:172.24.0.0/13 cidr:172.24.0.0/14 cidr:172.26.0.0/15 cidr:172.26.0.0/16 cidr:172.26.0.0/17 cidr:172.26.0.0/18 cidr:172.26.0.0/19 cidr:172.26.0.0/20 cidr:172.26.0.0/21 cidr:172.26.0.0/22 cidr:172.26.0.0/23 cidr:172.26.0.0/24 cidr:172.26.0.0/25 cidr:172.26.0.0/26 cidr:172.26.0.0/27 cidr:172.26.0.0/28 cidr:172.26.0.0/29 cidr:172.26.0.0/30 cidr:172.26.0.2/31 cidr:172.26.0.2/32 reserved:kube-apiserver reserved:world]" subsys=policy
level=debug msg="Waiting for proxy updates to complete..." subsys=endpoint-manager
level=debug msg="Wait time for proxy updates: 11.833µs" subsys=endpoint-manager
level=debug msg="Upserting IP into ipcache layer" identity="{16777217 kube-apiserver [] false true}" ipAddr=172.26.0.2/32 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity="{16777217 kube-apiserver [] false true}" ipAddr="{172.26.0.2 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Controller func execution time: 1.6815ms" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Controller run succeeded; waiting for next controller update or stop" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=info msg="Endpoints restored" failed=0 restored=0 subsys=daemon
level=debug msg="Allocated random IP" ip=10.244.3.223 owner=router pool=default subsys=ipam
level=info msg="Addressing information:" subsys=daemon
level=info msg="  Cluster-Name: kind-kind" subsys=daemon
level=info msg="  Cluster-ID: 0" subsys=daemon
level=info msg="  Local node-name: kind-worker3" subsys=daemon
level=info msg="  Node-IPv6: fc00:c111::3" subsys=daemon
level=info msg="  External-Node IPv4: 172.26.0.3" subsys=daemon
level=info msg="  Internal-Node IPv4: 10.244.3.223" subsys=daemon
level=info msg="  IPv4 allocation prefix: 10.244.3.0/24" subsys=daemon
level=info msg="  IPv4 native routing prefix: 10.0.0.0/24" subsys=daemon
level=info msg="  Loopback IPv4: 169.254.42.1" subsys=daemon
level=info msg="  Local IPv4 addresses:" subsys=daemon
level=info msg="  - 172.26.0.3" subsys=daemon
level=debug msg="Allocated random IP" ip=10.244.3.91 owner=health pool=default subsys=ipam
level=debug msg="IPv4 health endpoint address: 10.244.3.91" subsys=daemon
level=debug msg="Upserted service entry" backendSlot=1 subsys=map-lb svcKey="10.96.0.1:47873" svcVal="1 0 (256) [0x0 0x0]"
level=debug msg="Upserted service entry" backendSlot=0 subsys=map-lb svcKey="10.96.0.1:47873" svcVal="0 1 (256) [0x0 0x0]"
level=debug msg="Adding backends to affinity match map" backends="[]" serviceID=1 subsys=service
level=debug msg="Received node update event from local: types.Node{Name:\"kind-worker3\", Cluster:\"kind-kind\", IPAddresses:[]types.Address{types.Address{Type:\"InternalIP\", IP:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xac, 0x1a, 0x0, 0x3}}, types.Address{Type:\"InternalIP\", IP:net.IP{0xfc, 0x0, 0xc1, 0x11, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x3}}, types.Address{Type:\"CiliumInternalIP\", IP:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xa, 0xf4, 0x3, 0xdf}}}, IPv4AllocCIDR:(*cidr.CIDR)(0x40005b0348), IPv4SecondaryAllocCIDRs:[]*cidr.CIDR(nil), IPv6AllocCIDR:(*cidr.CIDR)(nil), IPv6SecondaryAllocCIDRs:[]*cidr.CIDR(nil), IPv4HealthIP:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xa, 0xf4, 0x3, 0x5b}, IPv6HealthIP:net.IP(nil), IPv4IngressIP:net.IP(nil), IPv6IngressIP:net.IP(nil), ClusterID:0x0, Source:\"local\", EncryptionKey:0x0, Labels:map[string]string{\"beta.kubernetes.io/arch\":\"arm64\", \"beta.kubernetes.io/os\":\"linux\", \"kubernetes.io/arch\":\"arm64\", \"kubernetes.io/hostname\":\"kind-worker3\", \"kubernetes.io/os\":\"linux\"}, Annotations:map[string]string(nil), NodeIdentity:0x1, WireguardPubKey:\"\"}" subsys=nodemanager
level=debug msg="UpdateIdentities: Skipping add of an existing identical identity" identity=16777217 subsys=policy
level=debug msg="Regenerating all endpoints" subsys=policy
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=debug msg="Kubernetes service definition changed" action=service-updated endpoints= k8sNamespace=kube-system k8sSvcName=kube-dns old-service=nil service="frontends:[10.96.0.10]/ports=[dns-tcp metrics dns]/selector=map[k8s-app:kube-dns]" subsys=k8s-watcher
level=debug msg="Upserting service" backends="[]" l7LBFrontendPorts="[]" l7LBProxyPort=0 loadBalancerSourceRanges="[]" serviceIP="{10.96.0.10 {UDP 53} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcExtTrafficPolicy=Cluster svcHealthCheckNodePort=0 svcIntTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Resolving service" l3n4Addr="{AddrCluster:10.96.0.10 L4Addr:{Protocol:UDP Port:53} Scope:0}" subsys=service
level=debug msg="Acquired service ID" backends="[]" l7LBFrontendPorts="[]" l7LBProxyPort=0 loadBalancerSourceRanges="[]" serviceID=2 serviceIP="{10.96.0.10 {UDP 53} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcExtTrafficPolicy=Cluster svcHealthCheckNodePort=0 svcIntTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Deleting backends from session affinity match" backends="[]" serviceID=2 subsys=service
level=debug msg="Upserted service entry" backendSlot=0 subsys=map-lb svcKey="10.96.0.10:13568" svcVal="0 0 (512) [0x0 0x0]"
level=debug msg="Adding backends to affinity match map" backends="[]" serviceID=2 subsys=service
level=debug msg="Upserting service" backends="[]" l7LBFrontendPorts="[]" l7LBProxyPort=0 loadBalancerSourceRanges="[]" serviceIP="{10.96.0.10 {TCP 9153} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcExtTrafficPolicy=Cluster svcHealthCheckNodePort=0 svcIntTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Resolving service" l3n4Addr="{AddrCluster:10.96.0.10 L4Addr:{Protocol:TCP Port:9153} Scope:0}" subsys=service
level=debug msg="Acquired service ID" backends="[]" l7LBFrontendPorts="[]" l7LBProxyPort=0 loadBalancerSourceRanges="[]" serviceID=3 serviceIP="{10.96.0.10 {TCP 9153} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcExtTrafficPolicy=Cluster svcHealthCheckNodePort=0 svcIntTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Deleting backends from session affinity match" backends="[]" serviceID=3 subsys=service
level=debug msg="Upserted service entry" backendSlot=0 subsys=map-lb svcKey="10.96.0.10:49443" svcVal="0 0 (768) [0x0 0x0]"
level=debug msg="Adding backends to affinity match map" backends="[]" serviceID=3 subsys=service
level=info msg="Adding local node to cluster" node="{kind-worker3 kind-kind [{InternalIP 172.26.0.3} {InternalIP fc00:c111::3} {CiliumInternalIP 10.244.3.223}] 10.244.3.0/24 [] <nil> [] 10.244.3.91 <nil> <nil> <nil> 0 local 0 map[beta.kubernetes.io/arch:arm64 beta.kubernetes.io/os:linux kubernetes.io/arch:arm64 kubernetes.io/hostname:kind-worker3 kubernetes.io/os:linux] map[] 1 }" subsys=nodediscovery
level=debug msg="Upserting IP into ipcache layer" identity="{host local [] false false}" ipAddr=172.26.0.3/32 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity="{host local [] false false}" ipAddr="{172.26.0.3 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Updating existing controller" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Controller update time: 5.208µs" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Upserting IP into ipcache layer" identity="{host local [] false false}" ipAddr="fc00:c111::3/128" key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity="{host local [] false false}" ipAddr="{fc00:c111::3 ffffffffffffffffffffffffffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Updating existing controller" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Controller update time: 3.875µs" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Upserting IP into ipcache layer" identity="{host local [] false false}" ipAddr=10.244.3.223/32 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity="{host local [] false false}" ipAddr="{10.244.3.223 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Updating existing controller" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Controller update time: 3.833µs" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Upserting IP into ipcache layer" identity="{health local [] false false}" ipAddr=10.244.3.91 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity="{health local [] false false}" ipAddr="{10.244.3.91 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=info msg="Creating or updating CiliumNode resource" node=kind-worker3 subsys=nodediscovery
level=debug msg="Controller func execution time: 14.542µs" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Controller run succeeded; waiting for next controller update or stop" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=info msg="Waiting until all pre-existing resources have been received" subsys=k8s-watcher
level=debug msg="Controller func execution time: 500ns" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Controller run succeeded; waiting for next controller update or stop" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="core/v1::Node" subsys=k8s
level=debug msg="resource \"core/v1::Node\" cache has synced, stopping timeout watcher" subsys=k8s
level=debug msg="resource \"discovery/v1beta1::EndpointSlice\" cache has synced, stopping timeout watcher" subsys=k8s
level=debug msg="resource \"core/v1::Endpoint\" cache has synced, stopping timeout watcher" subsys=k8s
level=warning msg="Unable to get node resource" error="ciliumnodes.cilium.io \"kind-worker3\" not found" subsys=nodediscovery
level=debug msg="Received node update event from custom-resource: types.Node{Name:\"kind-worker2\", Cluster:\"kind-kind\", IPAddresses:[]types.Address{types.Address{Type:\"InternalIP\", IP:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xac, 0x1a, 0x0, 0x4}}, types.Address{Type:\"InternalIP\", IP:net.IP{0xfc, 0x0, 0xc1, 0x11, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x4}}, types.Address{Type:\"CiliumInternalIP\", IP:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xa, 0xf4, 0x1, 0xf2}}}, IPv4AllocCIDR:(*cidr.CIDR)(0x40005b0668), IPv4SecondaryAllocCIDRs:[]*cidr.CIDR(nil), IPv6AllocCIDR:(*cidr.CIDR)(nil), IPv6SecondaryAllocCIDRs:[]*cidr.CIDR(nil), IPv4HealthIP:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xa, 0xf4, 0x1, 0x12}, IPv6HealthIP:net.IP(nil), IPv4IngressIP:net.IP(nil), IPv6IngressIP:net.IP(nil), ClusterID:0x0, Source:\"custom-resource\", EncryptionKey:0x0, Labels:map[string]string{\"beta.kubernetes.io/arch\":\"arm64\", \"beta.kubernetes.io/os\":\"linux\", \"kubernetes.io/arch\":\"arm64\", \"kubernetes.io/hostname\":\"kind-worker2\", \"kubernetes.io/os\":\"linux\"}, Annotations:map[string]string(nil), NodeIdentity:0x0, WireguardPubKey:\"\"}" subsys=nodemanager
level=debug msg="Upserting IP into ipcache layer" identity="{remote-node custom-resource [] false false}" ipAddr=172.26.0.4/32 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity="{remote-node custom-resource [] false false}" ipAddr="{172.26.0.4 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Updating existing controller" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Controller update time: 7.5µs" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Upserting IP into ipcache layer" identity="{remote-node custom-resource [] false false}" ipAddr="fc00:c111::4/128" key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity="{remote-node custom-resource [] false false}" ipAddr="{fc00:c111::4 ffffffffffffffffffffffffffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Updating existing controller" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Controller update time: 3.083µs" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Upserting IP into ipcache layer" identity="{remote-node custom-resource [] false false}" ipAddr=10.244.1.242/32 key=0 subsys=ipcache
level=debug msg="Allocated new node ID for node IP address" ipAddr=172.26.0.4 nodeID=385 subsys=linux-datapath
level=debug msg="Skip pod event using host networking" hostIP=172.26.0.3 k8sNamespace=kube-system k8sPodName=kube-proxy-kfqxt podIP=172.26.0.3 podIPs="[{172.26.0.3} {fc00:c111::3}]" subsys=k8s-watcher
level=debug msg="Skip pod event using host networking" hostIP=172.26.0.3 k8sNamespace=kube-system k8sPodName=cilium-4sh4b podIP=172.26.0.3 podIPs="[{172.26.0.3} {fc00:c111::3}]" subsys=k8s-watcher
level=debug msg="Daemon notified of IP-Identity cache state change" identity="{remote-node custom-resource [] false false}" ipAddr="{10.244.1.242 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Resolving identity" identityLabels="reserved:remote-node" subsys=identity-cache
level=debug msg="Resolved reserved identity" identity=remote-node identityLabels="reserved:remote-node" isNew=false subsys=identity-cache
level=debug msg="Resolving identity" identityLabels="reserved:remote-node" subsys=identity-cache
level=debug msg="Resolved reserved identity" identity=remote-node identityLabels="reserved:remote-node" isNew=false subsys=identity-cache
level=debug msg="Controller func execution time: 128.042µs" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Controller run succeeded; waiting for next controller update or stop" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Updating existing controller" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Controller update time: 5µs" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Upserting IP into ipcache layer" identity="{health custom-resource [] false false}" ipAddr=10.244.1.18 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity="{health custom-resource [] false false}" ipAddr="{10.244.1.18 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Received node update event from custom-resource: types.Node{Name:\"kind-control-plane\", Cluster:\"kind-kind\", IPAddresses:[]types.Address{types.Address{Type:\"InternalIP\", IP:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xac, 0x1a, 0x0, 0x2}}, types.Address{Type:\"InternalIP\", IP:net.IP{0xfc, 0x0, 0xc1, 0x11, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x2}}, types.Address{Type:\"CiliumInternalIP\", IP:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xa, 0xf4, 0x0, 0x3d}}}, IPv4AllocCIDR:(*cidr.CIDR)(0x40008086d8), IPv4SecondaryAllocCIDRs:[]*cidr.CIDR(nil), IPv6AllocCIDR:(*cidr.CIDR)(nil), IPv6SecondaryAllocCIDRs:[]*cidr.CIDR(nil), IPv4HealthIP:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xa, 0xf4, 0x0, 0xfc}, IPv6HealthIP:net.IP(nil), IPv4IngressIP:net.IP(nil), IPv6IngressIP:net.IP(nil), ClusterID:0x0, Source:\"custom-resource\", EncryptionKey:0x0, Labels:map[string]string{\"beta.kubernetes.io/arch\":\"arm64\", \"beta.kubernetes.io/os\":\"linux\", \"kubernetes.io/arch\":\"arm64\", \"kubernetes.io/hostname\":\"kind-control-plane\", \"kubernetes.io/os\":\"linux\", \"node-role.kubernetes.io/control-plane\":\"\", \"node.kubernetes.io/exclude-from-external-load-balancers\":\"\"}, Annotations:map[string]string(nil), NodeIdentity:0x0, WireguardPubKey:\"\"}" subsys=nodemanager
level=debug msg="Controller func execution time: 106.583µs" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Controller run succeeded; waiting for next controller update or stop" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Updating existing controller" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Controller update time: 4.209µs" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Upserting IP into ipcache layer" identity="{remote-node custom-resource [] false false}" ipAddr="fc00:c111::2/128" key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity="{remote-node custom-resource [] false false}" ipAddr="{fc00:c111::2 ffffffffffffffffffffffffffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Resolving identity" identityLabels="reserved:remote-node" subsys=identity-cache
level=debug msg="Resolved reserved identity" identity=remote-node identityLabels="reserved:remote-node" isNew=false subsys=identity-cache
level=debug msg="Controller func execution time: 36.5µs" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Controller run succeeded; waiting for next controller update or stop" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Updating existing controller" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Controller update time: 3.292µs" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Upserting IP into ipcache layer" identity="{remote-node custom-resource [] false false}" ipAddr=10.244.0.61/32 key=0 subsys=ipcache
level=debug msg="Allocated new node ID for node IP address" ipAddr=172.26.0.2 nodeID=25872 subsys=linux-datapath
level=debug msg="Daemon notified of IP-Identity cache state change" identity="{remote-node custom-resource [] false false}" ipAddr="{10.244.0.61 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Resolving identity" identityLabels="reserved:kube-apiserver,reserved:remote-node" subsys=identity-cache
level=debug msg="Resolved reserved identity" identity=kube-apiserver identityLabels="reserved:kube-apiserver,reserved:remote-node" isNew=false subsys=identity-cache
level=debug msg="UpdateIdentities: Skipping add of an existing identical identity" identity=kube-apiserver subsys=policy
level=debug msg="Waiting for proxy updates to complete..." subsys=endpoint-manager
level=debug msg="Wait time for proxy updates: 13.209µs" subsys=endpoint-manager
level=debug msg="Upserting IP into ipcache layer" identity="{kube-apiserver kube-apiserver [] false true}" ipAddr=172.26.0.2/32 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity="{kube-apiserver kube-apiserver [] false true}" ipAddr="{172.26.0.2 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="UpdateIdentities: Deleting identity" identity=16777217 labels="[cidr:0.0.0.0/0 cidr:128.0.0.0/1 cidr:128.0.0.0/2 cidr:160.0.0.0/3 cidr:160.0.0.0/4 cidr:168.0.0.0/5 cidr:172.0.0.0/10 cidr:172.0.0.0/11 cidr:172.0.0.0/6 cidr:172.0.0.0/7 cidr:172.0.0.0/8 cidr:172.0.0.0/9 cidr:172.16.0.0/12 cidr:172.24.0.0/13 cidr:172.24.0.0/14 cidr:172.26.0.0/15 cidr:172.26.0.0/16 cidr:172.26.0.0/17 cidr:172.26.0.0/18 cidr:172.26.0.0/19 cidr:172.26.0.0/20 cidr:172.26.0.0/21 cidr:172.26.0.0/22 cidr:172.26.0.0/23 cidr:172.26.0.0/24 cidr:172.26.0.0/25 cidr:172.26.0.0/26 cidr:172.26.0.0/27 cidr:172.26.0.0/28 cidr:172.26.0.0/29 cidr:172.26.0.0/30 cidr:172.26.0.2/31 cidr:172.26.0.2/32 reserved:kube-apiserver reserved:world]" subsys=policy
level=debug msg="Waiting for proxy updates to complete..." subsys=endpoint-manager
level=debug msg="Wait time for proxy updates: 8.291µs" subsys=endpoint-manager
level=debug msg="Controller func execution time: 146.625µs" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Controller run succeeded; waiting for next controller update or stop" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=warning msg="UpdateIdentities: Skipping Delete of a non-existing identity" identity=16777217 subsys=policy
level=debug msg="Updating existing controller" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Controller update time: 35.291µs" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Upserting IP into ipcache layer" identity="{health custom-resource [] false false}" ipAddr=10.244.0.252 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity="{health custom-resource [] false false}" ipAddr="{10.244.0.252 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Received node update event from custom-resource: types.Node{Name:\"kind-worker\", Cluster:\"kind-kind\", IPAddresses:[]types.Address{types.Address{Type:\"InternalIP\", IP:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xac, 0x1a, 0x0, 0x6}}, types.Address{Type:\"InternalIP\", IP:net.IP{0xfc, 0x0, 0xc1, 0x11, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x6}}, types.Address{Type:\"CiliumInternalIP\", IP:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xa, 0xf4, 0x2, 0x5f}}}, IPv4AllocCIDR:(*cidr.CIDR)(0x4000678228), IPv4SecondaryAllocCIDRs:[]*cidr.CIDR(nil), IPv6AllocCIDR:(*cidr.CIDR)(nil), IPv6SecondaryAllocCIDRs:[]*cidr.CIDR(nil), IPv4HealthIP:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xa, 0xf4, 0x2, 0xa7}, IPv6HealthIP:net.IP(nil), IPv4IngressIP:net.IP(nil), IPv6IngressIP:net.IP(nil), ClusterID:0x0, Source:\"custom-resource\", EncryptionKey:0x0, Labels:map[string]string{\"beta.kubernetes.io/arch\":\"arm64\", \"beta.kubernetes.io/os\":\"linux\", \"kubernetes.io/arch\":\"arm64\", \"kubernetes.io/hostname\":\"kind-worker\", \"kubernetes.io/os\":\"linux\"}, Annotations:map[string]string(nil), NodeIdentity:0x0, WireguardPubKey:\"\"}" subsys=nodemanager
level=debug msg="Upserting IP into ipcache layer" identity="{remote-node custom-resource [] false false}" ipAddr=172.26.0.6/32 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity="{remote-node custom-resource [] false false}" ipAddr="{172.26.0.6 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Resolving identity" identityLabels="reserved:remote-node" subsys=identity-cache
level=debug msg="Resolved reserved identity" identity=remote-node identityLabels="reserved:remote-node" isNew=false subsys=identity-cache
level=debug msg="Controller func execution time: 451.042µs" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Controller run succeeded; waiting for next controller update or stop" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Updating existing controller" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Controller update time: 10.209µs" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Upserting IP into ipcache layer" identity="{remote-node custom-resource [] false false}" ipAddr="fc00:c111::6/128" key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity="{remote-node custom-resource [] false false}" ipAddr="{fc00:c111::6 ffffffffffffffffffffffffffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Resolving identity" identityLabels="reserved:remote-node" subsys=identity-cache
level=debug msg="Resolved reserved identity" identity=remote-node identityLabels="reserved:remote-node" isNew=false subsys=identity-cache
level=debug msg="Controller func execution time: 177.625µs" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Controller run succeeded; waiting for next controller update or stop" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Updating existing controller" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Controller update time: 24.75µs" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Upserting IP into ipcache layer" identity="{remote-node custom-resource [] false false}" ipAddr=10.244.2.95/32 key=0 subsys=ipcache
level=debug msg="Allocated new node ID for node IP address" ipAddr=172.26.0.6 nodeID=27917 subsys=linux-datapath
level=debug msg="Daemon notified of IP-Identity cache state change" identity="{remote-node custom-resource [] false false}" ipAddr="{10.244.2.95 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Updating existing controller" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Controller update time: 27.917µs" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Upserting IP into ipcache layer" identity="{health custom-resource [] false false}" ipAddr=10.244.2.167 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity="{health custom-resource [] false false}" ipAddr="{10.244.2.167 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Received node update event from custom-resource: types.Node{Name:\"kind-worker4\", Cluster:\"kind-kind\", IPAddresses:[]types.Address{types.Address{Type:\"InternalIP\", IP:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xac, 0x1a, 0x0, 0x5}}, types.Address{Type:\"InternalIP\", IP:net.IP{0xfc, 0x0, 0xc1, 0x11, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x5}}, types.Address{Type:\"CiliumInternalIP\", IP:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xa, 0xf4, 0x4, 0xeb}}}, IPv4AllocCIDR:(*cidr.CIDR)(0x40007e0410), IPv4SecondaryAllocCIDRs:[]*cidr.CIDR(nil), IPv6AllocCIDR:(*cidr.CIDR)(nil), IPv6SecondaryAllocCIDRs:[]*cidr.CIDR(nil), IPv4HealthIP:net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xff, 0xff, 0xa, 0xf4, 0x4, 0x20}, IPv6HealthIP:net.IP(nil), IPv4IngressIP:net.IP(nil), IPv6IngressIP:net.IP(nil), ClusterID:0x0, Source:\"custom-resource\", EncryptionKey:0x0, Labels:map[string]string{\"beta.kubernetes.io/arch\":\"arm64\", \"beta.kubernetes.io/os\":\"linux\", \"kubernetes.io/arch\":\"arm64\", \"kubernetes.io/hostname\":\"kind-worker4\", \"kubernetes.io/os\":\"linux\"}, Annotations:map[string]string(nil), NodeIdentity:0x0, WireguardPubKey:\"\"}" subsys=nodemanager
level=debug msg="Upserting IP into ipcache layer" identity="{remote-node custom-resource [] false false}" ipAddr=172.26.0.5/32 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity="{remote-node custom-resource [] false false}" ipAddr="{172.26.0.5 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Updating existing controller" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Controller update time: 20.917µs" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Upserting IP into ipcache layer" identity="{remote-node custom-resource [] false false}" ipAddr="fc00:c111::5/128" key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity="{remote-node custom-resource [] false false}" ipAddr="{fc00:c111::5 ffffffffffffffffffffffffffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Updating existing controller" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Controller update time: 21.041µs" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Upserting IP into ipcache layer" identity="{remote-node custom-resource [] false false}" ipAddr=10.244.4.235/32 key=0 subsys=ipcache
level=debug msg="Allocated new node ID for node IP address" ipAddr=172.26.0.5 nodeID=22683 subsys=linux-datapath
level=debug msg="Daemon notified of IP-Identity cache state change" identity="{remote-node custom-resource [] false false}" ipAddr="{10.244.4.235 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Updating existing controller" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Controller update time: 11.375µs" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Upserting IP into ipcache layer" identity="{health custom-resource [] false false}" ipAddr=10.244.4.32 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity="{health custom-resource [] false false}" ipAddr="{10.244.4.32 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Resolving identity" identityLabels="reserved:remote-node" subsys=identity-cache
level=debug msg="Resolved reserved identity" identity=remote-node identityLabels="reserved:remote-node" isNew=false subsys=identity-cache
level=debug msg="Resolving identity" identityLabels="reserved:remote-node" subsys=identity-cache
level=debug msg="Resolved reserved identity" identity=remote-node identityLabels="reserved:remote-node" isNew=false subsys=identity-cache
level=debug msg="Resolving identity" identityLabels="reserved:remote-node" subsys=identity-cache
level=debug msg="Resolved reserved identity" identity=remote-node identityLabels="reserved:remote-node" isNew=false subsys=identity-cache
level=debug msg="Resolving identity" identityLabels="reserved:remote-node" subsys=identity-cache
level=debug msg="Resolved reserved identity" identity=remote-node identityLabels="reserved:remote-node" isNew=false subsys=identity-cache
level=debug msg="Resolving identity" identityLabels="reserved:remote-node" subsys=identity-cache
level=debug msg="Resolved reserved identity" identity=remote-node identityLabels="reserved:remote-node" isNew=false subsys=identity-cache
level=debug msg="Resolving identity" identityLabels="reserved:remote-node" subsys=identity-cache
level=debug msg="Resolved reserved identity" identity=remote-node identityLabels="reserved:remote-node" isNew=false subsys=identity-cache
level=debug msg="Controller func execution time: 96.25µs" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=debug msg="Controller run succeeded; waiting for next controller update or stop" name=ipcache-inject-labels subsys=controller uuid=9706ad54-c66b-4e39-b042-4e04a9649f69
level=info msg="Successfully created CiliumNode resource" subsys=nodediscovery
level=debug msg="Annotate k8s node is disabled." subsys=daemon
level=info msg="Initializing identity allocator" subsys=identity-cache
level=info msg="Allocating identities between range" cluster-id=0 max=65535 min=256 subsys=identity-cache
level=info msg="Cluster-ID is not specified, skipping ClusterMesh initialization" subsys=daemon
level=debug msg="Identity allocation backed by CRD" subsys=identity-cache
level=debug msg="Starting new controller" name=template-dir-watcher subsys=controller uuid=ae464d85-c80d-4ebf-a530-38481cea0209
level=debug msg="writing configuration" file-path=netdev_config.h subsys=datapath-loader
level=info msg="Setting up BPF datapath" bpfClockSource=ktime bpfInsnSet="<nil>" subsys=datapath-loader
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.core.bpf_jit_enable sysParamValue=1
level=warning msg="Unable to ensure that BPF JIT compilation is enabled. This can be ignored when Cilium is running inside non-host network namespace (e.g. with kind or minikube)" error="could not open the sysctl file /host/proc/sys/net/core/bpf_jit_enable: open /host/proc/sys/net/core/bpf_jit_enable: no such file or directory" subsys=sysctl sysParamName=net.core.bpf_jit_enable sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.all.rp_filter sysParamValue=0
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.fib_multipath_use_neigh sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=kernel.unprivileged_bpf_disabled sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=kernel.timer_migration sysParamValue=0
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.cilium_host.forwarding sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.cilium_host.rp_filter sysParamValue=0
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.cilium_host.accept_local sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.cilium_host.send_redirects sysParamValue=0
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.cilium_net.forwarding sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.cilium_net.rp_filter sysParamValue=0
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.cilium_net.accept_local sysParamValue=1
level=info msg="Setting sysctl" subsys=sysctl sysParamName=net.ipv4.conf.cilium_net.send_redirects sysParamValue=0
level=debug msg="cache synced" kubernetesResource="networking.k8s.io/v1::NetworkPolicy" subsys=k8s
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="networking.k8s.io/v1::NetworkPolicy" subsys=k8s
level=debug msg="resource \"networking.k8s.io/v1::NetworkPolicy\" cache has synced, stopping timeout watcher" subsys=k8s
level=debug msg="cache synced" kubernetesResource="discovery/v1::EndpointSlice" subsys=k8s
level=debug msg="cache synced" kubernetesResource="core/v1::Service" subsys=k8s
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="core/v1::Service" subsys=k8s
level=debug msg="resource \"core/v1::Service\" cache has synced, stopping timeout watcher" subsys=k8s
level=debug msg="cache synced" kubernetesResource="core/v1::Pods" subsys=k8s
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="core/v1::Pods" subsys=k8s
level=debug msg="resource \"core/v1::Pods\" cache has synced, stopping timeout watcher" subsys=k8s
level=debug msg="cache synced" kubernetesResource="cilium/v2::CiliumNode" subsys=k8s
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="cilium/v2::CiliumNode" subsys=k8s
level=debug msg="resource \"cilium/v2::CiliumNode\" cache has synced, stopping timeout watcher" subsys=k8s
level=debug msg="cache synced" kubernetesResource="cilium/v2::CiliumClusterwideNetworkPolicy" subsys=k8s
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="cilium/v2::CiliumClusterwideNetworkPolicy" subsys=k8s
level=debug msg="resource \"cilium/v2::CiliumClusterwideNetworkPolicy\" cache has synced, stopping timeout watcher" subsys=k8s
level=debug msg="cache synced" kubernetesResource="cilium/v2::CiliumNetworkPolicy" subsys=k8s
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="cilium/v2::CiliumNetworkPolicy" subsys=k8s
level=debug msg="resource \"cilium/v2::CiliumNetworkPolicy\" cache has synced, stopping timeout watcher" subsys=k8s
level=debug msg="cache synced" kubernetesResource="cilium/v2alpha1::CiliumCIDRGroup" subsys=k8s
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="cilium/v2alpha1::CiliumCIDRGroup" subsys=k8s
level=debug msg="resource \"cilium/v2alpha1::CiliumCIDRGroup\" cache has synced, stopping timeout watcher" subsys=k8s
level=debug msg="cache synced" kubernetesResource="cilium/v2::CiliumEndpoint" subsys=k8s
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="cilium/v2::CiliumEndpoint" subsys=k8s
level=debug msg="resource \"cilium/v2::CiliumEndpoint\" cache has synced, stopping timeout watcher" subsys=k8s
level=debug msg="Initial list of identities received" subsys=allocator
level=debug msg="Launching compiler" args="[-emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -DCALLS_MAP=cilium_calls_lb -I/var/run/cilium/state/globals -I/var/run/cilium/state -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_sock.c -o -]" subsys=datapath-loader target=clang
level=debug msg="cache synced" kubernetesResource="core/v1::Namespace" subsys=k8s
level=debug msg="stopped waiting for caches to be synced" kubernetesResource="core/v1::Namespace" subsys=k8s
level=debug msg="resource \"core/v1::Namespace\" cache has synced, stopping timeout watcher" subsys=k8s
level=info msg="No existing link found at /sys/fs/bpf/cilium/socketlb/links/cgroup/cil_sock4_sendmsg for program cil_sock4_sendmsg" subsys=socketlb
level=debug msg="Program cil_sock4_sendmsg attached using bpf_link" subsys=socketlb
level=info msg="No existing link found at /sys/fs/bpf/cilium/socketlb/links/cgroup/cil_sock4_recvmsg for program cil_sock4_recvmsg" subsys=socketlb
level=debug msg="Program cil_sock4_recvmsg attached using bpf_link" subsys=socketlb
level=info msg="No existing link found at /sys/fs/bpf/cilium/socketlb/links/cgroup/cil_sock4_getpeername for program cil_sock4_getpeername" subsys=socketlb
level=debug msg="Program cil_sock4_getpeername attached using bpf_link" subsys=socketlb
level=debug msg="No pinned link '/sys/fs/bpf/cilium/socketlb/links/cgroup/cil_sock4_pre_bind', querying cgroup" subsys=socketlb
level=debug msg="No programs in cgroup /run/cilium/cgroupv2 with attach type CGroupInet4Bind" subsys=socketlb
level=info msg="No existing link found at /sys/fs/bpf/cilium/socketlb/links/cgroup/cil_sock6_connect for program cil_sock6_connect" subsys=socketlb
level=debug msg="Program cil_sock6_connect attached using bpf_link" subsys=socketlb
level=info msg="No existing link found at /sys/fs/bpf/cilium/socketlb/links/cgroup/cil_sock6_recvmsg for program cil_sock6_recvmsg" subsys=socketlb
level=debug msg="Program cil_sock6_recvmsg attached using bpf_link" subsys=socketlb
level=debug msg="No pinned link '/sys/fs/bpf/cilium/socketlb/links/cgroup/cil_sock6_pre_bind', querying cgroup" subsys=socketlb
level=debug msg="No programs in cgroup /run/cilium/cgroupv2 with attach type CGroupInet6Bind" subsys=socketlb
level=info msg="No existing link found at /sys/fs/bpf/cilium/socketlb/links/cgroup/cil_sock4_connect for program cil_sock4_connect" subsys=socketlb
level=debug msg="Program cil_sock4_connect attached using bpf_link" subsys=socketlb
level=info msg="No existing link found at /sys/fs/bpf/cilium/socketlb/links/cgroup/cil_sock4_post_bind for program cil_sock4_post_bind" subsys=socketlb
level=debug msg="Program cil_sock4_post_bind attached using bpf_link" subsys=socketlb
level=info msg="No existing link found at /sys/fs/bpf/cilium/socketlb/links/cgroup/cil_sock6_sendmsg for program cil_sock6_sendmsg" subsys=socketlb
level=debug msg="Program cil_sock6_sendmsg attached using bpf_link" subsys=socketlb
level=info msg="No existing link found at /sys/fs/bpf/cilium/socketlb/links/cgroup/cil_sock6_getpeername for program cil_sock6_getpeername" subsys=socketlb
level=debug msg="Program cil_sock6_getpeername attached using bpf_link" subsys=socketlb
level=info msg="No existing link found at /sys/fs/bpf/cilium/socketlb/links/cgroup/cil_sock6_post_bind for program cil_sock6_post_bind" subsys=socketlb
level=debug msg="Program cil_sock6_post_bind attached using bpf_link" subsys=socketlb
level=debug msg="Finalizing bpffs map migration" subsys=socketlb
level=debug msg="Launching compiler" args="[-emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_alignchecker.c -o -]" subsys=datapath-loader target=clang
level=debug msg="Updating direct route" addedCIDRs="[10.244.2.0/24]" newIP=172.26.0.6 oldIP="<nil>" removedCIDRs="[]" subsys=linux-datapath
level=debug msg="Updating direct route" addedCIDRs="[10.244.4.0/24]" newIP=172.26.0.5 oldIP="<nil>" removedCIDRs="[]" subsys=linux-datapath
level=debug msg="Updating direct route" addedCIDRs="[10.244.1.0/24]" newIP=172.26.0.4 oldIP="<nil>" removedCIDRs="[]" subsys=linux-datapath
level=debug msg="Updating direct route" addedCIDRs="[10.244.0.0/24]" newIP=172.26.0.2 oldIP="<nil>" removedCIDRs="[]" subsys=linux-datapath
level=debug msg="Running 'iptables -t nat -S' command" subsys=iptables
level=debug msg="Unable to insert new next hop" error="invalid argument" interface=eth0 ipAddr=172.26.0.6 neighbor="{LinkIndex:165 Family:0 State:4 Type:0 Flags:16 FlagsExt:0 IP:172.26.0.6 HardwareAddr: LLIPAddr:<nil> Vlan:0 VNI:0 MasterIndex:0}" nextHop=172.26.0.6 subsys=node-neigh-debug
level=debug msg="Unable to insert new next hop" error="invalid argument" interface=eth0 ipAddr="fc00:c111::2" neighbor="{LinkIndex:165 Family:0 State:4 Type:0 Flags:16 FlagsExt:0 IP:fc00:c111::2 HardwareAddr: LLIPAddr:<nil> Vlan:0 VNI:0 MasterIndex:0}" nextHop="fc00:c111::2" subsys=node-neigh-debug
level=debug msg="Unable to insert new next hop" error="invalid argument" interface=eth0 ipAddr="fc00:c111::6" neighbor="{LinkIndex:165 Family:0 State:4 Type:0 Flags:16 FlagsExt:0 IP:fc00:c111::6 HardwareAddr: LLIPAddr:<nil> Vlan:0 VNI:0 MasterIndex:0}" nextHop="fc00:c111::6" subsys=node-neigh-debug
level=debug msg="Unable to insert new next hop" error="invalid argument" interface=eth0 ipAddr="fc00:c111::4" neighbor="{LinkIndex:165 Family:0 State:4 Type:0 Flags:16 FlagsExt:0 IP:fc00:c111::4 HardwareAddr: LLIPAddr:<nil> Vlan:0 VNI:0 MasterIndex:0}" nextHop="fc00:c111::4" subsys=node-neigh-debug
level=debug msg="Unable to insert new next hop" error="invalid argument" interface=eth0 ipAddr="fc00:c111::5" neighbor="{LinkIndex:165 Family:0 State:4 Type:0 Flags:16 FlagsExt:0 IP:fc00:c111::5 HardwareAddr: LLIPAddr:<nil> Vlan:0 VNI:0 MasterIndex:0}" nextHop="fc00:c111::5" subsys=node-neigh-debug
level=debug msg="Running 'ip6tables -t nat -S' command" subsys=iptables
level=debug msg="Running 'iptables -t mangle -S' command" subsys=iptables
level=debug msg="Running 'ip6tables -t mangle -S' command" subsys=iptables
level=debug msg="Running 'iptables -t raw -S' command" subsys=iptables
level=debug msg="Running 'ip6tables -t raw -S' command" subsys=iptables
level=debug msg="Running 'iptables -t filter -S' command" subsys=iptables
level=debug msg="Running 'ip6tables -t filter -S' command" subsys=iptables
level=debug msg="Running 'iptables -t filter -S OLD_CILIUM_INPUT' command" subsys=iptables
level=debug msg="Running 'ip6tables -t filter -S OLD_CILIUM_INPUT' command" subsys=iptables
level=debug msg="Running 'iptables -t filter -S OLD_CILIUM_OUTPUT' command" subsys=iptables
level=debug msg="Running 'ip6tables -t filter -S OLD_CILIUM_OUTPUT' command" subsys=iptables
level=debug msg="Running 'iptables -t raw -S OLD_CILIUM_OUTPUT_raw' command" subsys=iptables
level=debug msg="Running 'ip6tables -t raw -S OLD_CILIUM_OUTPUT_raw' command" subsys=iptables
level=debug msg="Running 'iptables -t nat -S OLD_CILIUM_POST_nat' command" subsys=iptables
level=debug msg="Running 'ip6tables -t nat -S OLD_CILIUM_POST_nat' command" subsys=iptables
level=debug msg="Running 'iptables -t nat -S OLD_CILIUM_OUTPUT_nat' command" subsys=iptables
level=debug msg="Running 'iptables -t nat -S OLD_CILIUM_PRE_nat' command" subsys=iptables
level=debug msg="Running 'iptables -t mangle -S OLD_CILIUM_POST_mangle' command" subsys=iptables
level=debug msg="Running 'iptables -t mangle -S OLD_CILIUM_PRE_mangle' command" subsys=iptables
level=debug msg="Running 'ip6tables -t mangle -S OLD_CILIUM_PRE_mangle' command" subsys=iptables
level=debug msg="Running 'iptables -t raw -S OLD_CILIUM_PRE_raw' command" subsys=iptables
level=debug msg="Running 'ip6tables -t raw -S OLD_CILIUM_PRE_raw' command" subsys=iptables
level=debug msg="Running 'iptables -t filter -S OLD_CILIUM_FORWARD' command" subsys=iptables
level=debug msg="Running 'ip6tables -t filter -S OLD_CILIUM_FORWARD' command" subsys=iptables
level=debug msg="Running 'iptables -t filter -S CILIUM_INPUT' command" subsys=iptables
level=debug msg="Running 'ip6tables -t filter -S CILIUM_INPUT' command" subsys=iptables
level=debug msg="Running 'iptables -t filter -S CILIUM_OUTPUT' command" subsys=iptables
level=debug msg="Running 'ip6tables -t filter -S CILIUM_OUTPUT' command" subsys=iptables
level=debug msg="Running 'iptables -t raw -S CILIUM_OUTPUT_raw' command" subsys=iptables
level=debug msg="Running 'ip6tables -t raw -S CILIUM_OUTPUT_raw' command" subsys=iptables
level=debug msg="Running 'iptables -t nat -S CILIUM_POST_nat' command" subsys=iptables
level=debug msg="Running 'ip6tables -t nat -S CILIUM_POST_nat' command" subsys=iptables
level=debug msg="Running 'iptables -t nat -S CILIUM_OUTPUT_nat' command" subsys=iptables
level=debug msg="Running 'iptables -t nat -S CILIUM_PRE_nat' command" subsys=iptables
level=debug msg="Running 'iptables -t mangle -S CILIUM_POST_mangle' command" subsys=iptables
level=debug msg="Running 'iptables -t mangle -S CILIUM_PRE_mangle' command" subsys=iptables
level=debug msg="Running 'ip6tables -t mangle -S CILIUM_PRE_mangle' command" subsys=iptables
level=debug msg="Running 'iptables -t raw -S CILIUM_PRE_raw' command" subsys=iptables
level=debug msg="Running 'ip6tables -t raw -S CILIUM_PRE_raw' command" subsys=iptables
level=debug msg="Running 'iptables -t filter -S CILIUM_FORWARD' command" subsys=iptables
level=debug msg="Running 'ip6tables -t filter -S CILIUM_FORWARD' command" subsys=iptables
level=debug msg="Running 'iptables -t filter -N CILIUM_INPUT' command" subsys=iptables
level=debug msg="Running 'iptables -t filter -N CILIUM_OUTPUT' command" subsys=iptables
level=debug msg="Running 'iptables -t raw -N CILIUM_OUTPUT_raw' command" subsys=iptables
level=debug msg="Running 'iptables -t nat -N CILIUM_POST_nat' command" subsys=iptables
level=debug msg="Running 'iptables -t nat -N CILIUM_OUTPUT_nat' command" subsys=iptables
level=debug msg="Running 'iptables -t nat -N CILIUM_PRE_nat' command" subsys=iptables
level=debug msg="Running 'iptables -t mangle -N CILIUM_POST_mangle' command" subsys=iptables
level=debug msg="Running 'iptables -t mangle -N CILIUM_PRE_mangle' command" subsys=iptables
level=debug msg="Running 'iptables -t raw -N CILIUM_PRE_raw' command" subsys=iptables
level=debug msg="Running 'iptables -t filter -N CILIUM_FORWARD' command" subsys=iptables
level=debug msg="Running 'iptables -t raw -A CILIUM_PRE_raw -m mark --mark 0x00000200/0x00000f00 -m comment --comment cilium: NOTRACK for proxy traffic -j CT --notrack' command" subsys=iptables
level=debug msg="Running 'iptables -t filter -A CILIUM_INPUT -m mark --mark 0x00000200/0x00000f00 -m comment --comment cilium: ACCEPT for proxy traffic -j ACCEPT' command" subsys=iptables
level=debug msg="Running 'iptables -t raw -A CILIUM_OUTPUT_raw -o lxc+ -m mark --mark 0x00000a00/0xfffffeff -m comment --comment cilium: NOTRACK for proxy return traffic -j CT --notrack' command" subsys=iptables
level=debug msg="Running 'iptables -t raw -A CILIUM_OUTPUT_raw -o cilium_host -m mark --mark 0x00000a00/0xfffffeff -m comment --comment cilium: NOTRACK for proxy return traffic -j CT --notrack' command" subsys=iptables
level=debug msg="Running 'iptables -t raw -A CILIUM_OUTPUT_raw -o lxc+ -m mark --mark 0x00000800/0x00000e00 -m comment --comment cilium: NOTRACK for L7 proxy upstream traffic -j CT --notrack' command" subsys=iptables
level=debug msg="Running 'iptables -t raw -A CILIUM_OUTPUT_raw -o cilium_host -m mark --mark 0x00000800/0x00000e00 -m comment --comment cilium: NOTRACK for L7 proxy upstream traffic -j CT --notrack' command" subsys=iptables
level=debug msg="Running 'iptables -t filter -A CILIUM_OUTPUT -m mark --mark 0x00000a00/0xfffffeff -m comment --comment cilium: ACCEPT for proxy return traffic -j ACCEPT' command" subsys=iptables
level=debug msg="Running 'iptables -t filter -A CILIUM_OUTPUT -m mark --mark 0x00000800/0x00000e00 -m comment --comment cilium: ACCEPT for l7 proxy upstream traffic -j ACCEPT' command" subsys=iptables
level=debug msg="Running 'iptables -t mangle -A CILIUM_PRE_mangle -m socket --transparent -m comment --comment cilium: any->pod redirect proxied traffic to host proxy -j MARK --set-mark 0x00000200' command" subsys=iptables
level=debug msg="Running 'iptables -A CILIUM_FORWARD -o cilium_host -m comment --comment cilium: any->cluster on cilium_host forward accept -j ACCEPT' command" subsys=iptables
level=debug msg="Running 'iptables -A CILIUM_FORWARD -i cilium_host -m comment --comment cilium: cluster->any on cilium_host forward accept (nodeport) -j ACCEPT' command" subsys=iptables
level=debug msg="Running 'iptables -A CILIUM_FORWARD -i lxc+ -m comment --comment cilium: cluster->any on lxc+ forward accept -j ACCEPT' command" subsys=iptables
level=debug msg="Running 'iptables -A CILIUM_FORWARD -i cilium_net -m comment --comment cilium: cluster->any on cilium_net forward accept (nodeport) -j ACCEPT' command" subsys=iptables
level=debug msg="Running 'iptables -t filter -A CILIUM_OUTPUT -m mark ! --mark 0x00000e00/0x00000f00 -m mark ! --mark 0x00000d00/0x00000f00 -m mark ! --mark 0x00000a00/0x00000e00 -m mark ! --mark 0x00000800/0x00000e00 -m mark ! --mark 0x00000f00/0x00000f00 -m comment --comment cilium: host->any mark as from host -j MARK --set-xmark 0x00000c00/0x00000f00' command" subsys=iptables
level=debug msg="Running 'iptables -t filter -I INPUT -m comment --comment cilium-feeder: CILIUM_INPUT -j CILIUM_INPUT' command" subsys=iptables
level=debug msg="Running 'iptables -t filter -I OUTPUT -m comment --comment cilium-feeder: CILIUM_OUTPUT -j CILIUM_OUTPUT' command" subsys=iptables
level=debug msg="Running 'iptables -t raw -I OUTPUT -m comment --comment cilium-feeder: CILIUM_OUTPUT_raw -j CILIUM_OUTPUT_raw' command" subsys=iptables
level=debug msg="Running 'iptables -t nat -I POSTROUTING -m comment --comment cilium-feeder: CILIUM_POST_nat -j CILIUM_POST_nat' command" subsys=iptables
level=debug msg="Running 'iptables -t nat -I OUTPUT -m comment --comment cilium-feeder: CILIUM_OUTPUT_nat -j CILIUM_OUTPUT_nat' command" subsys=iptables
level=debug msg="Running 'iptables -t nat -I PREROUTING -m comment --comment cilium-feeder: CILIUM_PRE_nat -j CILIUM_PRE_nat' command" subsys=iptables
level=debug msg="Running 'iptables -t mangle -I POSTROUTING -m comment --comment cilium-feeder: CILIUM_POST_mangle -j CILIUM_POST_mangle' command" subsys=iptables
level=debug msg="Running 'iptables -t mangle -I PREROUTING -m comment --comment cilium-feeder: CILIUM_PRE_mangle -j CILIUM_PRE_mangle' command" subsys=iptables
level=debug msg="Running 'iptables -t raw -I PREROUTING -m comment --comment cilium-feeder: CILIUM_PRE_raw -j CILIUM_PRE_raw' command" subsys=iptables
level=debug msg="Running 'iptables -t filter -I FORWARD -m comment --comment cilium-feeder: CILIUM_FORWARD -j CILIUM_FORWARD' command" subsys=iptables
level=debug msg="Running 'iptables -t mangle -S' command" subsys=iptables
level=debug msg="Running 'iptables -t nat -S' command" subsys=iptables
level=debug msg="Running 'ip6tables -t nat -S' command" subsys=iptables
level=debug msg="Running 'iptables -t mangle -S' command" subsys=iptables
level=debug msg="Running 'ip6tables -t mangle -S' command" subsys=iptables
level=debug msg="Running 'iptables -t raw -S' command" subsys=iptables
level=debug msg="Running 'ip6tables -t raw -S' command" subsys=iptables
level=debug msg="Running 'iptables -t filter -S' command" subsys=iptables
level=debug msg="Running 'ip6tables -t filter -S' command" subsys=iptables
level=debug msg="Running 'iptables -t filter -S OLD_CILIUM_INPUT' command" subsys=iptables
level=debug msg="Running 'ip6tables -t filter -S OLD_CILIUM_INPUT' command" subsys=iptables
level=debug msg="Running 'iptables -t filter -S OLD_CILIUM_OUTPUT' command" subsys=iptables
level=debug msg="Running 'ip6tables -t filter -S OLD_CILIUM_OUTPUT' command" subsys=iptables
level=debug msg="Running 'iptables -t raw -S OLD_CILIUM_OUTPUT_raw' command" subsys=iptables
level=debug msg="Running 'ip6tables -t raw -S OLD_CILIUM_OUTPUT_raw' command" subsys=iptables
level=debug msg="Running 'iptables -t nat -S OLD_CILIUM_POST_nat' command" subsys=iptables
level=debug msg="Running 'ip6tables -t nat -S OLD_CILIUM_POST_nat' command" subsys=iptables
level=debug msg="Running 'iptables -t nat -S OLD_CILIUM_OUTPUT_nat' command" subsys=iptables
level=debug msg="Running 'iptables -t nat -S OLD_CILIUM_PRE_nat' command" subsys=iptables
level=debug msg="Running 'iptables -t mangle -S OLD_CILIUM_POST_mangle' command" subsys=iptables
level=debug msg="Running 'iptables -t mangle -S OLD_CILIUM_PRE_mangle' command" subsys=iptables
level=debug msg="Regenerating all endpoints" subsys=policy
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=debug msg="Running 'ip6tables -t mangle -S OLD_CILIUM_PRE_mangle' command" subsys=iptables
level=debug msg="Running 'iptables -t raw -S OLD_CILIUM_PRE_raw' command" subsys=iptables
level=debug msg="Running 'ip6tables -t raw -S OLD_CILIUM_PRE_raw' command" subsys=iptables
level=debug msg="Running 'iptables -t filter -S OLD_CILIUM_FORWARD' command" subsys=iptables
level=debug msg="Running 'ip6tables -t filter -S OLD_CILIUM_FORWARD' command" subsys=iptables
level=debug msg="Running 'ipset list cilium_node_set_v4' command" subsys=iptables
level=debug msg="Running 'ipset list cilium_node_set_v6' command" subsys=iptables
level=info msg="Iptables rules installed" subsys=iptables
level=info msg="Adding new proxy port rules for cilium-dns-egress:38779" id=cilium-dns-egress subsys=proxy
level=debug msg="Running 'iptables -t mangle -S' command" subsys=iptables
level=debug msg="Running 'iptables -t mangle -A CILIUM_PRE_mangle -p tcp -m mark --mark 0x7b970200 -m comment --comment cilium: TPROXY to host cilium-dns-egress proxy -j TPROXY --tproxy-mark 0x200 --on-ip 0.0.0.0 --on-port 38779' command" subsys=iptables
level=debug msg="Running 'iptables -t mangle -A CILIUM_PRE_mangle -p udp -m mark --mark 0x7b970200 -m comment --comment cilium: TPROXY to host cilium-dns-egress proxy -j TPROXY --tproxy-mark 0x200 --on-ip 0.0.0.0 --on-port 38779' command" subsys=iptables
level=info msg="Iptables proxy rules installed" subsys=iptables
level=debug msg="AckProxyPort: acked proxy port 38779 ({true dns false 38779 1 true 38779 false})" id=cilium-dns-egress subsys=proxy
level=info msg="Beginning to read perf buffer" startTime="2023-07-16 12:01:21.056739298 +0000 UTC m=+3.463922336" subsys=monitor-agent
level=debug msg="Group not found" error="group: unknown group cilium" file-path=/var/run/cilium/monitor1_2.sock group=cilium subsys=api
level=info msg="Serving cilium node monitor v1.2 API at unix:///var/run/cilium/monitor1_2.sock" subsys=monitor-agent
level=debug msg="Starting new controller" name=sync-host-ips subsys=controller uuid=3f657dff-ea70-4210-9bdf-516e7028368b
level=debug msg="Added local ip to endpoint map" ipAddr=10.244.3.223 subsys=daemon
level=debug msg="New endpoint IP started shadowing existing CIDR to identity mapping" identity="{host local [] false false}" ipAddr=10.244.3.223 key=0 subsys=ipcache
level=debug msg="Upserting IP into ipcache layer" identity="{host local [] false false}" ipAddr=10.244.3.223 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity="{host local [] false false}" ipAddr="{10.244.3.223 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Added local ip to endpoint map" ipAddr=172.26.0.3 subsys=daemon
level=debug msg="Upserting IP into ipcache layer" identity="{host local [] false false}" ipAddr=172.26.0.3 key=0 subsys=ipcache
level=debug msg="Upserting IP into ipcache layer" identity="{world local [] false false}" ipAddr=0.0.0.0/0 key=0 subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity="{world local [] false false}" ipAddr="{0.0.0.0 00000000}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Controller func execution time: 2.634458ms" name=sync-host-ips subsys=controller uuid=3f657dff-ea70-4210-9bdf-516e7028368b
level=info msg="Start hook executed" duration=2.307411334s function="cmd.newDaemonPromise.func1 (daemon_main.go:1629)" subsys=hive
level=debug msg="Executing start hook" function="utime.initUtimeSync.func1 (cell.go:34)" subsys=hive
level=debug msg="Starting new controller" name=sync-utime subsys=controller uuid=792a2d8b-6899-4c24-b752-ae3995a7145e
level=info msg="Start hook executed" duration="158.5µs" function="utime.initUtimeSync.func1 (cell.go:34)" subsys=hive
level=info msg="Initializing daemon" subsys=daemon
level=info msg="Validating configured node address ranges" subsys=daemon
level=info msg="Starting connection tracking garbage collector" subsys=daemon
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct4_global subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct_any4_global subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct_any4_global subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct4_global subsys=bpf
level=info msg="Starting IP identity watcher" subsys=ipcache
level=debug msg="Controller func execution time: 448.25µs" name=sync-utime subsys=controller uuid=792a2d8b-6899-4c24-b752-ae3995a7145e
level=info msg="Initial scan of connection tracking completed" subsys=ct-gc
level=info msg="Regenerating restored endpoints" numRestored=0 subsys=daemon
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct4_global subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct4_global subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct_any4_global subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct_any4_global subsys=bpf
level=debug msg="Couldn't open CT map for upgrade" error="Unable to get object /sys/fs/bpf/tc/globals/cilium_ct6_global: no such file or directory" file-path=/sys/fs/bpf/tc/globals/cilium_ct6_global subsys=map-ct
level=debug msg="Couldn't open CT map for upgrade" error="Unable to get object /sys/fs/bpf/tc/globals/cilium_ct_any6_global: no such file or directory" file-path=/sys/fs/bpf/tc/globals/cilium_ct_any6_global subsys=map-ct
level=info msg="Creating host endpoint" subsys=daemon
level=debug msg="Endpoint creation" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=0 endpointState=waiting-for-identity ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="Starting new controller" name=endpoint-3957-regeneration-recovery subsys=controller uuid=962097aa-872a-4027-89af-674b4854432c
level=debug msg="creating new EventQueue" name=endpoint-3957 numBufferedEvents=25 subsys=eventqueue
level=info msg="New endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3957 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting new controller" name="sync-to-k8s-ciliumendpoint (3957)" subsys=controller uuid=c65c936a-e669-40ca-a20d-6b4a5aa52d45
level=debug msg="Refreshing labels of endpoint" containerID= endpointID=3957 identityLabels="reserved:host" infoLabels="reserved:host" subsys=endpoint
level=debug msg="Assigning information label" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3957 ipv4= ipv6= k8sPodName=/ obj="{Key:host Value: Source:reserved}" subsys=endpoint
level=debug msg="Assigning security relevant label" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3957 ipv4= ipv6= k8sPodName=/ obj="{Key:host Value: Source:reserved}" subsys=endpoint
level=info msg="Resolving identity labels (blocking)" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3957 identityLabels="reserved:host" ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Resolving identity for labels" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3957 identityLabels="reserved:host" ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Resolving identity" identityLabels="reserved:host" subsys=identity-cache
level=debug msg="Resolved reserved identity" identity=host identityLabels="reserved:host" isNew=false subsys=identity-cache
level=debug msg="Assigned new identity to endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3957 identity=1 identityLabels="reserved:host" ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing old and adding new identity" new=1 old="<nil>" subsys=identitymanager
level=debug msg="Set identity for this endpoint" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3957 endpointState=ready ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=info msg="Identity of endpoint changed" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3957 identity=1 identityLabels="reserved:host" ipv4= ipv6= k8sPodName=/ oldIdentity="no identity" subsys=endpoint
level=debug msg="Triggering endpoint regeneration due to updated security labels" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3957 endpointState=waiting-to-regenerate identity=1 ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="Starting new controller" name=resolve-identity-3957 subsys=controller uuid=34341b95-c843-43fc-99ca-2fefdd4d2997
level=info msg="Launching Cilium health daemon" subsys=daemon
level=info msg="Finished regenerating restored endpoints" regenerated=0 subsys=daemon total=0
level=debug msg="Starting new controller" name=sync-lb-maps-with-k8s-services subsys=controller uuid=f86fe331-a6aa-44b1-b235-59ac97fa473b
level=info msg="Deleted orphan backends" orphanBackends=0 subsys=service
level=debug msg="Controller func execution time: 231.458µs" name=sync-lb-maps-with-k8s-services subsys=controller uuid=f86fe331-a6aa-44b1-b235-59ac97fa473b
level=debug msg="Controller run succeeded; waiting for next controller update or stop" name=sync-lb-maps-with-k8s-services subsys=controller uuid=f86fe331-a6aa-44b1-b235-59ac97fa473b
level=debug msg="Endpoint labels unchanged, skipping resolution of identity" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3957 identity=1 identityLabels="reserved:host" ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Controller func execution time: 40.375µs" name=resolve-identity-3957 subsys=controller uuid=34341b95-c843-43fc-99ca-2fefdd4d2997
level=debug msg="Skipping CiliumEndpoint update because it has no k8s pod name" containerID= controller="sync-to-k8s-ciliumendpoint (3957)" datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 62.75µs" name="sync-to-k8s-ciliumendpoint (3957)" subsys=controller uuid=c65c936a-e669-40ca-a20d-6b4a5aa52d45
level=debug msg="Skipping CiliumEndpoint update because it has no k8s pod name" containerID= controller="sync-to-k8s-ciliumendpoint (3957)" datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 62.916µs" name="sync-to-k8s-ciliumendpoint (3957)" subsys=controller uuid=c65c936a-e669-40ca-a20d-6b4a5aa52d45
level=info msg="Datapath signal listener running" subsys=signal
level=debug msg="Dequeued endpoint from build queue" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Regenerating endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ reason="updated security labels" startTime="2023-07-16 12:01:21.078792173 +0000 UTC m=+3.485975211" subsys=endpoint
level=debug msg="Regenerating endpoint: updated security labels" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3957 endpointState=regenerating identity=1 ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="removing directory" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 directory=3957_next endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_policy_03957 subsys=bpf
level=debug msg="Starting policy recalculation..." containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Forced policy recalculation" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint policy recalculation" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3957 forcedRegeneration=true identity=1 ipv4= ipv6= k8sPodName=/ policyCalculation="&{{{{{0 0} 0 0 {{} 0} {{} 0}}}} {0 0 <nil>} 11208 0}" subsys=endpoint waitingForIdentityCache="&{{{{{0 0} 0 0 {{} 0} {{} 0}}}} {0 0 <nil>} 0 0}" waitingForPolicyRepository="&{{{{{0 0} 0 0 {{} 0} {{} 0}}}} {0 0 <nil>} 375 0}"
level=debug msg="Endpoint has no IP addresses" endpointID=3957 subsys=envoy-manager
level=debug msg="BPF header file hashed (was: \"\")" bpfHeaderfileHash=d6b162efae099e5d5da51d51c571ec20cccdd8f3c83f1392130628eb9048d67c containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="writing header file" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3957 file-path=3957_next/ep_config.h identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="writing header file with DNSRules" DNSRules="map[]" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3957 file-path=3957_next/ep_config.h identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=info msg="Launching Cilium health endpoint" subsys=daemon
level=debug msg="Starting new controller" name=cilium-health-ep subsys=controller uuid=440c0eb1-b90d-42b6-b434-0425a22663d9
level=debug msg="Cannot establish connection to local cilium instance" error="Get \"http://%2Fvar%2Frun%2Fcilium%2Fcilium.sock/v1/healthz\": dial unix /var/run/cilium/cilium.sock: connect: no such file or directory" subsys=cilium-health-launcher
level=debug msg="Killing old health endpoint process" pidfile=/var/run/cilium/state/health-endpoint.pid subsys=cilium-health-launcher
level=debug msg="Didn't find existing cilium-health endpoint to delete" subsys=daemon
level=debug msg="Didn't find existing device" error="Link not found" subsys=cilium-health-launcher veth=cilium_health
level=debug msg="Didn't find existing device" error="Link not found" subsys=cilium-health-launcher veth=lxc_health
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct4_global subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unable to remove cilium-health namespace" error="Unable to delete named netns cilium-health: Cannot remove namespace file \"/var/run/netns/cilium-health\": No such file or directory\n exit status 1" subsys=cilium-health-launcher
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct_any4_global subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct_any4_global subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct4_global subsys=bpf
level=info msg="Serving prometheus metrics on :9962" subsys=daemon
level=info msg="Started healthz status API server" address="127.0.0.1:9879" subsys=daemon
level=info msg="Processing queued endpoint deletion requests from /var/run/cilium/deleteQueue" subsys=daemon
level=info msg="processing 0 queued deletion requests" subsys=daemon
level=info msg="Initializing Cilium API" subsys=daemon
level=debug msg="Preparing to compile BPF" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ regeneration-level=rewrite+load subsys=endpoint
level=debug msg="Load 1-min: 4.11 5-min: 3.00 15min: 2.06" endpointID=3957 subsys=endpoint
level=debug msg="Memory: Total: 3933 Used: 2398 (60.97%) Free: 92 Buffers: 33 Cached: 1409" endpointID=3957 subsys=endpoint
level=debug msg="Swap: Total: 1023 Used: 1009 (98.56%) Free: 14" endpointID=3957 subsys=endpoint
level=debug msg="NAME cilium-agent STATUS [sleep] PID 1 CPU: 7.64% MEM: 2.32% CMDLINE: cilium-agent --config-dir=/tmp/cilium/config-map MEM-EXT: RSS: 91 VMS: 784 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="Created veth pair" subsys=endpoint-connector vethPair="[cilium lxc_health]"
level=debug msg="Starting new controller" name=neighbor-table-refresh subsys=controller uuid=90749931-1352-4d99-9297-a5fcfef193ab
level=info msg="Daemon initialization completed" bootstrapTime=3.31338146s subsys=daemon
level=debug msg="Controller func execution time: 18.75µs" name=neighbor-table-refresh subsys=controller uuid=90749931-1352-4d99-9297-a5fcfef193ab
level=debug msg="Starting new controller" name=link-cache subsys=controller uuid=c86a630b-99d8-4932-a653-8c34270205c6
level=debug msg="Spawning health endpoint with command \"ip\" [\"netns\" \"exec\" \"cilium-health\" \"cilium-health-responder\" \"--listen\" \"4240\" \"--pidfile\" \"/var/run/cilium/state/health-endpoint.pid\"]" subsys=cilium-health-launcher
level=debug msg="Group not found" error="group: unknown group cilium" file-path=/var/run/cilium/cilium.sock group=cilium subsys=api
level=info msg="Serving cilium API at unix:///var/run/cilium/cilium.sock" subsys=daemon
level=info msg="Configuring Hubble server" eventQueueSize=5120 maxFlows=4095 subsys=hubble
level=debug msg="Group not found" error="group: unknown group cilium" file-path=/var/run/cilium/hubble.sock group=cilium subsys=api
level=debug msg="Compiling datapath" clang="clang version 10.0.0 (https://github.com/llvm/llvm-project.git f5ae66a41b809db97c4fc34b29bb76be3a86fbe9)\nTarget: x86_64-unknown-linux-gnu\nThread model: posix\nInstalledDir: /usr/local/bin\n" debug=true llc="LLVM (http://llvm.org/):\n  LLVM version 10.0.0\n  Optimized build.\n  Default target: x86_64-unknown-linux-gnu\n  Host CPU: (unknown)\n\n  Registered Targets:\n    bpf   - BPF (host endian)\n    bpfeb - BPF (big endian)\n    bpfel - BPF (little endian)\n" subsys=datapath-loader
level=debug msg="Launching compiler" args="[-emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_host.c -o -]" subsys=datapath-loader target=clang
level=debug msg="Endpoint creation" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=0 endpointState=waiting-for-identity ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=info msg="Starting local Hubble server" address="unix:///var/run/cilium/hubble.sock" subsys=hubble
level=debug msg="TLS configuration ready" config=tls-server subsys=hubble
level=info msg="Starting Hubble server" address=":4244" subsys=hubble
level=debug msg="Controller func execution time: 1.298875ms" name=link-cache subsys=controller uuid=c86a630b-99d8-4932-a653-8c34270205c6
level=debug msg="Skip pod event using host networking" k8sNamespace=kube-system k8sPodName=cilium-4sh4b new-hostIP=172.26.0.3 new-podIP=172.26.0.3 new-podIPs="[{172.26.0.3} {fc00:c111::3}]" old-hostIP=172.26.0.3 old-podIP=172.26.0.3 old-podIPs="[{172.26.0.3} {fc00:c111::3}]" subsys=k8s-watcher
level=debug msg="Load 1-min: 4.11 5-min: 3.00 15min: 2.06" endpointID=3957 subsys=endpoint
level=debug msg="Memory: Total: 3933 Used: 2478 (63.01%) Free: 75 Buffers: 31 Cached: 1347" endpointID=3957 subsys=endpoint
level=debug msg="Swap: Total: 1023 Used: 1013 (99.01%) Free: 10" endpointID=3957 subsys=endpoint
level=debug msg="NAME cilium-agent STATUS [sleep] PID 1 CPU: 6.72% MEM: 2.20% CMDLINE: cilium-agent --config-dir=/tmp/cilium/config-map MEM-EXT: RSS: 86 VMS: 784 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME clang STATUS [running] PID 247 CPU: 40.37% MEM: 1.48% CMDLINE: clang -emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_host.c -o - MEM-EXT: RSS: 58 VMS: 93 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="Cannot find socket" error="stat /var/run/cilium/health.sock: no such file or directory" file-path=/var/run/cilium/health.sock subsys=cilium-health-launcher
level=debug msg="Sending request for /cluster/nodes ..." subsys=health-server
level=debug msg="Got cilium /cluster/nodes" subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.6 nodeName=kind-kind/kind-worker primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::6" nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.2.167 nodeName=kind-kind/kind-worker primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.2.95 nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.4 nodeName=kind-kind/kind-worker2 primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::4" nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.1.18 nodeName=kind-kind/kind-worker2 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.1.242 nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.4 nodeName=kind-kind/kind-worker2 primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::4" nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.1.18 nodeName=kind-kind/kind-worker2 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.1.242 nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.2 nodeName=kind-kind/kind-control-plane primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::2" nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.0.252 nodeName=kind-kind/kind-control-plane primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.0.61 nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.2 nodeName=kind-kind/kind-control-plane primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::2" nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.0.252 nodeName=kind-kind/kind-control-plane primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.0.61 nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::4" nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.1.18 nodeName=kind-kind/kind-worker2 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.1.242 nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.4 nodeName=kind-kind/kind-worker2 primary=true protocol=icmp subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.3 nodeName=kind-kind/kind-worker3 primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::3" nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.3.91 nodeName=kind-kind/kind-worker3 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.3.223 nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.3.223 nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.3 nodeName=kind-kind/kind-worker3 primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::3" nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.3.91 nodeName=kind-kind/kind-worker3 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.3 nodeName=kind-kind/kind-worker3 primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::3" nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.3.91 nodeName=kind-kind/kind-worker3 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.3.223 nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.2.95 nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.6 nodeName=kind-kind/kind-worker primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::6" nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.2.167 nodeName=kind-kind/kind-worker primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.6 nodeName=kind-kind/kind-worker primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::6" nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.2.167 nodeName=kind-kind/kind-worker primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.2.95 nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.5 nodeName=kind-kind/kind-worker4 primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::5" nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.4.32 nodeName=kind-kind/kind-worker4 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.4.235 nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.4.235 nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.5 nodeName=kind-kind/kind-worker4 primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::5" nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.4.32 nodeName=kind-kind/kind-worker4 primary=false protocol=icmp subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.5 nodeName=kind-kind/kind-worker4 primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::5" nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.4.32 nodeName=kind-kind/kind-worker4 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.4.235 nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.0.61 nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.2 nodeName=kind-kind/kind-control-plane primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::2" nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.0.252 nodeName=kind-kind/kind-control-plane primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::2" nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.0.252 nodeName=kind-kind/kind-control-plane primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.0.61 nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.2 nodeName=kind-kind/kind-control-plane primary=true protocol=icmp subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.2.167 nodeName=kind-kind/kind-worker primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.2.95 nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.6 nodeName=kind-kind/kind-worker primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::6" nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.5 nodeName=kind-kind/kind-worker4 primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::5" nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.4.32 nodeName=kind-kind/kind-worker4 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.4.235 nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::3" nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.3.91 nodeName=kind-kind/kind-worker3 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.3.223 nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.3 nodeName=kind-kind/kind-worker3 primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.1.242 nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.4 nodeName=kind-kind/kind-worker2 primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::4" nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.1.18 nodeName=kind-kind/kind-worker2 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.0.61 nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.2 nodeName=kind-kind/kind-control-plane primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::2" nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.0.252 nodeName=kind-kind/kind-control-plane primary=false protocol=icmp subsys=health-server
level=debug msg="cilium-health agent running" pidfile=/var/run/cilium/state/health-endpoint.pid subsys=cilium-health-launcher
level=debug msg="Adding route" command="ip route add 10.244.3.223/32 dev cilium" netns=cilium-health subsys=cilium-health-launcher
level=debug msg="Adding route" command="ip route add 0.0.0.0/0 via 10.244.3.223 mtu 1500 dev cilium" netns=cilium-health subsys=cilium-health-launcher
level=debug msg="Running \"ip [netns exec cilium-health bash -c ip route add 10.244.3.223/32 dev cilium && ip route add 0.0.0.0/0 via 10.244.3.223 mtu 1500 dev cilium]\"" subsys=cilium-health-launcher
level=debug msg="Probe successful" ipAddr=172.26.0.4 nodeName=kind-kind/kind-worker2 rtt="483.916µs" subsys=health-server
level=debug msg="Probe successful" ipAddr=172.26.0.5 nodeName=kind-kind/kind-worker4 rtt="492.75µs" subsys=health-server
level=debug msg="Probe successful" ipAddr=172.26.0.6 nodeName=kind-kind/kind-worker rtt="497.167µs" subsys=health-server
level=debug msg="Probe successful" ipAddr=172.26.0.3 nodeName=kind-kind/kind-worker3 rtt="505.959µs" subsys=health-server
level=debug msg="Probe successful" ipAddr=172.26.0.2 nodeName=kind-kind/kind-control-plane rtt="504.375µs" subsys=health-server
level=debug msg="Starting new controller" name=endpoint-1588-regeneration-recovery subsys=controller uuid=0d05ebef-3d24-4f06-86ca-937e9acc501a
level=debug msg="creating new EventQueue" name=endpoint-1588 numBufferedEvents=25 subsys=eventqueue
level=info msg="New endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1588 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting new controller" name="sync-to-k8s-ciliumendpoint (1588)" subsys=controller uuid=6bd536b2-cbcf-426c-9dcf-45f0bd9e6add
level=debug msg="Refreshing labels of endpoint" containerID= endpointID=1588 identityLabels="reserved:health" infoLabels= subsys=endpoint
level=debug msg="Assigning security relevant label" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1588 ipv4=10.244.3.91 ipv6= k8sPodName=/ obj="{Key:health Value: Source:reserved}" subsys=endpoint
level=info msg="Resolving identity labels (blocking)" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1588 identityLabels="reserved:health" ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Resolving identity for labels" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1588 identityLabels="reserved:health" ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Resolving identity" identityLabels="reserved:health" subsys=identity-cache
level=debug msg="Resolved reserved identity" identity=health identityLabels="reserved:health" isNew=false subsys=identity-cache
level=debug msg="Assigned new identity to endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1588 identity=4 identityLabels="reserved:health" ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing old and adding new identity" new=4 old="<nil>" subsys=identitymanager
level=debug msg="Set identity for this endpoint" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1588 endpointState=ready ipv4=10.244.3.91 ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=info msg="Identity of endpoint changed" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1588 identity=4 identityLabels="reserved:health" ipv4=10.244.3.91 ipv6= k8sPodName=/ oldIdentity="no identity" subsys=endpoint
level=debug msg="Triggering endpoint regeneration due to updated security labels" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1588 endpointState=waiting-to-regenerate identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="Starting new controller" name=resolve-identity-1588 subsys=controller uuid=9f633cdf-34b7-40e8-96c0-8139dcfa9341
level=debug msg="Controller func execution time: 1.054388542s" name=cilium-health-ep subsys=controller uuid=440c0eb1-b90d-42b6-b434-0425a22663d9
level=debug msg="Endpoint labels unchanged, skipping resolution of identity" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1588 identity=4 identityLabels="reserved:health" ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Controller func execution time: 26.375µs" name=resolve-identity-1588 subsys=controller uuid=9f633cdf-34b7-40e8-96c0-8139dcfa9341
level=debug msg="Dequeued endpoint from build queue" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Regenerating endpoint" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ reason="updated security labels" startTime="2023-07-16 12:01:22.142050049 +0000 UTC m=+4.549233045" subsys=endpoint
level=debug msg="Regenerating endpoint: updated security labels" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1588 endpointState=regenerating identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="removing directory" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 directory=1588_next endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_policy_01588 subsys=bpf
level=debug msg="Starting policy recalculation..." containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Forced policy recalculation" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint policy recalculation" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1588 forcedRegeneration=true identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ policyCalculation="&{{{{{0 0} 0 0 {{} 0} {{} 0}}}} {0 0 <nil>} 33917 0}" subsys=endpoint waitingForIdentityCache="&{{{{{0 0} 0 0 {{} 0} {{} 0}}}} {0 0 <nil>} 0 0}" waitingForPolicyRepository="&{{{{{0 0} 0 0 {{} 0} {{} 0}}}} {0 0 <nil>} 375 0}"
level=debug msg="preparing new cache transaction: upserting 1 entries, deleting 0 entries" subsys=xds xdsCachedVersion=2 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="inserting resource into cache" subsys=xds xdsCachedVersion=2 xdsResourceName=1588 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="committing cache transaction and notifying of new version" subsys=xds xdsCachedVersion=2 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="BPF header file hashed (was: \"\")" bpfHeaderfileHash=c980f992bea92acc6727288e20ee0d49cd3972683dcd39eda9bc45326f35c275 containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="writing header file" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1588 file-path=1588_next/ep_config.h identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="writing header file with DNSRules" DNSRules="map[]" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1588 file-path=1588_next/ep_config.h identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Skipping CiliumEndpoint update because it has no k8s pod name" containerID= controller="sync-to-k8s-ciliumendpoint (1588)" datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 5.954958ms" name="sync-to-k8s-ciliumendpoint (1588)" subsys=controller uuid=6bd536b2-cbcf-426c-9dcf-45f0bd9e6add
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct4_global subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct_any4_global subsys=bpf
level=debug msg="Registered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_snat_v4_external subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct_any4_global subsys=bpf
level=debug msg="Unregistered BPF map" path=/sys/fs/bpf/tc/globals/cilium_ct4_global subsys=bpf
level=debug msg="Preparing to compile BPF" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ regeneration-level=rewrite+load subsys=endpoint
level=debug msg="Skipping CiliumEndpoint update because it has no k8s pod name" containerID= controller="sync-to-k8s-ciliumendpoint (1588)" datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Load 1-min: 4.11 5-min: 3.00 15min: 2.06" endpointID=1588 subsys=endpoint
level=debug msg="Memory: Total: 3933 Used: 2487 (63.25%) Free: 72 Buffers: 31 Cached: 1341" endpointID=1588 subsys=endpoint
level=debug msg="Controller func execution time: 856.334µs" name="sync-to-k8s-ciliumendpoint (1588)" subsys=controller uuid=6bd536b2-cbcf-426c-9dcf-45f0bd9e6add
level=debug msg="Swap: Total: 1023 Used: 1015 (99.15%) Free: 8" endpointID=1588 subsys=endpoint
level=debug msg="NAME cilium-agent STATUS [sleep] PID 1 CPU: 6.83% MEM: 2.21% CMDLINE: cilium-agent --config-dir=/tmp/cilium/config-map MEM-EXT: RSS: 87 VMS: 784 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME clang STATUS [running] PID 247 CPU: 40.47% MEM: 1.48% CMDLINE: clang -emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_host.c -o - MEM-EXT: RSS: 58 VMS: 93 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="Compiling datapath" clang="clang version 10.0.0 (https://github.com/llvm/llvm-project.git f5ae66a41b809db97c4fc34b29bb76be3a86fbe9)\nTarget: x86_64-unknown-linux-gnu\nThread model: posix\nInstalledDir: /usr/local/bin\n" debug=true llc="LLVM (http://llvm.org/):\n  LLVM version 10.0.0\n  Optimized build.\n  Default target: x86_64-unknown-linux-gnu\n  Host CPU: (unknown)\n\n  Registered Targets:\n    bpf   - BPF (host endian)\n    bpfeb - BPF (big endian)\n    bpfel - BPF (little endian)\n" subsys=datapath-loader
level=debug msg="Launching compiler" args="[-emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18 -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_lxc.c -o -]" subsys=datapath-loader target=clang
level=debug msg="Load 1-min: 4.11 5-min: 3.00 15min: 2.06" endpointID=3957 subsys=endpoint
level=debug msg="Memory: Total: 3933 Used: 2584 (65.71%) Free: 90 Buffers: 25 Cached: 1232" endpointID=3957 subsys=endpoint
level=debug msg="Swap: Total: 1023 Used: 1023 (99.99%) Free: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME cilium-agent STATUS [sleep] PID 1 CPU: 5.91% MEM: 1.97% CMDLINE: cilium-agent --config-dir=/tmp/cilium/config-map MEM-EXT: RSS: 77 VMS: 784 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME clang STATUS [zombie] PID 247 CPU: 34.39% MEM: 0.00% CMDLINE:  MEM-EXT: RSS: 0 VMS: 0 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME llc STATUS [running] PID 248 CPU: 7.07% MEM: 1.00% CMDLINE: llc -march=bpf -mcpu=v3 -filetype=obj -o /var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df/bpf_host.dbg.o MEM-EXT: RSS: 39 VMS: 48 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME clang STATUS [running] PID 260 CPU: 21.78% MEM: 1.41% CMDLINE: clang -emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18 -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_lxc.c -o - MEM-EXT: RSS: 55 VMS: 90 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="Cannot find socket" error="stat /var/run/cilium/health.sock: no such file or directory" file-path=/var/run/cilium/health.sock subsys=cilium-health-launcher
level=debug msg="Load 1-min: 4.11 5-min: 3.00 15min: 2.06" endpointID=1588 subsys=endpoint
level=debug msg="Memory: Total: 3933 Used: 2590 (65.85%) Free: 82 Buffers: 25 Cached: 1235" endpointID=1588 subsys=endpoint
level=debug msg="Swap: Total: 1023 Used: 1023 (99.99%) Free: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME cilium-agent STATUS [sleep] PID 1 CPU: 5.87% MEM: 1.97% CMDLINE: cilium-agent --config-dir=/tmp/cilium/config-map MEM-EXT: RSS: 77 VMS: 784 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME clang STATUS [zombie] PID 247 CPU: 33.93% MEM: 0.00% CMDLINE:  MEM-EXT: RSS: 0 VMS: 0 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME llc STATUS [running] PID 248 CPU: 7.29% MEM: 1.03% CMDLINE: llc -march=bpf -mcpu=v3 -filetype=obj -o /var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df/bpf_host.dbg.o MEM-EXT: RSS: 40 VMS: 50 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME clang STATUS [running] PID 260 CPU: 22.75% MEM: 1.41% CMDLINE: clang -emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18 -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_lxc.c -o - MEM-EXT: RSS: 55 VMS: 90 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="Probing for connectivity to node" ipAddr=10.244.1.18 nodeName=kind-kind/kind-worker2 primary=false protocol=http subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.1.242 nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.4 nodeName=kind-kind/kind-worker2 primary=true protocol=http subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::4" nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.2 nodeName=kind-kind/kind-control-plane primary=true protocol=http subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::2" nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.0.252 nodeName=kind-kind/kind-control-plane primary=false protocol=http subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.0.61 nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.4.32 nodeName=kind-kind/kind-worker4 primary=false protocol=http subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.4.235 nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.5 nodeName=kind-kind/kind-worker4 primary=true protocol=http subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::5" nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.2.95 nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.6 nodeName=kind-kind/kind-worker primary=true protocol=http subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::6" nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.2.167 nodeName=kind-kind/kind-worker primary=false protocol=http subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.3 nodeName=kind-kind/kind-worker3 primary=true protocol=http subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::3" nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.3.91 nodeName=kind-kind/kind-worker3 primary=false protocol=http subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.3.223 nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Greeting host" host="http://10.244.1.18:4240" ipAddr=10.244.1.18 nodeName=kind-kind/kind-worker2 path="Via L3" subsys=health-server
level=debug msg="Controller func execution time: 33.458µs" name=metricsmap-bpf-prom-sync subsys=controller uuid=5e26b3a0-3934-459d-a21b-050456120824
level=debug msg="Launching compiler" args="[-emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_host.c -o -]" subsys=datapath-loader target=clang
level=debug msg="Load 1-min: 4.98 5-min: 3.20 15min: 2.13" endpointID=3957 subsys=endpoint
level=debug msg="Memory: Total: 3933 Used: 2518 (64.03%) Free: 159 Buffers: 24 Cached: 1231" endpointID=3957 subsys=endpoint
level=debug msg="Swap: Total: 1023 Used: 1023 (99.99%) Free: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME cilium-agent STATUS [sleep] PID 1 CPU: 5.42% MEM: 1.95% CMDLINE: cilium-agent --config-dir=/tmp/cilium/config-map MEM-EXT: RSS: 76 VMS: 784 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME clang STATUS [running] PID 260 CPU: 31.48% MEM: 1.45% CMDLINE: clang -emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18 -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_lxc.c -o - MEM-EXT: RSS: 56 VMS: 91 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="Cannot find socket" error="stat /var/run/cilium/health.sock: no such file or directory" file-path=/var/run/cilium/health.sock subsys=cilium-health-launcher
level=debug msg="Load 1-min: 4.98 5-min: 3.20 15min: 2.13" endpointID=1588 subsys=endpoint
level=debug msg="Memory: Total: 3933 Used: 2532 (64.38%) Free: 144 Buffers: 24 Cached: 1231" endpointID=1588 subsys=endpoint
level=debug msg="Swap: Total: 1023 Used: 1023 (99.99%) Free: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME cilium-agent STATUS [sleep] PID 1 CPU: 5.39% MEM: 1.95% CMDLINE: cilium-agent --config-dir=/tmp/cilium/config-map MEM-EXT: RSS: 76 VMS: 784 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME clang STATUS [running] PID 260 CPU: 31.69% MEM: 1.45% CMDLINE: clang -emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18 -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_lxc.c -o - MEM-EXT: RSS: 56 VMS: 91 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME clang STATUS [running] PID 273 CPU: 3.46% MEM: 0.91% CMDLINE: clang -emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_host.c -o - MEM-EXT: RSS: 35 VMS: 77 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="Cannot find socket" error="stat /var/run/cilium/health.sock: no such file or directory" file-path=/var/run/cilium/health.sock subsys=cilium-health-launcher
level=debug msg="Load 1-min: 4.98 5-min: 3.20 15min: 2.13" endpointID=3957 subsys=endpoint
level=debug msg="Memory: Total: 3933 Used: 2590 (65.85%) Free: 70 Buffers: 24 Cached: 1248" endpointID=3957 subsys=endpoint
level=debug msg="Swap: Total: 1023 Used: 1023 (99.94%) Free: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME cilium-agent STATUS [sleep] PID 1 CPU: 4.82% MEM: 1.95% CMDLINE: cilium-agent --config-dir=/tmp/cilium/config-map MEM-EXT: RSS: 76 VMS: 784 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME clang STATUS [zombie] PID 260 CPU: 25.23% MEM: 0.00% CMDLINE:  MEM-EXT: RSS: 0 VMS: 0 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME llc STATUS [running] PID 261 CPU: 10.91% MEM: 1.24% CMDLINE: llc -march=bpf -mcpu=v3 -filetype=obj -o /var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18/bpf_lxc.dbg.o MEM-EXT: RSS: 48 VMS: 58 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME clang STATUS [running] PID 273 CPU: 22.61% MEM: 1.40% CMDLINE: clang -emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_host.c -o - MEM-EXT: RSS: 55 VMS: 89 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="Load 1-min: 4.98 5-min: 3.20 15min: 2.13" endpointID=1588 subsys=endpoint
level=debug msg="Memory: Total: 3933 Used: 2594 (65.95%) Free: 64 Buffers: 24 Cached: 1250" endpointID=1588 subsys=endpoint
level=debug msg="Swap: Total: 1023 Used: 1023 (99.94%) Free: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME cilium-agent STATUS [sleep] PID 1 CPU: 4.80% MEM: 1.95% CMDLINE: cilium-agent --config-dir=/tmp/cilium/config-map MEM-EXT: RSS: 76 VMS: 784 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME clang STATUS [zombie] PID 260 CPU: 25.00% MEM: 0.00% CMDLINE:  MEM-EXT: RSS: 0 VMS: 0 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME llc STATUS [running] PID 261 CPU: 11.30% MEM: 1.24% CMDLINE: llc -march=bpf -mcpu=v3 -filetype=obj -o /var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18/bpf_lxc.dbg.o MEM-EXT: RSS: 48 VMS: 58 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME clang STATUS [running] PID 273 CPU: 22.68% MEM: 1.40% CMDLINE: clang -emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_host.c -o - MEM-EXT: RSS: 55 VMS: 89 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="Launching compiler" args="[-emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18 -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_lxc.c -o -]" subsys=datapath-loader target=clang
level=debug msg="UpdateIdentities: Adding a new identity" identity=53560 labels="[k8s:io.cilium.k8s.namespace.labels.kubernetes.io/metadata.name=kube-system k8s:io.cilium.k8s.policy.cluster=kind-kind k8s:io.cilium.k8s.policy.serviceaccount=coredns k8s:io.kubernetes.pod.namespace=kube-system k8s:k8s-app=kube-dns]" subsys=policy
level=debug msg="Regenerating all endpoints" subsys=policy
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=debug msg="Triggering endpoint regeneration due to one or more identities created or deleted" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1588 endpointState=waiting-to-regenerate identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="Triggering endpoint regeneration due to one or more identities created or deleted" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3957 endpointState=waiting-to-regenerate identity=1 ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="Upserting IP into ipcache layer" identity="{53560 custom-resource [] false false}" ipAddr=10.244.0.214 k8sNamespace=kube-system k8sPodName=coredns-565d847f94-q4dpf key=0 namedPorts="map[dns:{53 17} dns-tcp:{53 6} metrics:{9153 6}]" subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity="{53560 custom-resource [] false false}" ipAddr="{10.244.0.214 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Upserting IP into ipcache layer" identity="{53560 custom-resource [] false false}" ipAddr=10.244.0.242 k8sNamespace=kube-system k8sPodName=coredns-565d847f94-cjxrp key=0 namedPorts="map[dns:{53 17} dns-tcp:{53 6} metrics:{9153 6}]" subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity="{53560 custom-resource [] false false}" ipAddr="{10.244.0.242 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Load 1-min: 4.98 5-min: 3.20 15min: 2.13" endpointID=3957 subsys=endpoint
level=debug msg="Memory: Total: 3933 Used: 2606 (66.28%) Free: 88 Buffers: 17 Cached: 1220" endpointID=3957 subsys=endpoint
level=debug msg="Swap: Total: 1023 Used: 1023 (99.96%) Free: 0" endpointID=3957 subsys=endpoint
level=debug msg="Cannot find socket" error="stat /var/run/cilium/health.sock: no such file or directory" file-path=/var/run/cilium/health.sock subsys=cilium-health-launcher
level=debug msg="NAME cilium-agent STATUS [sleep] PID 1 CPU: 4.44% MEM: 1.82% CMDLINE: cilium-agent --config-dir=/tmp/cilium/config-map MEM-EXT: RSS: 71 VMS: 784 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME clang STATUS [running] PID 273 CPU: 27.20% MEM: 1.48% CMDLINE: clang -emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_host.c -o - MEM-EXT: RSS: 58 VMS: 93 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME clang STATUS [running] PID 275 CPU: 23.49% MEM: 1.44% CMDLINE: clang -emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18 -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_lxc.c -o - MEM-EXT: RSS: 56 VMS: 91 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="Load 1-min: 4.98 5-min: 3.20 15min: 2.13" endpointID=1588 subsys=endpoint
level=debug msg="Memory: Total: 3933 Used: 2614 (66.46%) Free: 81 Buffers: 17 Cached: 1220" endpointID=1588 subsys=endpoint
level=debug msg="Swap: Total: 1023 Used: 1023 (99.94%) Free: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME cilium-agent STATUS [sleep] PID 1 CPU: 4.43% MEM: 1.82% CMDLINE: cilium-agent --config-dir=/tmp/cilium/config-map MEM-EXT: RSS: 71 VMS: 784 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME clang STATUS [running] PID 273 CPU: 27.15% MEM: 1.48% CMDLINE: clang -emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_host.c -o - MEM-EXT: RSS: 58 VMS: 93 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME clang STATUS [running] PID 275 CPU: 23.52% MEM: 1.44% CMDLINE: clang -emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18 -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_lxc.c -o - MEM-EXT: RSS: 56 VMS: 91 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="UpdateIdentities: Adding a new identity" identity=42731 labels="[k8s:app=local-path-provisioner k8s:io.cilium.k8s.namespace.labels.kubernetes.io/metadata.name=local-path-storage k8s:io.cilium.k8s.policy.cluster=kind-kind k8s:io.cilium.k8s.policy.serviceaccount=local-path-provisioner-service-account k8s:io.kubernetes.pod.namespace=local-path-storage]" subsys=policy
level=debug msg="Upserting IP into ipcache layer" identity="{42731 custom-resource [] false false}" ipAddr=10.244.0.95 k8sNamespace=local-path-storage k8sPodName=local-path-provisioner-684f458cdd-lpvx2 key=0 namedPorts="map[]" subsys=ipcache
level=debug msg="Daemon notified of IP-Identity cache state change" identity="{42731 custom-resource [] false false}" ipAddr="{10.244.0.95 ffffffff}" modification=Upsert subsys=datapath-ipcache
level=debug msg="Regenerating all endpoints" subsys=policy
level=info msg="regenerating all endpoints" reason="one or more identities created or deleted" subsys=endpoint-manager
level=debug msg="Skipped duplicate endpoint regeneration level no-rebuild trigger due to one or more identities created or deleted" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3957 endpointState=waiting-to-regenerate identity=1 ipv4= ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="Skipped duplicate endpoint regeneration level no-rebuild trigger due to one or more identities created or deleted" code=OK containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1588 endpointState=waiting-to-regenerate identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ policyRevision=0 subsys=endpoint type=0
level=debug msg="Cannot find socket" error="stat /var/run/cilium/health.sock: no such file or directory" file-path=/var/run/cilium/health.sock subsys=cilium-health-launcher
level=debug msg="Load 1-min: 4.98 5-min: 3.20 15min: 2.13" endpointID=3957 subsys=endpoint
level=debug msg="Memory: Total: 3933 Used: 2588 (65.82%) Free: 109 Buffers: 14 Cached: 1220" endpointID=3957 subsys=endpoint
level=debug msg="Swap: Total: 1023 Used: 1022 (99.89%) Free: 1" endpointID=3957 subsys=endpoint
level=debug msg="NAME cilium-agent STATUS [sleep] PID 1 CPU: 4.04% MEM: 1.79% CMDLINE: cilium-agent --config-dir=/tmp/cilium/config-map MEM-EXT: RSS: 70 VMS: 784 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME clang STATUS [zombie] PID 273 CPU: 27.08% MEM: 0.00% CMDLINE:  MEM-EXT: RSS: 0 VMS: 0 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME llc STATUS [running] PID 274 CPU: 1.93% MEM: 0.66% CMDLINE: llc -march=bpf -mcpu=v3 -filetype=asm -o /var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df/bpf_host.asm MEM-EXT: RSS: 26 VMS: 37 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME clang STATUS [running] PID 275 CPU: 28.06% MEM: 1.44% CMDLINE: clang -emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18 -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_lxc.c -o - MEM-EXT: RSS: 56 VMS: 91 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="Load 1-min: 4.98 5-min: 3.20 15min: 2.13" endpointID=1588 subsys=endpoint
level=debug msg="Memory: Total: 3933 Used: 2596 (66.01%) Free: 100 Buffers: 14 Cached: 1221" endpointID=1588 subsys=endpoint
level=debug msg="Swap: Total: 1023 Used: 1022 (99.89%) Free: 1" endpointID=1588 subsys=endpoint
level=debug msg="NAME cilium-agent STATUS [sleep] PID 1 CPU: 4.03% MEM: 1.80% CMDLINE: cilium-agent --config-dir=/tmp/cilium/config-map MEM-EXT: RSS: 70 VMS: 784 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME clang STATUS [zombie] PID 273 CPU: 26.85% MEM: 0.00% CMDLINE:  MEM-EXT: RSS: 0 VMS: 0 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME llc STATUS [running] PID 274 CPU: 2.40% MEM: 0.69% CMDLINE: llc -march=bpf -mcpu=v3 -filetype=asm -o /var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df/bpf_host.asm MEM-EXT: RSS: 27 VMS: 38 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME clang STATUS [running] PID 275 CPU: 28.38% MEM: 1.44% CMDLINE: clang -emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18 -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_lxc.c -o - MEM-EXT: RSS: 56 VMS: 91 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="Cannot find socket" error="stat /var/run/cilium/health.sock: no such file or directory" file-path=/var/run/cilium/health.sock subsys=cilium-health-launcher
level=debug msg="Load 1-min: 4.98 5-min: 3.20 15min: 2.13" endpointID=3957 subsys=endpoint
level=debug msg="Memory: Total: 3933 Used: 2608 (66.31%) Free: 81 Buffers: 14 Cached: 1229" endpointID=3957 subsys=endpoint
level=debug msg="Swap: Total: 1023 Used: 1023 (99.97%) Free: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME cilium-agent STATUS [sleep] PID 1 CPU: 3.79% MEM: 1.78% CMDLINE: cilium-agent --config-dir=/tmp/cilium/config-map MEM-EXT: RSS: 69 VMS: 784 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME clang STATUS [zombie] PID 273 CPU: 21.79% MEM: 0.00% CMDLINE:  MEM-EXT: RSS: 0 VMS: 0 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME llc STATUS [running] PID 274 CPU: 10.70% MEM: 1.33% CMDLINE: llc -march=bpf -mcpu=v3 -filetype=asm -o /var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df/bpf_host.asm MEM-EXT: RSS: 52 VMS: 62 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME clang STATUS [zombie] PID 275 CPU: 25.60% MEM: 0.00% CMDLINE:  MEM-EXT: RSS: 0 VMS: 0 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME llc STATUS [running] PID 276 CPU: 6.28% MEM: 1.12% CMDLINE: llc -march=bpf -mcpu=v3 -filetype=asm -o /var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18/bpf_lxc.asm MEM-EXT: RSS: 44 VMS: 54 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="Load 1-min: 4.98 5-min: 3.20 15min: 2.13" endpointID=1588 subsys=endpoint
level=debug msg="Memory: Total: 3933 Used: 2623 (66.70%) Free: 65 Buffers: 14 Cached: 1230" endpointID=1588 subsys=endpoint
level=debug msg="Swap: Total: 1023 Used: 1023 (99.97%) Free: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME cilium-agent STATUS [sleep] PID 1 CPU: 3.78% MEM: 1.78% CMDLINE: cilium-agent --config-dir=/tmp/cilium/config-map MEM-EXT: RSS: 70 VMS: 784 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME clang STATUS [zombie] PID 273 CPU: 21.60% MEM: 0.00% CMDLINE:  MEM-EXT: RSS: 0 VMS: 0 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME llc STATUS [running] PID 274 CPU: 10.99% MEM: 1.33% CMDLINE: llc -march=bpf -mcpu=v3 -filetype=asm -o /var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df/bpf_host.asm MEM-EXT: RSS: 52 VMS: 62 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME clang STATUS [zombie] PID 275 CPU: 25.33% MEM: 0.00% CMDLINE:  MEM-EXT: RSS: 0 VMS: 0 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME llc STATUS [running] PID 276 CPU: 6.45% MEM: 1.19% CMDLINE: llc -march=bpf -mcpu=v3 -filetype=asm -o /var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18/bpf_lxc.asm MEM-EXT: RSS: 46 VMS: 56 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="Launching compiler" args="[-E -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_host.c -o /var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df/bpf_host.c]" subsys=datapath-loader target=clang
level=debug msg="Launching compiler" args="[-emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_host.c -o -]" subsys=datapath-loader target=clang
level=debug msg="Controller func execution time: 256.459µs" name=metricsmap-bpf-prom-sync subsys=controller uuid=5e26b3a0-3934-459d-a21b-050456120824
level=debug msg="Launching compiler" args="[-E -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18 -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_lxc.c -o /var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18/bpf_lxc.c]" subsys=datapath-loader target=clang
level=debug msg="Launching compiler" args="[-emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18 -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_lxc.c -o -]" subsys=datapath-loader target=clang
level=debug msg="Cannot find socket" error="stat /var/run/cilium/health.sock: no such file or directory" file-path=/var/run/cilium/health.sock subsys=cilium-health-launcher
level=debug msg="Load 1-min: 5.38 5-min: 3.31 15min: 2.17" endpointID=3957 subsys=endpoint
level=debug msg="Memory: Total: 3933 Used: 2597 (66.03%) Free: 137 Buffers: 13 Cached: 1185" endpointID=3957 subsys=endpoint
level=debug msg="Swap: Total: 1023 Used: 1023 (99.99%) Free: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME cilium-agent STATUS [sleep] PID 1 CPU: 3.57% MEM: 1.59% CMDLINE: cilium-agent --config-dir=/tmp/cilium/config-map MEM-EXT: RSS: 62 VMS: 784 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME clang STATUS [running] PID 300 CPU: 15.10% MEM: 1.39% CMDLINE: clang -emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_host.c -o - MEM-EXT: RSS: 54 VMS: 89 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="Load 1-min: 5.38 5-min: 3.31 15min: 2.17" endpointID=1588 subsys=endpoint
level=debug msg="NAME clang STATUS [running] PID 303 CPU: 5.91% MEM: 1.04% CMDLINE: clang -emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18 -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_lxc.c -o - MEM-EXT: RSS: 40 VMS: 81 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="Memory: Total: 3933 Used: 2607 (66.28%) Free: 121 Buffers: 13 Cached: 1191" endpointID=1588 subsys=endpoint
level=debug msg="Swap: Total: 1023 Used: 1023 (99.99%) Free: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME cilium-agent STATUS [sleep] PID 1 CPU: 3.56% MEM: 1.59% CMDLINE: cilium-agent --config-dir=/tmp/cilium/config-map MEM-EXT: RSS: 62 VMS: 784 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME clang STATUS [running] PID 300 CPU: 15.09% MEM: 1.39% CMDLINE: clang -emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_host.c -o - MEM-EXT: RSS: 54 VMS: 89 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME clang STATUS [running] PID 303 CPU: 5.90% MEM: 1.04% CMDLINE: clang -emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18 -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_lxc.c -o - MEM-EXT: RSS: 40 VMS: 81 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="Cannot find socket" error="stat /var/run/cilium/health.sock: no such file or directory" file-path=/var/run/cilium/health.sock subsys=cilium-health-launcher
level=debug msg="Load 1-min: 5.38 5-min: 3.31 15min: 2.17" endpointID=3957 subsys=endpoint
level=debug msg="Memory: Total: 3933 Used: 2607 (66.29%) Free: 105 Buffers: 13 Cached: 1206" endpointID=3957 subsys=endpoint
level=debug msg="Swap: Total: 1023 Used: 1023 (99.99%) Free: 0" endpointID=3957 subsys=endpoint
level=debug msg="Load 1-min: 5.38 5-min: 3.31 15min: 2.17" endpointID=1588 subsys=endpoint
level=debug msg="Memory: Total: 3933 Used: 2607 (66.29%) Free: 105 Buffers: 13 Cached: 1206" endpointID=1588 subsys=endpoint
level=debug msg="Swap: Total: 1023 Used: 1023 (99.99%) Free: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME cilium-agent STATUS [sleep] PID 1 CPU: 3.31% MEM: 1.60% CMDLINE: cilium-agent --config-dir=/tmp/cilium/config-map MEM-EXT: RSS: 62 VMS: 784 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME clang STATUS [running] PID 300 CPU: 24.43% MEM: 1.48% CMDLINE: clang -emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_host.c -o - MEM-EXT: RSS: 58 VMS: 93 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME clang STATUS [running] PID 303 CPU: 27.35% MEM: 1.44% CMDLINE: clang -emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18 -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_lxc.c -o - MEM-EXT: RSS: 56 VMS: 91 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME cilium-agent STATUS [sleep] PID 1 CPU: 3.31% MEM: 1.60% CMDLINE: cilium-agent --config-dir=/tmp/cilium/config-map MEM-EXT: RSS: 62 VMS: 784 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME clang STATUS [running] PID 300 CPU: 24.70% MEM: 1.48% CMDLINE: clang -emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_host.c -o - MEM-EXT: RSS: 58 VMS: 93 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME clang STATUS [running] PID 303 CPU: 27.30% MEM: 1.44% CMDLINE: clang -emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18 -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_lxc.c -o - MEM-EXT: RSS: 56 VMS: 91 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="Skipping CiliumEndpoint update because it has no k8s pod name" containerID= controller="sync-to-k8s-ciliumendpoint (3957)" datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 10.899916ms" name="sync-to-k8s-ciliumendpoint (3957)" subsys=controller uuid=c65c936a-e669-40ca-a20d-6b4a5aa52d45
level=debug msg="Cannot find socket" error="stat /var/run/cilium/health.sock: no such file or directory" file-path=/var/run/cilium/health.sock subsys=cilium-health-launcher
level=debug msg="Load 1-min: 5.38 5-min: 3.31 15min: 2.17" endpointID=1588 subsys=endpoint
level=debug msg="Memory: Total: 3933 Used: 2640 (67.13%) Free: 251 Buffers: 9 Cached: 1032" endpointID=1588 subsys=endpoint
level=debug msg="Swap: Total: 1023 Used: 1023 (100.00%) Free: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME cilium-agent STATUS [sleep] PID 1 CPU: 3.09% MEM: 1.45% CMDLINE: cilium-agent --config-dir=/tmp/cilium/config-map MEM-EXT: RSS: 56 VMS: 784 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME clang STATUS [zombie] PID 300 CPU: 26.21% MEM: 0.00% CMDLINE:  MEM-EXT: RSS: 0 VMS: 0 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME llc STATUS [running] PID 301 CPU: 1.43% MEM: 0.62% CMDLINE: llc -march=bpf -mcpu=v3 -filetype=obj -o /var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df/bpf_host.o MEM-EXT: RSS: 24 VMS: 37 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="Load 1-min: 5.38 5-min: 3.31 15min: 2.17" endpointID=3957 subsys=endpoint
level=debug msg="Memory: Total: 3933 Used: 2631 (66.91%) Free: 259 Buffers: 9 Cached: 1032" endpointID=3957 subsys=endpoint
level=debug msg="Swap: Total: 1023 Used: 1023 (100.00%) Free: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME clang STATUS [running] PID 303 CPU: 31.91% MEM: 1.48% CMDLINE: clang -emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18 -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_lxc.c -o - MEM-EXT: RSS: 58 VMS: 92 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME cilium-agent STATUS [sleep] PID 1 CPU: 3.09% MEM: 1.45% CMDLINE: cilium-agent --config-dir=/tmp/cilium/config-map MEM-EXT: RSS: 57 VMS: 784 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME clang STATUS [zombie] PID 300 CPU: 26.16% MEM: 0.00% CMDLINE:  MEM-EXT: RSS: 0 VMS: 0 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME llc STATUS [running] PID 301 CPU: 1.43% MEM: 0.62% CMDLINE: llc -march=bpf -mcpu=v3 -filetype=obj -o /var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df/bpf_host.o MEM-EXT: RSS: 24 VMS: 37 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME clang STATUS [running] PID 303 CPU: 32.13% MEM: 1.48% CMDLINE: clang -emit-llvm -g -O2 -target bpf -std=gnu89 -nostdinc -D__NR_CPUS__=5 -Wall -Wextra -Werror -Wshadow -Wno-address-of-packed-member -Wno-unknown-warning-option -Wno-gnu-variable-sized-type-not-at-end -Wdeclaration-after-statement -Wimplicit-int-conversion -Wenum-conversion -I/var/run/cilium/state/globals -I/var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18 -I/var/lib/cilium/bpf -I/var/lib/cilium/bpf/include -c /var/lib/cilium/bpf/bpf_lxc.c -o - MEM-EXT: RSS: 58 VMS: 92 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="Cannot find socket" error="stat /var/run/cilium/health.sock: no such file or directory" file-path=/var/run/cilium/health.sock subsys=cilium-health-launcher
level=debug msg="Skipping CiliumEndpoint update because it has no k8s pod name" containerID= controller="sync-to-k8s-ciliumendpoint (1588)" datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpointsynchronizer
level=debug msg="Controller func execution time: 88.458µs" name="sync-to-k8s-ciliumendpoint (1588)" subsys=controller uuid=6bd536b2-cbcf-426c-9dcf-45f0bd9e6add
level=debug msg="Load 1-min: 5.38 5-min: 3.31 15min: 2.17" endpointID=3957 subsys=endpoint
level=debug msg="Load 1-min: 5.38 5-min: 3.31 15min: 2.17" endpointID=1588 subsys=endpoint
level=debug msg="Memory: Total: 3933 Used: 2697 (68.57%) Free: 216 Buffers: 8 Cached: 1010" endpointID=1588 subsys=endpoint
level=debug msg="Swap: Total: 1023 Used: 1023 (100.00%) Free: 0" endpointID=1588 subsys=endpoint
level=debug msg="Memory: Total: 3933 Used: 2697 (68.57%) Free: 216 Buffers: 8 Cached: 1010" endpointID=3957 subsys=endpoint
level=debug msg="Swap: Total: 1023 Used: 1023 (100.00%) Free: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME cilium-agent STATUS [sleep] PID 1 CPU: 2.96% MEM: 1.39% CMDLINE: cilium-agent --config-dir=/tmp/cilium/config-map MEM-EXT: RSS: 54 VMS: 784 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME cilium-agent STATUS [sleep] PID 1 CPU: 2.96% MEM: 1.39% CMDLINE: cilium-agent --config-dir=/tmp/cilium/config-map MEM-EXT: RSS: 54 VMS: 784 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME clang STATUS [zombie] PID 300 CPU: 21.08% MEM: 0.00% CMDLINE:  MEM-EXT: RSS: 0 VMS: 0 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME llc STATUS [running] PID 301 CPU: 11.11% MEM: 1.29% CMDLINE: llc -march=bpf -mcpu=v3 -filetype=obj -o /var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df/bpf_host.o MEM-EXT: RSS: 50 VMS: 60 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME clang STATUS [zombie] PID 303 CPU: 24.89% MEM: 0.00% CMDLINE:  MEM-EXT: RSS: 0 VMS: 0 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME llc STATUS [running] PID 304 CPU: 9.48% MEM: 1.24% CMDLINE: llc -march=bpf -mcpu=v3 -filetype=obj -o /var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18/bpf_lxc.o MEM-EXT: RSS: 48 VMS: 58 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME clang STATUS [zombie] PID 300 CPU: 21.08% MEM: 0.00% CMDLINE:  MEM-EXT: RSS: 0 VMS: 0 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME llc STATUS [running] PID 301 CPU: 11.11% MEM: 1.29% CMDLINE: llc -march=bpf -mcpu=v3 -filetype=obj -o /var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df/bpf_host.o MEM-EXT: RSS: 50 VMS: 60 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME clang STATUS [zombie] PID 303 CPU: 24.88% MEM: 0.00% CMDLINE:  MEM-EXT: RSS: 0 VMS: 0 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=debug msg="NAME llc STATUS [running] PID 304 CPU: 9.48% MEM: 1.24% CMDLINE: llc -march=bpf -mcpu=v3 -filetype=obj -o /var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18/bpf_lxc.o MEM-EXT: RSS: 48 VMS: 58 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=1588 subsys=endpoint
level=info msg="Compiled new BPF template" BPFCompilationTime=11.213023713s file-path=/var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df/bpf_host.o subsys=datapath-loader
level=debug msg="Watching template path" file-path=/var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df/bpf_host.o subsys=datapath-loader
level=debug msg="Found variable with offset 203460" subsys=elf symbol=IPV4_MASQUERADE
level=debug msg="Found variable with offset 203456" subsys=elf symbol=NATIVE_DEV_IFINDEX
level=debug msg="Found variable with offset 203468" subsys=elf symbol=NODE_MAC_1
level=debug msg="Found variable with offset 203472" subsys=elf symbol=NODE_MAC_2
level=debug msg="Found variable with offset 203484" subsys=elf symbol=POLICY_VERDICT_LOG_FILTER
level=debug msg="Found variable with offset 203464" subsys=elf symbol=SECCTX_FROM_IPCACHE
level=debug msg="Found variable with offset 203476" subsys=elf symbol=SECLABEL
level=debug msg="Found variable with offset 203480" subsys=elf symbol=SECLABEL_NB
level=debug msg="Found symbol referring to unknown section id 28" subsys=elf symbol=____license
level=debug msg="Skipping __send_drop_notify" subsys=elf
level=debug msg="Skipping cil_from_host" subsys=elf
level=debug msg="Skipping cil_from_netdev" subsys=elf
level=debug msg="Skipping cil_to_host" subsys=elf
level=debug msg="Skipping cil_to_netdev" subsys=elf
level=debug msg="Found symbol with offset 1574941" subsys=elf symbol=cilium_auth_map
level=debug msg="Found symbol with offset 1574592" subsys=elf symbol=cilium_call_policy
level=debug msg="Found symbol with offset 1579733" subsys=elf symbol=cilium_calls_hostns_65535
level=debug msg="Found symbol with offset 1575035" subsys=elf symbol=cilium_ct4_global
level=debug msg="Found symbol with offset 1575013" subsys=elf symbol=cilium_ct_any4_global
level=debug msg="Found symbol with offset 1575121" subsys=elf symbol=cilium_encrypt_state
level=debug msg="Found symbol with offset 1574835" subsys=elf symbol=cilium_events
level=debug msg="Found symbol with offset 1575187" subsys=elf symbol=cilium_ipcache
level=debug msg="Found symbol with offset 1574855" subsys=elf symbol=cilium_ipv4_frag_datagrams
level=debug msg="Found symbol with offset 1574553" subsys=elf symbol=cilium_lb4_affinity
level=debug msg="Found symbol with offset 1581096" subsys=elf symbol=cilium_lb4_backends_v3
level=debug msg="Found symbol with offset 1575279" subsys=elf symbol=cilium_lb4_crab
level=debug msg="Found symbol with offset 1575078" subsys=elf symbol=cilium_lb4_crab_long
level=debug msg="Found symbol with offset 1574812" subsys=elf symbol=cilium_lb4_reverse_nat
level=debug msg="Found symbol with offset 1581918" subsys=elf symbol=cilium_lb4_services_v2
level=debug msg="Found symbol with offset 1575202" subsys=elf symbol=cilium_lb4_source_range
level=debug msg="Found symbol with offset 1575053" subsys=elf symbol=cilium_lb_affinity_match
level=debug msg="Found symbol with offset 1575226" subsys=elf symbol=cilium_lxc
level=debug msg="Found symbol with offset 1574915" subsys=elf symbol=cilium_metrics
level=debug msg="Found symbol with offset 1574957" subsys=elf symbol=cilium_node_map
level=debug msg="Found symbol with offset 1580203" subsys=elf symbol=cilium_nodeport_neigh4
level=debug msg="Found symbol with offset 1579713" subsys=elf symbol=cilium_policy_65535
level=debug msg="Found symbol with offset 1575099" subsys=elf symbol=cilium_runtime_config
level=debug msg="Found symbol with offset 1574882" subsys=elf symbol=cilium_signals
level=debug msg="Found symbol with offset 1574989" subsys=elf symbol=cilium_snat_v4_external
level=debug msg="Found symbol with offset 1580178" subsys=elf symbol=cilium_tail_call_buffer4
level=debug msg="Skipping tail_handle_ipv4_from_host" subsys=elf
level=debug msg="Skipping tail_handle_ipv4_from_netdev" subsys=elf
level=debug msg="Skipping tail_handle_snat_fwd_ipv4" subsys=elf
level=debug msg="Skipping tail_nodeport_nat_egress_ipv4" subsys=elf
level=debug msg="Skipping tail_nodeport_nat_ingress_ipv4" subsys=elf
level=debug msg="Skipping tail_rev_nodeport_lb4" subsys=elf
level=debug msg="Found section with offset 1574688" subsys=elf symbol=to-netdev
level=debug msg="Found section with offset 1574702" subsys=elf symbol=from-netdev
level=debug msg="Found section with offset 1574724" subsys=elf symbol=.BTF.ext
level=debug msg="Found section with offset 1574790" subsys=elf symbol=to-host
level=debug msg="Found section with offset 1574802" subsys=elf symbol=from-host
level=debug msg="Found section with offset 1574901" subsys=elf symbol=.debug_ranges
level=debug msg="Found section with offset 1574977" subsys=elf symbol=.debug_info
level=debug msg="Found section with offset 1575158" subsys=elf symbol=.debug_line
level=debug msg="Found section with offset 1575174" subsys=elf symbol=.debug_frame
level=debug msg="Found section with offset 1575241" subsys=elf symbol=.debug_loc
level=debug msg="Found section with offset 1575359" subsys=elf symbol=.BTF
level=debug msg="Found section with offset 1576979" subsys=elf symbol=2/38
level=debug msg="Found section with offset 1578214" subsys=elf symbol=2/17
level=debug msg="Found section with offset 1578340" subsys=elf symbol=2/7
level=debug msg="Found section with offset 1578934" subsys=elf symbol=2/36
level=debug msg="Found section with offset 1580041" subsys=elf symbol=2/15
level=debug msg="Found section with offset 1582655" subsys=elf symbol=2/22
level=debug msg="Found section with offset 1583717" subsys=elf symbol=2/1
level=debug msg="Finished writing ELF" error="<nil>" new-elf-path=3957_next/bpf_host.o subsys=elf template-path=/var/run/cilium/state/templates/c851c91cc8a07d6a23683d98970b7fec4ec8486102073cf617a93c200d0909df/bpf_host.o
level=debug msg="Found variable with offset 203460" subsys=elf symbol=IPV4_MASQUERADE
level=debug msg="Found variable with offset 203456" subsys=elf symbol=NATIVE_DEV_IFINDEX
level=debug msg="Found variable with offset 203468" subsys=elf symbol=NODE_MAC_1
level=debug msg="Found variable with offset 203472" subsys=elf symbol=NODE_MAC_2
level=debug msg="Found variable with offset 203484" subsys=elf symbol=POLICY_VERDICT_LOG_FILTER
level=debug msg="Found variable with offset 203464" subsys=elf symbol=SECCTX_FROM_IPCACHE
level=debug msg="Found variable with offset 203476" subsys=elf symbol=SECLABEL
level=debug msg="Found variable with offset 203480" subsys=elf symbol=SECLABEL_NB
level=debug msg="Found symbol referring to unknown section id 28" subsys=elf symbol=____license
level=debug msg="Skipping __send_drop_notify" subsys=elf
level=debug msg="Skipping cil_from_host" subsys=elf
level=debug msg="Skipping cil_from_netdev" subsys=elf
level=debug msg="Skipping cil_to_host" subsys=elf
level=debug msg="Skipping cil_to_netdev" subsys=elf
level=debug msg="Found symbol with offset 1574941" subsys=elf symbol=cilium_auth_map
level=debug msg="Found symbol with offset 1574592" subsys=elf symbol=cilium_call_policy
level=debug msg="Found symbol with offset 1579733" subsys=elf symbol=cilium_calls_hostns_03957
level=debug msg="Found symbol with offset 1575035" subsys=elf symbol=cilium_ct4_global
level=debug msg="Found symbol with offset 1575013" subsys=elf symbol=cilium_ct_any4_global
level=debug msg="Found symbol with offset 1575121" subsys=elf symbol=cilium_encrypt_state
level=debug msg="Found symbol with offset 1574835" subsys=elf symbol=cilium_events
level=debug msg="Found symbol with offset 1575187" subsys=elf symbol=cilium_ipcache
level=debug msg="Found symbol with offset 1574855" subsys=elf symbol=cilium_ipv4_frag_datagrams
level=debug msg="Found symbol with offset 1574553" subsys=elf symbol=cilium_lb4_affinity
level=debug msg="Found symbol with offset 1581096" subsys=elf symbol=cilium_lb4_backends_v3
level=debug msg="Found symbol with offset 1575279" subsys=elf symbol=cilium_lb4_crab
level=debug msg="Found symbol with offset 1575078" subsys=elf symbol=cilium_lb4_crab_long
level=debug msg="Found symbol with offset 1574812" subsys=elf symbol=cilium_lb4_reverse_nat
level=debug msg="Found symbol with offset 1581918" subsys=elf symbol=cilium_lb4_services_v2
level=debug msg="Found symbol with offset 1575202" subsys=elf symbol=cilium_lb4_source_range
level=debug msg="Found symbol with offset 1575053" subsys=elf symbol=cilium_lb_affinity_match
level=debug msg="Found symbol with offset 1575226" subsys=elf symbol=cilium_lxc
level=debug msg="Found symbol with offset 1574915" subsys=elf symbol=cilium_metrics
level=debug msg="Found symbol with offset 1574957" subsys=elf symbol=cilium_node_map
level=debug msg="Found symbol with offset 1580203" subsys=elf symbol=cilium_nodeport_neigh4
level=debug msg="Found symbol with offset 1579713" subsys=elf symbol=cilium_policy_03957
level=debug msg="Found symbol with offset 1575099" subsys=elf symbol=cilium_runtime_config
level=debug msg="Found symbol with offset 1574882" subsys=elf symbol=cilium_signals
level=debug msg="Found symbol with offset 1574989" subsys=elf symbol=cilium_snat_v4_external
level=debug msg="Found symbol with offset 1580178" subsys=elf symbol=cilium_tail_call_buffer4
level=debug msg="Skipping tail_handle_ipv4_from_host" subsys=elf
level=debug msg="Skipping tail_handle_ipv4_from_netdev" subsys=elf
level=debug msg="Skipping tail_handle_snat_fwd_ipv4" subsys=elf
level=debug msg="Skipping tail_nodeport_nat_egress_ipv4" subsys=elf
level=debug msg="Skipping tail_nodeport_nat_ingress_ipv4" subsys=elf
level=debug msg="Skipping tail_rev_nodeport_lb4" subsys=elf
level=debug msg="Found section with offset 1574688" subsys=elf symbol=to-netdev
level=debug msg="Found section with offset 1574702" subsys=elf symbol=from-netdev
level=debug msg="Found section with offset 1574724" subsys=elf symbol=.BTF.ext
level=debug msg="Found section with offset 1574790" subsys=elf symbol=to-host
level=debug msg="Found section with offset 1574802" subsys=elf symbol=from-host
level=debug msg="Found section with offset 1574901" subsys=elf symbol=.debug_ranges
level=debug msg="Found section with offset 1574977" subsys=elf symbol=.debug_info
level=debug msg="Found section with offset 1575158" subsys=elf symbol=.debug_line
level=debug msg="Found section with offset 1575174" subsys=elf symbol=.debug_frame
level=debug msg="Found section with offset 1575241" subsys=elf symbol=.debug_loc
level=debug msg="Found section with offset 1575359" subsys=elf symbol=.BTF
level=debug msg="Found section with offset 1576979" subsys=elf symbol=2/38
level=debug msg="Found section with offset 1578214" subsys=elf symbol=2/17
level=debug msg="Found section with offset 1578340" subsys=elf symbol=2/7
level=debug msg="Found section with offset 1578934" subsys=elf symbol=2/36
level=debug msg="Found section with offset 1580041" subsys=elf symbol=2/15
level=debug msg="Found section with offset 1582655" subsys=elf symbol=2/22
level=debug msg="Found section with offset 1583717" subsys=elf symbol=2/1
level=debug msg="Finished writing ELF" error="<nil>" new-elf-path=3957_next/bpf_host_cilium_net.o subsys=elf template-path=3957_next/bpf_host.o
level=debug msg="Found variable with offset 203460" subsys=elf symbol=IPV4_MASQUERADE
level=debug msg="Found variable with offset 203456" subsys=elf symbol=NATIVE_DEV_IFINDEX
level=debug msg="Found variable with offset 203468" subsys=elf symbol=NODE_MAC_1
level=debug msg="Found variable with offset 203472" subsys=elf symbol=NODE_MAC_2
level=debug msg="Found variable with offset 203484" subsys=elf symbol=POLICY_VERDICT_LOG_FILTER
level=debug msg="Found variable with offset 203464" subsys=elf symbol=SECCTX_FROM_IPCACHE
level=debug msg="Found variable with offset 203476" subsys=elf symbol=SECLABEL
level=debug msg="Found variable with offset 203480" subsys=elf symbol=SECLABEL_NB
level=debug msg="Found symbol referring to unknown section id 28" subsys=elf symbol=____license
level=debug msg="Skipping __send_drop_notify" subsys=elf
level=debug msg="Skipping cil_from_host" subsys=elf
level=debug msg="Skipping cil_from_netdev" subsys=elf
level=debug msg="Skipping cil_to_host" subsys=elf
level=debug msg="Skipping cil_to_netdev" subsys=elf
level=debug msg="Found symbol with offset 1574941" subsys=elf symbol=cilium_auth_map
level=debug msg="Found symbol with offset 1574592" subsys=elf symbol=cilium_call_policy
level=debug msg="Found symbol with offset 1579733" subsys=elf symbol=cilium_calls_hostns_03957
level=debug msg="Found symbol with offset 1575035" subsys=elf symbol=cilium_ct4_global
level=debug msg="Found symbol with offset 1575013" subsys=elf symbol=cilium_ct_any4_global
level=debug msg="Found symbol with offset 1575121" subsys=elf symbol=cilium_encrypt_state
level=debug msg="Found symbol with offset 1574835" subsys=elf symbol=cilium_events
level=debug msg="Found symbol with offset 1575187" subsys=elf symbol=cilium_ipcache
level=debug msg="Found symbol with offset 1574855" subsys=elf symbol=cilium_ipv4_frag_datagrams
level=debug msg="Found symbol with offset 1574553" subsys=elf symbol=cilium_lb4_affinity
level=debug msg="Found symbol with offset 1581096" subsys=elf symbol=cilium_lb4_backends_v3
level=debug msg="Found symbol with offset 1575279" subsys=elf symbol=cilium_lb4_crab
level=debug msg="Found symbol with offset 1575078" subsys=elf symbol=cilium_lb4_crab_long
level=debug msg="Found symbol with offset 1574812" subsys=elf symbol=cilium_lb4_reverse_nat
level=debug msg="Found symbol with offset 1581918" subsys=elf symbol=cilium_lb4_services_v2
level=debug msg="Found symbol with offset 1575202" subsys=elf symbol=cilium_lb4_source_range
level=debug msg="Found symbol with offset 1575053" subsys=elf symbol=cilium_lb_affinity_match
level=debug msg="Found symbol with offset 1575226" subsys=elf symbol=cilium_lxc
level=debug msg="Found symbol with offset 1574915" subsys=elf symbol=cilium_metrics
level=debug msg="Found symbol with offset 1574957" subsys=elf symbol=cilium_node_map
level=debug msg="Found symbol with offset 1580203" subsys=elf symbol=cilium_nodeport_neigh4
level=debug msg="Found symbol with offset 1579713" subsys=elf symbol=cilium_policy_03957
level=debug msg="Found symbol with offset 1575099" subsys=elf symbol=cilium_runtime_config
level=debug msg="Found symbol with offset 1574882" subsys=elf symbol=cilium_signals
level=debug msg="Found symbol with offset 1574989" subsys=elf symbol=cilium_snat_v4_external
level=debug msg="Found symbol with offset 1580178" subsys=elf symbol=cilium_tail_call_buffer4
level=debug msg="Skipping tail_handle_ipv4_from_host" subsys=elf
level=debug msg="Skipping tail_handle_ipv4_from_netdev" subsys=elf
level=debug msg="Skipping tail_handle_snat_fwd_ipv4" subsys=elf
level=debug msg="Skipping tail_nodeport_nat_egress_ipv4" subsys=elf
level=debug msg="Skipping tail_nodeport_nat_ingress_ipv4" subsys=elf
level=debug msg="Skipping tail_rev_nodeport_lb4" subsys=elf
level=debug msg="Found section with offset 1574688" subsys=elf symbol=to-netdev
level=debug msg="Found section with offset 1574702" subsys=elf symbol=from-netdev
level=debug msg="Found section with offset 1574724" subsys=elf symbol=.BTF.ext
level=debug msg="Found section with offset 1574790" subsys=elf symbol=to-host
level=debug msg="Found section with offset 1574802" subsys=elf symbol=from-host
level=debug msg="Found section with offset 1574901" subsys=elf symbol=.debug_ranges
level=debug msg="Found section with offset 1574977" subsys=elf symbol=.debug_info
level=debug msg="Found section with offset 1575158" subsys=elf symbol=.debug_line
level=debug msg="Found section with offset 1575174" subsys=elf symbol=.debug_frame
level=debug msg="Found section with offset 1575241" subsys=elf symbol=.debug_loc
level=debug msg="Found section with offset 1575359" subsys=elf symbol=.BTF
level=debug msg="Found section with offset 1576979" subsys=elf symbol=2/38
level=debug msg="Found section with offset 1578214" subsys=elf symbol=2/17
level=debug msg="Found section with offset 1578340" subsys=elf symbol=2/7
level=debug msg="Found section with offset 1578934" subsys=elf symbol=2/36
level=debug msg="Found section with offset 1580041" subsys=elf symbol=2/15
level=debug msg="Found section with offset 1582655" subsys=elf symbol=2/22
level=debug msg="Found section with offset 1583717" subsys=elf symbol=2/1
level=debug msg="Finished writing ELF" error="<nil>" new-elf-path=3957_next/bpf_netdev_eth0.o subsys=elf template-path=3957_next/bpf_host.o
level=debug msg="Loading CollectionSpec from ELF" device=cilium_host ifindex=5 objPath=3957_next/bpf_host.o subsys=datapath-loader
level=debug msg="Loading Collection into kernel" device=cilium_host ifindex=5 objPath=3957_next/bpf_host.o subsys=datapath-loader
level=info msg="Compiled new BPF template" BPFCompilationTime=10.34686163s file-path=/var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18/bpf_lxc.o subsys=datapath-loader
level=debug msg="Found variable with offset 172492" subsys=elf symbol=LXC_ID
level=debug msg="Found variable with offset 172488" subsys=elf symbol=LXC_IPV4
level=debug msg="Found variable with offset 172472" subsys=elf symbol=LXC_IP_1
level=debug msg="Found variable with offset 172476" subsys=elf symbol=LXC_IP_2
level=debug msg="Found variable with offset 172480" subsys=elf symbol=LXC_IP_3
level=debug msg="Found variable with offset 172484" subsys=elf symbol=LXC_IP_4
level=debug msg="Found variable with offset 172496" subsys=elf symbol=NODE_MAC_1
level=debug msg="Found variable with offset 172500" subsys=elf symbol=NODE_MAC_2
level=debug msg="Found variable with offset 172512" subsys=elf symbol=POLICY_VERDICT_LOG_FILTER
level=debug msg="Found variable with offset 172504" subsys=elf symbol=SECLABEL
level=debug msg="Found variable with offset 172508" subsys=elf symbol=SECLABEL_NB
level=debug msg="Found symbol referring to unknown section id 30" subsys=elf symbol=____license
level=debug msg="Skipping __send_drop_notify" subsys=elf
level=debug msg="Skipping cil_from_container" subsys=elf
level=debug msg="Skipping cil_to_container" subsys=elf
level=debug msg="Found symbol with offset 1406496" subsys=elf symbol=cilium_auth_map
level=debug msg="Found symbol with offset 1406128" subsys=elf symbol=cilium_call_policy
level=debug msg="Found symbol with offset 1410936" subsys=elf symbol=cilium_calls_65535
level=debug msg="Found symbol with offset 1406590" subsys=elf symbol=cilium_ct4_global
level=debug msg="Found symbol with offset 1406568" subsys=elf symbol=cilium_ct_any4_global
level=debug msg="Found symbol with offset 1406689" subsys=elf symbol=cilium_encrypt_state
level=debug msg="Found symbol with offset 1406261" subsys=elf symbol=cilium_events
level=debug msg="Found symbol with offset 1406755" subsys=elf symbol=cilium_ipcache
level=debug msg="Found symbol with offset 1406322" subsys=elf symbol=cilium_ipv4_frag_datagrams
level=debug msg="Found symbol with offset 1406089" subsys=elf symbol=cilium_lb4_affinity
level=debug msg="Found symbol with offset 1412433" subsys=elf symbol=cilium_lb4_backends_v3
level=debug msg="Found symbol with offset 1406846" subsys=elf symbol=cilium_lb4_crab
level=debug msg="Found symbol with offset 1406633" subsys=elf symbol=cilium_lb4_crab_long
level=debug msg="Found symbol with offset 1406238" subsys=elf symbol=cilium_lb4_reverse_nat
level=debug msg="Found symbol with offset 1413395" subsys=elf symbol=cilium_lb4_services_v2
level=debug msg="Found symbol with offset 1406770" subsys=elf symbol=cilium_lb4_source_range
level=debug msg="Found symbol with offset 1406608" subsys=elf symbol=cilium_lb_affinity_match
level=debug msg="Found symbol with offset 1406794" subsys=elf symbol=cilium_lxc
level=debug msg="Found symbol with offset 1406382" subsys=elf symbol=cilium_metrics
level=debug msg="Found symbol with offset 1406512" subsys=elf symbol=cilium_node_map
level=debug msg="Found symbol with offset 1411311" subsys=elf symbol=cilium_nodeport_neigh4
level=debug msg="Found symbol with offset 1410916" subsys=elf symbol=cilium_policy_65535
level=debug msg="Found symbol with offset 1406654" subsys=elf symbol=cilium_runtime_config
level=debug msg="Found symbol with offset 1406349" subsys=elf symbol=cilium_signals
level=debug msg="Found symbol with offset 1406544" subsys=elf symbol=cilium_snat_v4_external
level=debug msg="Found symbol with offset 1411286" subsys=elf symbol=cilium_tail_call_buffer4
level=debug msg="Skipping handle_policy" subsys=elf
level=debug msg="Skipping tail_handle_arp" subsys=elf
level=debug msg="Skipping tail_handle_ipv4" subsys=elf
level=debug msg="Skipping tail_handle_ipv4_cont" subsys=elf
level=debug msg="Skipping tail_handle_snat_fwd_ipv4" subsys=elf
level=debug msg="Skipping tail_ipv4_ct_egress" subsys=elf
level=debug msg="Skipping tail_ipv4_ct_ingress" subsys=elf
level=debug msg="Skipping tail_ipv4_to_endpoint" subsys=elf
level=debug msg="Skipping tail_rev_nodeport_lb4" subsys=elf
level=debug msg="Found section with offset 1406185" subsys=elf symbol=.BTF.ext
level=debug msg="Found section with offset 1406368" subsys=elf symbol=.debug_ranges
level=debug msg="Found section with offset 1406448" subsys=elf symbol=to-container
level=debug msg="Found section with offset 1406465" subsys=elf symbol=from-container
level=debug msg="Found section with offset 1406532" subsys=elf symbol=.debug_info
level=debug msg="Found section with offset 1406680" subsys=elf symbol=1/0xffff
level=debug msg="Found section with offset 1406726" subsys=elf symbol=.debug_line
level=debug msg="Found section with offset 1406742" subsys=elf symbol=.debug_frame
level=debug msg="Found section with offset 1406809" subsys=elf symbol=.debug_loc
level=debug msg="Found section with offset 1406907" subsys=elf symbol=.BTF
level=debug msg="Found section with offset 1407514" subsys=elf symbol=2/29
level=debug msg="Found section with offset 1408298" subsys=elf symbol=2/38
level=debug msg="Found section with offset 1409351" subsys=elf symbol=2/27
level=debug msg="Found section with offset 1409431" subsys=elf symbol=2/17
level=debug msg="Found section with offset 1409477" subsys=elf symbol=2/7
level=debug msg="Found section with offset 1410433" subsys=elf symbol=2/6
level=debug msg="Found section with offset 1411140" subsys=elf symbol=2/25
level=debug msg="Found section with offset 1413316" subsys=elf symbol=2/13
level=debug msg="Found section with offset 1415309" subsys=elf symbol=2/1
level=debug msg="Watching template path" file-path=/var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18/bpf_lxc.o subsys=datapath-loader
level=debug msg="Finished writing ELF" error="<nil>" new-elf-path=1588_next/bpf_lxc.o subsys=elf template-path=/var/run/cilium/state/templates/1088afac623e603505b4e0ebd6aa17f1fa9fd42867e442e321df129c62804a18/bpf_lxc.o
level=debug msg="Loading CollectionSpec from ELF" device=lxc_health ifindex=7 objPath=1588_next/bpf_lxc.o subsys=datapath-loader
level=debug msg="Loading Collection into kernel" device=lxc_health ifindex=7 objPath=1588_next/bpf_lxc.o subsys=datapath-loader
level=debug msg="Attaching program to interface" device=cilium_host direction=ingress ifindex=5 objPath=3957_next/bpf_host.o progName=cil_to_host subsys=datapath-loader
level=debug msg="Attaching program to interface" device=lxc_health direction=ingress ifindex=7 objPath=1588_next/bpf_lxc.o progName=cil_from_container subsys=datapath-loader
level=debug msg="Successfully attached program to interface" device=cilium_host direction=ingress ifindex=5 objPath=3957_next/bpf_host.o progName=cil_to_host subsys=datapath-loader
level=debug msg="Successfully attached program to interface" device=lxc_health direction=ingress ifindex=7 objPath=1588_next/bpf_lxc.o progName=cil_from_container subsys=datapath-loader
level=info msg="Rewrote endpoint BPF program" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Loading CollectionSpec from ELF" device=cilium_host ifindex=5 objPath=3957_next/bpf_host.o subsys=datapath-loader
level=debug msg="Waiting for proxy updates to complete..." containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Wait time for proxy updates: 652.709µs" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Finalizing successful endpoint regeneration" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="synchronizing directories" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="attempting to make temporary directory new directory for endpoint programs" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ originalDirectory=/var/run/cilium/state/1588 subsys=endpoint temporaryDirectory=1588_next
level=debug msg="removing directory" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 directory=1588_next_fail endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting new controller" name=sync-policymap-1588 subsys=controller uuid=4946adf9-9d4c-4068-95f3-159f4369c04e
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=1588_next endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint regeneration" bpfCompilation=10.34686163s bpfLoadProg=274.925083ms bpfWaitForELF=10.347004546s bpfWriteELF=1.139833ms containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ mapSync=7.305251ms policyCalculation="70.958µs" prepareBuild="360µs" proxyConfiguration="6.75µs" proxyPolicyCalculation=5.38475ms proxyWaitForAck="704.25µs" reason="updated security labels" subsys=endpoint total=10.640440714s waitingForCTClean="582.75µs" waitingForLock="1.376µs"
level=debug msg="Loading Collection into kernel" device=cilium_host ifindex=5 objPath=3957_next/bpf_host.o subsys=datapath-loader
level=debug msg="Controller func execution time: 19.948ms" name=sync-policymap-1588 subsys=controller uuid=4946adf9-9d4c-4068-95f3-159f4369c04e
level=debug msg="Successfully regenerated endpoint program (Reason: updated security labels)" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1588 endpointState=waiting-to-regenerate identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=200
level=debug msg="EventQueue event processing statistics" eventConsumeOffQueueWaitTime="82.875µs" eventEnqueueWaitTime=459ns eventHandlingDuration=10.668032255s eventType="*endpoint.EndpointRegenerationEvent" name=endpoint-1588 subsys=eventqueue
level=debug msg="Dequeued endpoint from build queue" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Regenerating endpoint" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ reason="one or more identities created or deleted" startTime="2023-07-16 12:01:32.810645262 +0000 UTC m=+15.217828300" subsys=endpoint
level=debug msg="Regenerating endpoint: one or more identities created or deleted" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1588 endpointState=regenerating identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=1588_next endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting policy recalculation..." containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Skipping unnecessary endpoint policy recalculation" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ policyChanged=false policyRevision.next=1 policyRevision.repo=1 subsys=endpoint
level=debug msg="preparing new cache transaction: upserting 1 entries, deleting 0 entries" subsys=xds xdsCachedVersion=3 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="cache unmodified by transaction; aborting" subsys=xds xdsCachedVersion=3 xdsTypeURL=type.googleapis.com/cilium.NetworkPolicy
level=debug msg="BPF header file hashed (was: \"c980f992bea92acc6727288e20ee0d49cd3972683dcd39eda9bc45326f35c275\")" bpfHeaderfileHash=c980f992bea92acc6727288e20ee0d49cd3972683dcd39eda9bc45326f35c275 containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Preparing to compile BPF" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ regeneration-level=no-rebuild subsys=endpoint
level=debug msg="BPF header file unchanged, skipping BPF compilation and installation" bpfHeaderfileHash=c980f992bea92acc6727288e20ee0d49cd3972683dcd39eda9bc45326f35c275 containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Waiting for proxy updates to complete..." containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Wait time for proxy updates: 43.458µs" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Finalizing successful endpoint regeneration" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="synchronizing directories" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="endpoint directory exists; backing it up" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=1588_stale endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="moving current directory to backup location" backupDirectory=1588_stale containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ originalDirectory=/var/run/cilium/state/1588 subsys=endpoint
level=debug msg="some BPF state files were not recreated; moving old BPF objects into new directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=1588_next_fail endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=1588_stale endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Updating existing controller" name=sync-policymap-1588 subsys=controller uuid=4946adf9-9d4c-4068-95f3-159f4369c04e
level=debug msg="Controller update time: 51.667µs" name=sync-policymap-1588 subsys=controller uuid=4946adf9-9d4c-4068-95f3-159f4369c04e
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=1588_next endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint regeneration with no pending regeneration requests" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1588 endpointState=ready identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="Completed endpoint regeneration" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1588 identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ mapSync="73.541µs" policyCalculation="111.5µs" prepareBuild=2.714916ms proxyConfiguration="240.25µs" proxyPolicyCalculation=24.49675ms proxyWaitForAck="85.917µs" reason="one or more identities created or deleted" subsys=endpoint total=28.801875ms waitingForCTClean=500ns waitingForLock="2.332µs"
level=debug msg="Successfully regenerated endpoint program (Reason: one or more identities created or deleted)" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=1588 endpointState=ready identity=4 ipv4=10.244.3.91 ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=200
level=debug msg="Controller func execution time: 221.917µs" name=sync-policymap-1588 subsys=controller uuid=4946adf9-9d4c-4068-95f3-159f4369c04e
level=debug msg="EventQueue event processing statistics" eventConsumeOffQueueWaitTime=7.001746379s eventEnqueueWaitTime=959ns eventHandlingDuration=29.166166ms eventType="*endpoint.EndpointRegenerationEvent" name=endpoint-1588 subsys=eventqueue
level=debug msg="Attaching program to interface" device=cilium_host direction=egress ifindex=5 objPath=3957_next/bpf_host.o progName=cil_from_host subsys=datapath-loader
level=debug msg="Successfully attached program to interface" device=cilium_host direction=egress ifindex=5 objPath=3957_next/bpf_host.o progName=cil_from_host subsys=datapath-loader
level=debug msg="Loading CollectionSpec from ELF" device=cilium_net ifindex=4 objPath=3957_next/bpf_host_cilium_net.o subsys=datapath-loader
level=debug msg="Loading Collection into kernel" device=cilium_net ifindex=4 objPath=3957_next/bpf_host_cilium_net.o subsys=datapath-loader
level=debug msg="Cannot find socket" error="stat /var/run/cilium/health.sock: no such file or directory" file-path=/var/run/cilium/health.sock subsys=cilium-health-launcher
level=debug msg="Load 1-min: 5.38 5-min: 3.31 15min: 2.17" endpointID=3957 subsys=endpoint
level=debug msg="Memory: Total: 3933 Used: 2591 (65.87%) Free: 257 Buffers: 8 Cached: 1075" endpointID=3957 subsys=endpoint
level=debug msg="Swap: Total: 1023 Used: 1023 (99.95%) Free: 0" endpointID=3957 subsys=endpoint
level=debug msg="NAME cilium-agent STATUS [sleep] PID 1 CPU: 6.85% MEM: 2.26% CMDLINE: cilium-agent --config-dir=/tmp/cilium/config-map MEM-EXT: RSS: 88 VMS: 785 Data: 0 Stack: 0 Locked: 0 Swap: 0" endpointID=3957 subsys=endpoint
level=debug msg="Attaching program to interface" device=cilium_net direction=ingress ifindex=4 objPath=3957_next/bpf_host_cilium_net.o progName=cil_to_host subsys=datapath-loader
level=debug msg="Successfully attached program to interface" device=cilium_net direction=ingress ifindex=4 objPath=3957_next/bpf_host_cilium_net.o progName=cil_to_host subsys=datapath-loader
level=debug msg="Loading CollectionSpec from ELF" device=eth0 ifindex=165 objPath=3957_next/bpf_netdev_eth0.o subsys=datapath-loader
level=debug msg="Loading Collection into kernel" device=eth0 ifindex=165 objPath=3957_next/bpf_netdev_eth0.o subsys=datapath-loader
level=debug msg="Attaching program to interface" device=eth0 direction=ingress ifindex=165 objPath=3957_next/bpf_netdev_eth0.o progName=cil_from_netdev subsys=datapath-loader
level=debug msg="Successfully attached program to interface" device=eth0 direction=ingress ifindex=165 objPath=3957_next/bpf_netdev_eth0.o progName=cil_from_netdev subsys=datapath-loader
level=debug msg="Loading CollectionSpec from ELF" device=eth0 ifindex=165 objPath=3957_next/bpf_netdev_eth0.o subsys=datapath-loader
level=debug msg="Loading Collection into kernel" device=eth0 ifindex=165 objPath=3957_next/bpf_netdev_eth0.o subsys=datapath-loader
level=debug msg="Attaching program to interface" device=eth0 direction=egress ifindex=165 objPath=3957_next/bpf_netdev_eth0.o progName=cil_to_netdev subsys=datapath-loader
level=debug msg="Successfully attached program to interface" device=eth0 direction=egress ifindex=165 objPath=3957_next/bpf_netdev_eth0.o progName=cil_to_netdev subsys=datapath-loader
level=info msg="Rewrote endpoint BPF program" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Waiting for proxy updates to complete..." containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Wait time for proxy updates: 9.417µs" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Finalizing successful endpoint regeneration" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="synchronizing directories" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="attempting to make temporary directory new directory for endpoint programs" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ originalDirectory=/var/run/cilium/state/3957 subsys=endpoint temporaryDirectory=3957_next
level=debug msg="removing directory" containerID= datapathPolicyRevision=0 desiredPolicyRevision=1 directory=3957_next_fail endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting new controller" name=sync-policymap-3957 subsys=controller uuid=64193222-db62-4bac-8d41-b1e7137717c0
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=3957_next endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint regeneration" bpfCompilation=11.213023713s bpfLoadProg=1.257610959s bpfWaitForELF=11.21407688s bpfWriteELF=5.5155ms containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ mapSync="936.334µs" policyCalculation="80.709µs" prepareBuild=8.806959ms proxyConfiguration="7.375µs" proxyPolicyCalculation="7.459µs" proxyWaitForAck="21.041µs" reason="updated security labels" subsys=endpoint total=12.52522009s waitingForCTClean="495.375µs" waitingForLock="1.459µs"
level=debug msg="Controller func execution time: 146.917µs" name=sync-policymap-3957 subsys=controller uuid=64193222-db62-4bac-8d41-b1e7137717c0
level=debug msg="Successfully regenerated endpoint program (Reason: updated security labels)" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3957 endpointState=waiting-to-regenerate identity=1 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=200
level=debug msg="EventQueue event processing statistics" eventConsumeOffQueueWaitTime=9.632667ms eventEnqueueWaitTime=333ns eventHandlingDuration=12.525678756s eventType="*endpoint.EndpointRegenerationEvent" name=endpoint-3957 subsys=eventqueue
level=debug msg="Dequeued endpoint from build queue" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Regenerating endpoint" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ reason="one or more identities created or deleted" startTime="2023-07-16 12:01:33.604258929 +0000 UTC m=+16.011441967" subsys=endpoint
level=debug msg="Regenerating endpoint: one or more identities created or deleted" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3957 endpointState=regenerating identity=1 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=3957_next endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Starting policy recalculation..." containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Skipping unnecessary endpoint policy recalculation" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ policyChanged=false policyRevision.next=1 policyRevision.repo=1 subsys=endpoint
level=debug msg="Endpoint has no IP addresses" endpointID=3957 subsys=envoy-manager
level=debug msg="BPF header file hashed (was: \"d6b162efae099e5d5da51d51c571ec20cccdd8f3c83f1392130628eb9048d67c\")" bpfHeaderfileHash=d6b162efae099e5d5da51d51c571ec20cccdd8f3c83f1392130628eb9048d67c containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Preparing to compile BPF" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ regeneration-level=no-rebuild subsys=endpoint
level=debug msg="BPF header file unchanged, skipping BPF compilation and installation" bpfHeaderfileHash=d6b162efae099e5d5da51d51c571ec20cccdd8f3c83f1392130628eb9048d67c containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Waiting for proxy updates to complete..." containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Wait time for proxy updates: 6µs" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Finalizing successful endpoint regeneration" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="synchronizing directories" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="endpoint directory exists; backing it up" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=3957_stale endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="moving current directory to backup location" backupDirectory=3957_stale containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ originalDirectory=/var/run/cilium/state/3957 subsys=endpoint
level=debug msg="some BPF state files were not recreated; moving old BPF objects into new directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=3957_next_fail endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=3957_stale endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Updating existing controller" name=sync-policymap-3957 subsys=controller uuid=64193222-db62-4bac-8d41-b1e7137717c0
level=debug msg="Controller update time: 6.708µs" name=sync-policymap-3957 subsys=controller uuid=64193222-db62-4bac-8d41-b1e7137717c0
level=debug msg="removing directory" containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 directory=3957_next endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ subsys=endpoint
level=debug msg="Completed endpoint regeneration with no pending regeneration requests" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3957 endpointState=ready identity=1 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=0
level=debug msg="Completed endpoint regeneration" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3957 identity=1 ipv4= ipv6= k8sPodName=/ mapSync="25.583µs" policyCalculation="20.542µs" prepareBuild="167.417µs" proxyConfiguration="5.917µs" proxyPolicyCalculation="4.833µs" proxyWaitForAck="29.916µs" reason="one or more identities created or deleted" subsys=endpoint total="496.083µs" waitingForCTClean=166ns waitingForLock=750ns
level=debug msg="Successfully regenerated endpoint program (Reason: one or more identities created or deleted)" code=OK containerID= datapathPolicyRevision=1 desiredPolicyRevision=1 endpointID=3957 endpointState=ready identity=1 ipv4= ipv6= k8sPodName=/ policyRevision=1 subsys=endpoint type=200
level=debug msg="EventQueue event processing statistics" eventConsumeOffQueueWaitTime=7.795422504s eventEnqueueWaitTime=125ns eventHandlingDuration="546.542µs" eventType="*endpoint.EndpointRegenerationEvent" name=endpoint-3957 subsys=eventqueue
level=debug msg="Controller func execution time: 23.083µs" name=sync-policymap-3957 subsys=controller uuid=64193222-db62-4bac-8d41-b1e7137717c0
level=debug msg="Controller func execution time: 32.917µs" name=metricsmap-bpf-prom-sync subsys=controller uuid=5e26b3a0-3934-459d-a21b-050456120824
level=debug msg="Cannot find socket" error="stat /var/run/cilium/health.sock: no such file or directory" file-path=/var/run/cilium/health.sock subsys=cilium-health-launcher
level=debug msg="Processing 1 endpoints for EndpointSlice kube-dns-6b8l9" subsys=k8s
level=debug msg="discarding Endpoint on EndpointSlice kube-dns-6b8l9: not Serving and EnableK8sTerminatingEndpoint true" subsys=k8s
level=debug msg="EndpointSlice kube-dns-6b8l9 has 0 backends" subsys=k8s
level=debug msg="Processing 2 endpoints for EndpointSlice kube-dns-6b8l9" subsys=k8s
level=debug msg="discarding Endpoint on EndpointSlice kube-dns-6b8l9: not Serving and EnableK8sTerminatingEndpoint true" subsys=k8s
level=debug msg="discarding Endpoint on EndpointSlice kube-dns-6b8l9: not Serving and EnableK8sTerminatingEndpoint true" subsys=k8s
level=debug msg="EndpointSlice kube-dns-6b8l9 has 0 backends" subsys=k8s
level=debug msg="Cannot find socket" error="stat /var/run/cilium/health.sock: no such file or directory" file-path=/var/run/cilium/health.sock subsys=cilium-health-launcher
level=debug msg="Processing 2 endpoints for EndpointSlice kube-dns-6b8l9" subsys=k8s
level=debug msg="discarding Endpoint on EndpointSlice kube-dns-6b8l9: not Serving and EnableK8sTerminatingEndpoint true" subsys=k8s
level=debug msg="EndpointSlice kube-dns-6b8l9 has 1 backends" subsys=k8s
level=debug msg="Kubernetes service definition changed" action=service-updated endpoints="10.244.0.214:53/TCP,10.244.0.214:53/UDP,10.244.0.214:9153/TCP" k8sNamespace=kube-system k8sSvcName=kube-dns old-service=nil service="frontends:[10.96.0.10]/ports=[dns dns-tcp metrics]/selector=map[k8s-app:kube-dns]" subsys=k8s-watcher
level=debug msg="Upserting service" backends="[10.244.0.214:53]" l7LBFrontendPorts="[]" l7LBProxyPort=0 loadBalancerSourceRanges="[]" serviceIP="{10.96.0.10 {UDP 53} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcExtTrafficPolicy=Cluster svcHealthCheckNodePort=0 svcIntTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Acquired service ID" backends="[10.244.0.214:53]" l7LBFrontendPorts="[]" l7LBProxyPort=0 loadBalancerSourceRanges="[]" serviceID=2 serviceIP="{10.96.0.10 {UDP 53} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcExtTrafficPolicy=Cluster svcHealthCheckNodePort=0 svcIntTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Deleting backends from session affinity match" backends="[]" serviceID=2 subsys=service
level=debug msg="Adding new backend" backendID=2 backendWeight=100 backends="[10.244.0.214:53]" l3n4Addr="{10.244.0.214 {UDP 53} 0}" l7LBFrontendPorts="[]" l7LBProxyPort=0 loadBalancerSourceRanges="[]" serviceID=2 serviceIP="{10.96.0.10 {UDP 53} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcExtTrafficPolicy=Cluster svcHealthCheckNodePort=0 svcIntTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Upserted service entry" backendSlot=1 subsys=map-lb svcKey="10.96.0.10:13568" svcVal="2 0 (512) [0x0 0x0]"
level=debug msg="Upserted service entry" backendSlot=0 subsys=map-lb svcKey="10.96.0.10:13568" svcVal="0 1 (512) [0x0 0x0]"
level=debug msg="Adding backends to affinity match map" backends="[]" serviceID=2 subsys=service
level=debug msg="Upserting service" backends="[10.244.0.214:9153]" l7LBFrontendPorts="[]" l7LBProxyPort=0 loadBalancerSourceRanges="[]" serviceIP="{10.96.0.10 {TCP 9153} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcExtTrafficPolicy=Cluster svcHealthCheckNodePort=0 svcIntTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Acquired service ID" backends="[10.244.0.214:9153]" l7LBFrontendPorts="[]" l7LBProxyPort=0 loadBalancerSourceRanges="[]" serviceID=3 serviceIP="{10.96.0.10 {TCP 9153} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcExtTrafficPolicy=Cluster svcHealthCheckNodePort=0 svcIntTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Deleting backends from session affinity match" backends="[]" serviceID=3 subsys=service
level=debug msg="Adding new backend" backendID=3 backendWeight=100 backends="[10.244.0.214:9153]" l3n4Addr="{10.244.0.214 {TCP 9153} 0}" l7LBFrontendPorts="[]" l7LBProxyPort=0 loadBalancerSourceRanges="[]" serviceID=3 serviceIP="{10.96.0.10 {TCP 9153} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcExtTrafficPolicy=Cluster svcHealthCheckNodePort=0 svcIntTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Upserted service entry" backendSlot=1 subsys=map-lb svcKey="10.96.0.10:49443" svcVal="3 0 (768) [0x0 0x0]"
level=debug msg="Upserted service entry" backendSlot=0 subsys=map-lb svcKey="10.96.0.10:49443" svcVal="0 1 (768) [0x0 0x0]"
level=debug msg="Adding backends to affinity match map" backends="[]" serviceID=3 subsys=service
level=debug msg="Processing 2 endpoints for EndpointSlice kube-dns-6b8l9" subsys=k8s
level=debug msg="EndpointSlice kube-dns-6b8l9 has 2 backends" subsys=k8s
level=debug msg="Kubernetes service definition changed" action=service-updated endpoints="10.244.0.214:53/TCP,10.244.0.214:53/UDP,10.244.0.214:9153/TCP,10.244.0.242:53/TCP,10.244.0.242:53/UDP,10.244.0.242:9153/TCP" k8sNamespace=kube-system k8sSvcName=kube-dns old-service=nil service="frontends:[10.96.0.10]/ports=[dns dns-tcp metrics]/selector=map[k8s-app:kube-dns]" subsys=k8s-watcher
level=debug msg="Upserting service" backends="[10.244.0.214:53 10.244.0.242:53]" l7LBFrontendPorts="[]" l7LBProxyPort=0 loadBalancerSourceRanges="[]" serviceIP="{10.96.0.10 {UDP 53} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcExtTrafficPolicy=Cluster svcHealthCheckNodePort=0 svcIntTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Acquired service ID" backends="[10.244.0.214:53 10.244.0.242:53]" l7LBFrontendPorts="[]" l7LBProxyPort=0 loadBalancerSourceRanges="[]" serviceID=2 serviceIP="{10.96.0.10 {UDP 53} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcExtTrafficPolicy=Cluster svcHealthCheckNodePort=0 svcIntTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Deleting backends from session affinity match" backends="[]" serviceID=2 subsys=service
level=debug msg="Adding new backend" backendID=4 backendWeight=100 backends="[10.244.0.214:53 10.244.0.242:53]" l3n4Addr="{10.244.0.242 {UDP 53} 0}" l7LBFrontendPorts="[]" l7LBProxyPort=0 loadBalancerSourceRanges="[]" serviceID=2 serviceIP="{10.96.0.10 {UDP 53} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcExtTrafficPolicy=Cluster svcHealthCheckNodePort=0 svcIntTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Upserted service entry" backendSlot=1 subsys=map-lb svcKey="10.96.0.10:13568" svcVal="2 0 (512) [0x0 0x0]"
level=debug msg="Upserted service entry" backendSlot=2 subsys=map-lb svcKey="10.96.0.10:13568" svcVal="4 0 (512) [0x0 0x0]"
level=debug msg="Upserted service entry" backendSlot=0 subsys=map-lb svcKey="10.96.0.10:13568" svcVal="0 2 (512) [0x0 0x0]"
level=debug msg="Adding backends to affinity match map" backends="[]" serviceID=2 subsys=service
level=debug msg="Upserting service" backends="[10.244.0.214:9153 10.244.0.242:9153]" l7LBFrontendPorts="[]" l7LBProxyPort=0 loadBalancerSourceRanges="[]" serviceIP="{10.96.0.10 {TCP 9153} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcExtTrafficPolicy=Cluster svcHealthCheckNodePort=0 svcIntTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Acquired service ID" backends="[10.244.0.214:9153 10.244.0.242:9153]" l7LBFrontendPorts="[]" l7LBProxyPort=0 loadBalancerSourceRanges="[]" serviceID=3 serviceIP="{10.96.0.10 {TCP 9153} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcExtTrafficPolicy=Cluster svcHealthCheckNodePort=0 svcIntTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Deleting backends from session affinity match" backends="[]" serviceID=3 subsys=service
level=debug msg="Adding new backend" backendID=5 backendWeight=100 backends="[10.244.0.214:9153 10.244.0.242:9153]" l3n4Addr="{10.244.0.242 {TCP 9153} 0}" l7LBFrontendPorts="[]" l7LBProxyPort=0 loadBalancerSourceRanges="[]" serviceID=3 serviceIP="{10.96.0.10 {TCP 9153} 0}" serviceName=kube-dns serviceNamespace=kube-system sessionAffinity=false sessionAffinityTimeout=0 subsys=service svcExtTrafficPolicy=Cluster svcHealthCheckNodePort=0 svcIntTrafficPolicy=Cluster svcType=ClusterIP
level=debug msg="Upserted service entry" backendSlot=1 subsys=map-lb svcKey="10.96.0.10:49443" svcVal="3 0 (768) [0x0 0x0]"
level=debug msg="Upserted service entry" backendSlot=2 subsys=map-lb svcKey="10.96.0.10:49443" svcVal="5 0 (768) [0x0 0x0]"
level=debug msg="Upserted service entry" backendSlot=0 subsys=map-lb svcKey="10.96.0.10:49443" svcVal="0 2 (768) [0x0 0x0]"
level=debug msg="Adding backends to affinity match map" backends="[]" serviceID=3 subsys=service
level=debug msg="Controller func execution time: 392.625µs" name=link-cache subsys=controller uuid=c86a630b-99d8-4932-a653-8c34270205c6
level=debug msg="Cannot find socket" error="stat /var/run/cilium/health.sock: no such file or directory" file-path=/var/run/cilium/health.sock subsys=cilium-health-launcher
level=debug msg="Cannot find socket" error="stat /var/run/cilium/health.sock: no such file or directory" file-path=/var/run/cilium/health.sock subsys=cilium-health-launcher
level=debug msg="Cannot find socket" error="stat /var/run/cilium/health.sock: no such file or directory" file-path=/var/run/cilium/health.sock subsys=cilium-health-launcher
level=debug msg="Greeting successful" host="http://10.244.1.18:4240" ipAddr=10.244.1.18 nodeName=kind-kind/kind-worker2 path="Via L3" rtt=15.482824049s subsys=health-server
level=debug msg="Greeting host" host="http://172.26.0.4:4240" ipAddr=172.26.0.4 nodeName=kind-kind/kind-worker2 path="Via L3" subsys=health-server
level=debug msg="Greeting successful" host="http://172.26.0.4:4240" ipAddr=172.26.0.4 nodeName=kind-kind/kind-worker2 path="Via L3" rtt=3.594459ms subsys=health-server
level=debug msg="Greeting host" host="http://172.26.0.2:4240" ipAddr=172.26.0.2 nodeName=kind-kind/kind-control-plane path="Via L3" subsys=health-server
level=debug msg="Greeting successful" host="http://172.26.0.2:4240" ipAddr=172.26.0.2 nodeName=kind-kind/kind-control-plane path="Via L3" rtt=4.703709ms subsys=health-server
level=debug msg="Greeting host" host="http://10.244.0.252:4240" ipAddr=10.244.0.252 nodeName=kind-kind/kind-control-plane path="Via L3" subsys=health-server
level=debug msg="Greeting successful" host="http://10.244.0.252:4240" ipAddr=10.244.0.252 nodeName=kind-kind/kind-control-plane path="Via L3" rtt=21.660208ms subsys=health-server
level=debug msg="Greeting host" host="http://10.244.4.32:4240" ipAddr=10.244.4.32 nodeName=kind-kind/kind-worker4 path="Via L3" subsys=health-server
level=debug msg="Greeting successful" host="http://10.244.4.32:4240" ipAddr=10.244.4.32 nodeName=kind-kind/kind-worker4 path="Via L3" rtt=2.431084ms subsys=health-server
level=debug msg="Greeting host" host="http://172.26.0.5:4240" ipAddr=172.26.0.5 nodeName=kind-kind/kind-worker4 path="Via L3" subsys=health-server
level=debug msg="Greeting successful" host="http://172.26.0.5:4240" ipAddr=172.26.0.5 nodeName=kind-kind/kind-worker4 path="Via L3" rtt=2.1995ms subsys=health-server
level=debug msg="Greeting host" host="http://172.26.0.6:4240" ipAddr=172.26.0.6 nodeName=kind-kind/kind-worker path="Via L3" subsys=health-server
level=debug msg="Greeting successful" host="http://172.26.0.6:4240" ipAddr=172.26.0.6 nodeName=kind-kind/kind-worker path="Via L3" rtt=2.611709ms subsys=health-server
level=debug msg="Greeting host" host="http://10.244.2.167:4240" ipAddr=10.244.2.167 nodeName=kind-kind/kind-worker path="Via L3" subsys=health-server
level=debug msg="Greeting successful" host="http://10.244.2.167:4240" ipAddr=10.244.2.167 nodeName=kind-kind/kind-worker path="Via L3" rtt=2.326709ms subsys=health-server
level=debug msg="Greeting host" host="http://172.26.0.3:4240" ipAddr=172.26.0.3 nodeName=kind-kind/kind-worker3 path="Via L3" subsys=health-server
level=debug msg="Greeting successful" host="http://172.26.0.3:4240" ipAddr=172.26.0.3 nodeName=kind-kind/kind-worker3 path="Via L3" rtt="158.666µs" subsys=health-server
level=debug msg="Greeting host" host="http://10.244.3.91:4240" ipAddr=10.244.3.91 nodeName=kind-kind/kind-worker3 path="Via L3" subsys=health-server
level=debug msg="Greeting successful" host="http://10.244.3.91:4240" ipAddr=10.244.3.91 nodeName=kind-kind/kind-worker3 path="Via L3" rtt="961.083µs" subsys=health-server
level=debug msg="Run complete" subsys=health-server
level=debug msg="Sending request for /cluster/nodes ..." subsys=health-server
level=debug msg="Got cilium /cluster/nodes" subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::2" nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.0.252 nodeName=kind-kind/kind-control-plane primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.0.61 nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.2 nodeName=kind-kind/kind-control-plane primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::2" nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.0.252 nodeName=kind-kind/kind-control-plane primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.0.61 nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.2 nodeName=kind-kind/kind-control-plane primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::6" nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.2.167 nodeName=kind-kind/kind-worker primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.2.95 nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.6 nodeName=kind-kind/kind-worker primary=true protocol=icmp subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.5 nodeName=kind-kind/kind-worker4 primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::5" nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.4.32 nodeName=kind-kind/kind-worker4 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.4.235 nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.3 nodeName=kind-kind/kind-worker3 primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::3" nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.3.91 nodeName=kind-kind/kind-worker3 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.3.223 nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.3 nodeName=kind-kind/kind-worker3 primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::3" nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.3.91 nodeName=kind-kind/kind-worker3 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.3.223 nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.4 nodeName=kind-kind/kind-worker2 primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::4" nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.1.18 nodeName=kind-kind/kind-worker2 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.1.242 nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::4" nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.1.18 nodeName=kind-kind/kind-worker2 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.1.242 nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.4 nodeName=kind-kind/kind-worker2 primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.4.235 nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.5 nodeName=kind-kind/kind-worker4 primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::5" nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.4.32 nodeName=kind-kind/kind-worker4 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.0.61 nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.2 nodeName=kind-kind/kind-control-plane primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::2" nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.0.252 nodeName=kind-kind/kind-control-plane primary=false protocol=icmp subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.6 nodeName=kind-kind/kind-worker primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::6" nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.2.167 nodeName=kind-kind/kind-worker primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.2.95 nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.3 nodeName=kind-kind/kind-worker3 primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::3" nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.3.91 nodeName=kind-kind/kind-worker3 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.3.223 nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.4.32 nodeName=kind-kind/kind-worker4 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.4.235 nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.5 nodeName=kind-kind/kind-worker4 primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::5" nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.1.18 nodeName=kind-kind/kind-worker2 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.1.242 nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.4 nodeName=kind-kind/kind-worker2 primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::4" nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.0.61 nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.2 nodeName=kind-kind/kind-control-plane primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::2" nodeName=kind-kind/kind-control-plane primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.0.252 nodeName=kind-kind/kind-control-plane primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::4" nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.1.18 nodeName=kind-kind/kind-worker2 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.1.242 nodeName=kind-kind/kind-worker2 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.4 nodeName=kind-kind/kind-worker2 primary=true protocol=icmp subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.6 nodeName=kind-kind/kind-worker primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::6" nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.2.167 nodeName=kind-kind/kind-worker primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.2.95 nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.5 nodeName=kind-kind/kind-worker4 primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::5" nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.4.32 nodeName=kind-kind/kind-worker4 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.4.235 nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.3.223 nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.3 nodeName=kind-kind/kind-worker3 primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::3" nodeName=kind-kind/kind-worker3 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.3.91 nodeName=kind-kind/kind-worker3 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.2.95 nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.6 nodeName=kind-kind/kind-worker primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::6" nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.2.167 nodeName=kind-kind/kind-worker primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker primary=false subsys=health-server
level=debug msg="Skipping probe for address" ipAddr=10.244.4.235 nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=172.26.0.5 nodeName=kind-kind/kind-worker4 primary=true protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr="fc00:c111::5" nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Probing for connectivity to node" ipAddr=10.244.4.32 nodeName=kind-kind/kind-worker4 primary=false protocol=icmp subsys=health-server
level=debug msg="Skipping probe for address" ipAddr= nodeName=kind-kind/kind-worker4 primary=false subsys=health-server
level=debug msg="Probe successful" ipAddr=172.26.0.3 nodeName=kind-kind/kind-worker3 rtt="906.75µs" subsys=health-server
level=debug msg="Probe successful" ipAddr=10.244.3.91 nodeName=kind-kind/kind-worker3 rtt=1.005667ms subsys=health-server
level=debug msg="Group not found" error="group: unknown group cilium" file-path=/var/run/cilium/health.sock group=cilium subsys=api
level=info msg="Serving cilium health API at unix:///var/run/cilium/health.sock" subsys=health-server
level=debug msg="Controller func execution time: 334.417µs" name=metricsmap-bpf-prom-sync subsys=controller uuid=5e26b3a0-3934-459d-a21b-050456120824
level=debug msg="Group not found" error="group: unknown group cilium" file-path=/var/run/cilium/health.sock group=cilium subsys=api
level=debug msg="Handling request for /healthz" subsys=health-server
